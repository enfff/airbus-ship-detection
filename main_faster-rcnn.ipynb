{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8438193,"sourceType":"datasetVersion","datasetId":5026303},{"sourceId":8516397,"sourceType":"datasetVersion","datasetId":5084559,"isSourceIdPinned":true}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# MAIN CONFIGURATIONS\ncreate_log_file = True\nsave_to_drive = False\nmodel_id = '4'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 2  # Number of epochs the model will train for\nbatch_size = 32\n# init_lr = 0.1 # Initial Learning Rate\ndata_augmentation_type = 'fourier'    # Which data augmentation tecnique are we using?\n                                    # 'noaug':     no data augmentation\n                                    # 'fourier':   fourier transforms\n\n# !tree # Prints folder structure\ntest_only = False # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\n\nmodel_filepath = f\"model_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:42:07.693255Z","iopub.execute_input":"2024-06-03T12:42:07.693695Z","iopub.status.idle":"2024-06-03T12:42:07.732748Z","shell.execute_reply.started":"2024-06-03T12:42:07.693650Z","shell.execute_reply":"2024-06-03T12:42:07.731497Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_fourier_id4'\n","output_type":"stream"}]},{"cell_type":"code","source":"# clean everything\n# !rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:42:07.734933Z","iopub.execute_input":"2024-06-03T12:42:07.735645Z","iopub.status.idle":"2024-06-03T12:42:07.739798Z","shell.execute_reply.started":"2024-06-03T12:42:07.735604Z","shell.execute_reply":"2024-06-03T12:42:07.738773Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\n\n# !pip install torchsummary\n# from torchsummary import summary\n!pip install torchmetrics\n!pip install pycocotools faster-coco-eval\n!pip install torchmetrics[detection]","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-03T12:42:07.740888Z","iopub.execute_input":"2024-06-03T12:42:07.741226Z","iopub.status.idle":"2024-06-03T12:42:52.801328Z","shell.execute_reply.started":"2024-06-03T12:42:07.741197Z","shell.execute_reply":"2024-06-03T12:42:52.799997Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2+cpu)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nCollecting pycocotools\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting faster-coco-eval\n  Downloading faster_coco_eval-1.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (5.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (2.2.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->faster-coco-eval) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->faster-coco-eval) (2023.4)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->faster-coco-eval) (8.2.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading faster_coco_eval-1.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools, faster-coco-eval\nSuccessfully installed faster-coco-eval-1.5.4 pycocotools-2.0.7\nRequirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2+cpu)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nRequirement already satisfied: pycocotools>2.0.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.0.7)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2+cpu)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-03T12:42:52.804401Z","iopub.execute_input":"2024-06-03T12:42:52.805150Z","iopub.status.idle":"2024-06-03T12:42:52.812195Z","shell.execute_reply.started":"2024-06-03T12:42:52.805117Z","shell.execute_reply":"2024-06-03T12:42:52.811193Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"device = device(type='cpu')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\n\nif log_filepath:\n    print(log_filepath)\nelse:\n    print(\"No logging\")","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-06-03T12:42:52.813623Z","iopub.execute_input":"2024-06-03T12:42:52.813936Z","iopub.status.idle":"2024-06-03T12:42:52.828766Z","shell.execute_reply.started":"2024-06-03T12:42:52.813909Z","shell.execute_reply":"2024-06-03T12:42:52.827753Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_fourier_id4/log.txt'\nmodels/model_fourier_id4/log.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# FOURIER DATA AUGMENTATION\n\nimport torch.fft as fft\nimport torchvision\nimport random\n\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n        # Scale the image in range [0,1)\n        min_val = 0\n        max_val = 255\n        image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n        min_val = 0\n        max_val = 255\n        # Scale the image in range [0,1)\n        image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T14:04:22.634897Z","iopub.execute_input":"2024-06-03T14:04:22.635823Z","iopub.status.idle":"2024-06-03T14:04:22.656536Z","shell.execute_reply.started":"2024-06-03T14:04:22.635782Z","shell.execute_reply":"2024-06-03T14:04:22.655136Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n        \n        img_train_transforms = v2.Compose([\n             v2.RandomRotation(50),\n             v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n             v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n    \n    case 'fourier':\n        \n        img_train_transforms = v2.Compose([\n             FourierRandomNoise(),\n             PatchGaussian(),\n             FourierBasisAugmentation(),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n\nprint('ok')","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-06-03T14:04:27.390836Z","iopub.execute_input":"2024-06-03T14:04:27.391228Z","iopub.status.idle":"2024-06-03T14:04:27.403480Z","shell.execute_reply.started":"2024-06-03T14:04:27.391197Z","shell.execute_reply":"2024-06-03T14:04:27.402357Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        try:\n            label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n            label['boxes'] = torch.Tensor(label['boxes'])\n            label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(image.permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            image, label = self.transform(image, label)\n\n        return image, label\n\nprint('ok')","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","id":"V1Q6ogjksMqE","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-03T13:54:48.831618Z","iopub.execute_input":"2024-06-03T13:54:48.832746Z","iopub.status.idle":"2024-06-03T13:54:48.848428Z","shell.execute_reply.started":"2024-06-03T13:54:48.832710Z","shell.execute_reply":"2024-06-03T13:54:48.847080Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\nfrom torchvision import tv_tensors\n\n# DATASET_DIR = os.path.join(\".\")\nTRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\nTEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\ntrain_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\ntrain_list, test_list = train_test_split(train_list, test_size = 0.97)\ntrain_list, val_list = train_test_split(train_list, test_size = 0.2)\ntest_list, _ = train_test_split(test_list, test_size = 0.99)\ntest_list, _ = train_test_split(test_list, test_size = 0.5)\n\ntrain_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\ntest_data = ShipsDataset(test_list, transforms = img_test_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\nval_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\ndef custom_collate_fn(batch):\n    # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n    # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n    # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n    \n    return batch\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\nval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n\n# Why custom_collate_fn?\n# pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n# Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n\nprint(len(train_data),len(train_loader))\nprint(len(val_data), len(val_loader))\nprint(len(test_loader))\n\n# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n# La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-06-03T14:04:31.403627Z","iopub.execute_input":"2024-06-03T14:04:31.404013Z","iopub.status.idle":"2024-06-03T14:06:27.412167Z","shell.execute_reply.started":"2024-06-03T14:04:31.403981Z","shell.execute_reply":"2024-06-03T14:06:27.411107Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"4620 145\n1156 37\n30\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save loaders\ntorch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\ntorch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\ntorch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\nprint('Dataset Loaders saved succesfully!')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:44:52.910849Z","iopub.execute_input":"2024-06-03T12:44:52.911183Z","iopub.status.idle":"2024-06-03T12:45:58.543722Z","shell.execute_reply.started":"2024-06-03T12:44:52.911156Z","shell.execute_reply":"2024-06-03T12:45:58.542921Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dataset Loaders saved succesfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T14:38:45.862357Z","iopub.execute_input":"2024-06-03T14:38:45.862824Z","iopub.status.idle":"2024-06-03T14:38:45.872083Z","shell.execute_reply.started":"2024-06-03T14:38:45.862787Z","shell.execute_reply":"2024-06-03T14:38:45.871201Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0893, 0.0827, 0.0817]) original size\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0879, 0.0811, 0.0800]) 224x224\n\n# image_mean_train, image_std_train = get_mean_std(train_loader)\n# image_mean_val, image_std_val = get_mean_std(val_loader)\n# image_mean_test, image_std_test = get_mean_std(test_loader)\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n\n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n    \n    case 'fourier':\n        \n        image_mean_train = torch.Tensor([0.0954, 0.0930, 0.0948])\n        image_std_train = torch.Tensor([0.1039, 0.1016, 0.1036])\n\n        image_mean_val = torch.Tensor([0.1925, 0.2728, 0.3091])\n        image_std_val = torch.Tensor([0.0881, 0.0804, 0.0795])\n\n        image_mean_test = torch.tensor([0.1985, 0.2808, 0.3167])\n        image_std_test = torch.tensor([0.0846, 0.0770, 0.0768])\n\nprint(f\"{image_mean_train = }, {image_std_train =}\")\nprint(f\"{image_mean_val = }, {image_std_val =}\")\nprint(f\"{image_mean_test = }, {image_std_test =}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T14:51:35.676159Z","iopub.execute_input":"2024-06-03T14:51:35.676603Z","iopub.status.idle":"2024-06-03T14:51:35.690496Z","shell.execute_reply.started":"2024-06-03T14:51:35.676569Z","shell.execute_reply":"2024-06-03T14:51:35.689500Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"image_mean_train = tensor([0.0954, 0.0930, 0.0948]), image_std_train =tensor([0.1039, 0.1016, 0.1036])\nimage_mean_val = tensor([0.1925, 0.2728, 0.3091]), image_std_val =tensor([0.0881, 0.0804, 0.0795])\nimage_mean_test = tensor([0.1985, 0.2808, 0.3167]), image_std_test =tensor([0.0846, 0.0770, 0.0768])\n","output_type":"stream"}]},{"cell_type":"code","source":"def new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\nmodel_rcnn = new_model()\n\nprint('ok')","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","id":"5J9M_bnAxnDk","execution":{"iopub.status.busy":"2024-06-03T12:45:58.633825Z","iopub.execute_input":"2024-06-03T12:45:58.634558Z","iopub.status.idle":"2024-06-03T12:46:00.777534Z","shell.execute_reply.started":"2024-06-03T12:45:58.634507Z","shell.execute_reply":"2024-06-03T12:46:00.776164Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 140MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")\n\nprint(\"ok\")","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","id":"Du5q6_RRCmD4","execution":{"iopub.status.busy":"2024-06-03T12:46:00.778927Z","iopub.execute_input":"2024-06-03T12:46:00.779334Z","iopub.status.idle":"2024-06-03T12:46:00.787044Z","shell.execute_reply.started":"2024-06-03T12:46:00.779301Z","shell.execute_reply":"2024-06-03T12:46:00.785896Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\n# import gc\nfrom torchvision.models.detection.transform import GeneralizedRCNNTransform\n\ndef train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=\"cpu\", start_from_epoch=0):\n        \n    model.transform.image_mean = image_mean_train\n    model.transform.image_std = image_std_train\n    model._skip_resize = True\n    \n    for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n        \n        training_loss = 0.0\n        batch_cumsum = 0\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n            logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n            print(f\"epoch {epoch} batch {i}\")\n            batch_cumsum += len(batch) # needed to compute the training loss later\n            optimizer.zero_grad()\n            \n            # filtering out empty images (model does not accept empty targets)\n            inputs = []\n            targets = []\n            for el in batch:       # el = ((image,dict),dict) when transforms are active\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0].to(device))\n                    targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n                    \n                    # print(f\"{el = }\")\n                    # Example el\n                    # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                    #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                    #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                    #          ...,\n                    #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                    #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                    #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                    #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                    #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                    #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                    #          ...,\n                    #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                    #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                    #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                    #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                    #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                    #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                    #          ...,\n                    #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                    #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                    #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                    #         [0.2331, 0.2643, 0.3268, 0.3060],\n                    #         [0.2435, 0.2995, 0.4062, 0.3724],\n                    #         [0.7188, 0.6198, 0.8281, 0.6784],\n                    #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n                    \n            if len(inputs) == 0:\n                continue\n         \n            output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n            \"\"\" EXAMPLE :\n            {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n             'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n             'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n             \n             How losses are computed:\n             \n             -loss_classifier-\n             classification_loss = F.cross_entropy(class_logits, labels)\n             \n             -loss_box_reg-\n             box_loss = F.smooth_l1_loss(\n                box_regression[sampled_pos_inds_subset, labels_pos],\n                regression_targets[sampled_pos_inds_subset],\n                beta=1 / 9,\n                reduction=\"sum\",\n            )\n            box_loss = box_loss / labels.numel()\n            \n            -loss_rpn_box_reg-\n            box_loss = F.smooth_l1_loss(\n            pred_bbox_deltas[sampled_pos_inds],\n            regression_targets[sampled_pos_inds],\n            beta=1 / 9,\n            reduction=\"sum\",\n            ) / (sampled_inds.numel())\n            \n            -loss_objectness-\n            objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n             \n             \"\"\"\n          \n            loss = sum(loss for loss in output.values())\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item() * len(batch)\n            \n#             del inputs\n#             del targets\n#             gc.collect()    \n        \n        lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n        scheduler.step() # changes LR\n        training_loss /= batch_cumsum\n        # save_checkpoint(epoch, model, optimizer, scheduler, training_loss, lrs)\n        \n        # VALIDATION\n        model.transform.image_mean = image_mean_val\n        model.transform.image_std = image_std_val\n        \n        model.train()\n        num_correct = 0\n        num_examples = 0\n        valid_loss = 0\n        \n        with torch.no_grad():\n            for i,batch in enumerate(val_loader):\n                print(\"batch\", i)\n                inputs = []\n                targets = []\n\n                for el in batch:       # el = (image,labels)\n                    if el[1]['boxes'].size()[0] != 0:\n                        inputs.append(el[0].to(device))\n                        targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n\n                if len(inputs) == 0:\n                    continue\n                \n                output = model(inputs, targets)\n\n                loss = sum(loss for loss in output.values())\n                valid_loss += loss.data.item() *len(batch)\n\n#                 del inputs\n#                 del targets\n#                 gc.collect()\n\n        valid_loss /= len(val_loader.dataset)\n        \n        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n        valid_loss, lrs[-1]))\n\n        logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n        valid_loss, lrs[-1]))\n        \n        \n        \n        save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs)\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5\nprint('ok')","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-06-03T14:08:40.515417Z","iopub.execute_input":"2024-06-03T14:08:40.515866Z","iopub.status.idle":"2024-06-03T14:08:40.542781Z","shell.execute_reply.started":"2024-06-03T14:08:40.515836Z","shell.execute_reply":"2024-06-03T14:08:40.541924Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optimizer,\n        lr_lambda = lambda epoch: 0.8 ** epoch,\n    )\n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }\")\n    print(f\"Beginning training, {num_epochs = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n    # plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che è stata creata contenente il modello e i log\n\nfrom IPython.display import FileLink\n!zip -r file.zip {model_filepath}\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:46:03.307181Z","iopub.status.idle":"2024-06-03T12:46:03.307767Z","shell.execute_reply.started":"2024-06-03T12:46:03.307472Z","shell.execute_reply":"2024-06-03T12:46:03.307501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchmetrics.detection import MeanAveragePrecision\nimport torchvision.transforms.functional as F\n\ndef test(model, test_loader, device=\"cpu\"): \n    \n    model.transform.image_mean,  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in enumerate(test_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = ((image,dict),dict)\n            if el[1]['boxes'].size()[0] != 0:\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n        # print(type(model(torch.cuda.FloatTensor(inputs))))\n        print(\"out :\\n\", output)\n        print(\"target :\\n\",targets)\n        #     # Example output\n        #     {'boxes': tensor([[\n        #       0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(torch.device(\"cpu\"))\n            dic[\"labels\"] = dic[\"labels\"].to(torch.device(\"cpu\"))\n            dic[\"scores\"] = dic[\"scores\"].to(torch.device(\"cpu\"))\n            \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:46:03.309546Z","iopub.status.idle":"2024-06-03T12:46:03.310057Z","shell.execute_reply.started":"2024-06-03T12:46:03.309794Z","shell.execute_reply":"2024-06-03T12:46:03.309815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# START MODEL TEST\n\ndo_model_test = False\n\nif do_model_test:\n    checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n    #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # ENF\n    # model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)\n\ncheckpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n#checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\nmodel.load_state_dict(checkpoint['model_state_dict'])\ntest(model.to(device), test_loader, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:46:03.311442Z","iopub.status.idle":"2024-06-03T12:46:03.311935Z","shell.execute_reply.started":"2024-06-03T12:46:03.311682Z","shell.execute_reply":"2024-06-03T12:46:03.311702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN AGAIN (Continue training)\n\nimport pickle\n\ntrain_again = True\n\nif train_again:    \n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'))\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'))\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'))\n    print(\"Loadeders and model loaded succesfully\") \n    \n#     print(f\"{device = }\")\n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"))\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optimizer,\n        lr_lambda = lambda epoch: 0.8 ** epoch,\n    )\n    \n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch']\n    \n    torch.compile(model)\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, lrs, validation_losses, training_losses, epochs=num_epochs, start_from_epoch=epoch)\n    \n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\")) # decimal scale -> the output is hard to read, instead try with this: \n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")\n\n    # Resume training from a specific epoch\n    # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    \n    # Il file salvato model.tar contiene optiimzer, scheduler, loss e tanto altro\n\n    # train(model, model.optimizer, model.scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T12:46:03.313300Z","iopub.status.idle":"2024-06-03T12:46:03.313705Z","shell.execute_reply.started":"2024-06-03T12:46:03.313496Z","shell.execute_reply":"2024-06-03T12:46:03.313513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{"_uuid":"ce6b85c5-91c9-488b-9ae0-74bd3514487f","_cell_guid":"93e16d46-9dc5-41c7-8837-52d4920e6149","id":"MLPxPQrile1o","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-21T07:41:13.044527Z","iopub.execute_input":"2024-05-21T07:41:13.045158Z","iopub.status.idle":"2024-05-21T07:41:13.054176Z","shell.execute_reply.started":"2024-05-21T07:41:13.045063Z","shell.execute_reply":"2024-05-21T07:41:13.052798Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}