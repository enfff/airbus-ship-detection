{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","\n","## MAIN CONFIGURATIONS\n","num_epochs = 18  # Number of epochs the model will train for\n","batch_size = 32\n","init_lr = 1e-4 # Initial Learning Rate\n","data_augmentation_type = 'nothing'\n","# Which data augmentation tecnique are we using?\n","# 'nothing': no transformations, only image resize\n","# 'geometric': basic geometric transforms\n","# 'patch_gaussian': adds a gaussian patch to each\n","# 'fourier_random_noise': currently doesn't work\n","# 'fourier_basis_augmentation'\n","                                        \n","train_percentage = 0.40 #  0.03 means 192_556 * 0.03 ~ 4.6k images in the training set\n","val_percentage = train_percentage/3\n","test_percentage = val_percentage\n","\n","assert (train_percentage + val_percentage + test_percentage) <= 1.0, \"Bad dataset split!\"\n","\n","# WHAT WILL THIS SESSION DO?\n","test_only = False # When True it doesn't train the model, but it just tests an existing one\n","train_again = False # Trains the model again for num_epoch times\n","do_model_test = True # Tests the model after training\n","create_log_file = True # self-explicatory\n","calculate_mean_std = True # Calculates the mean, and std of the datasets. When set to false, it uses the pre-calculated means, std.\n","print_images_during_training = False # self-explicatory\n","print_images_during_test = True # self-explicatory\n","save_dataset = True # Saves the dataset to a file\n","\n","model_filepath = f\"model_{data_augmentation_type}\"\n","model_filepath = os.path.join(\"models\", model_filepath)\n","print(f\"{model_filepath = }\")\n","\n","# !tree # Prints folder structure\n","os.makedirs(model_filepath, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.models as models\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import glob\n","import pandas as pd\n","from torchvision.io import read_image\n","from torchvision.transforms.functional import rotate\n","import numpy as np\n","import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# if do_model_test or test_only:\n","#     try:\n","#         from torchmetrics.detection import MeanAveragePrecision\n","# #         metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n","#     except:\n","#         !pip install torchmetrics\n","#         !pip install torchmetrics[detection]\n","#         !pip install pycocotools\n","#         !pip install faster-coco-eval\n","        \n","#         from torchmetrics.detection import MeanAveragePrecision\n","# #         metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n","\n","!pip install torchmetrics\n","!pip install torchmetrics[detection]\n","!pip install pycocotools\n","#!pip install faster-coco-eval"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","trusted":true},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"{device = }\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","trusted":true},"outputs":[],"source":["import logging\n","from datetime import datetime\n","\n","if create_log_file:\n","    log_filepath = \"\"\n","    logger = logging.getLogger('RootLogger')\n","    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n","    print(f\"{log_filepath = }\")\n","    \n","    logging.basicConfig(filename=log_filepath,\n","                        filemode='a',\n","                        format='%(asctime)s %(levelname)s %(message)s',\n","                        level=logging.INFO,\n","                        datefmt='%m-%d %H:%M:%S',\n","                        force=True)\n","else:\n","    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n","                        level=logging.INFO,\n","                        datefmt='%m-%d %H:%M:%S',\n","                        force=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch.fft as fft\n","import torchvision\n","import random\n","from torch import sin, cos\n","\n","class FourierRandomNoise(object):\n","        \n","    def __call__(self, *sample ):\n","        image = sample[0]\n","\n","        # Fourier Transform\n","        fourier = fft.rfftn(image)\n","        magnitude, angle = self.__polar_form(fourier)\n","\n","        # Apply Noise in the Frequency Domain\n","        noise = torch.rand(fourier.size())\n","        noised_magnitude = torch.mul(magnitude,noise)\n","\n","        # Inverse Fourier Transform\n","        fourier = self.__complex_form(noised_magnitude,angle)\n","        modified_image = fft.irfftn(fourier).byte()\n","        \n","        if len(sample) >= 2:\n","            label = sample[1]\n","            return modified_image, label\n","\n","        return modified_image\n","    \n","    def __polar_form(self, complex_tensor):\n","        return complex_tensor.abs(), complex_tensor.angle()\n","\n","    def __complex_form(self, magnitude, angle):\n","        return torch.polar(magnitude,angle)\n","    \n","\n","class PatchGaussian(object):\n","    \n","    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n","        '''\n","        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n","        The noise of the patch can be modified by specifying its variance\n","        '''\n","        \n","        image = sample[0]\n","        size = image.size()\n","#         # Scale the image in range [0,1)\n","#         min_val = 0\n","#         max_val = 255\n","#         image = (image-min_val)/(max_val-min_val)\n","\n","        # Define Gaussian patch\n","        patch = torch.empty(size).normal_(0,sigma_max)\n","        # Sample Corner Indices\n","        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n","        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n","        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n","        u = torch.stack([u,u,u])\n","        v = torch.stack([v,v,v])\n","        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n","        patch = mask*patch\n","        \n","        if len(sample) >= 2:\n","            label = sample[1]\n","            return torch.clip(image+patch,0,1), label\n","        \n","        return torch.clip(image+patch,0,1)\n","\n","class FourierBasisAugmentation(object):\n","    \n","    def __call__(self,*sample, l=0.3):\n","        '''\n","        Adds a Fourier Basis Function to the image\n","        '''\n","        image = sample[0]\n","        shape = image.size()\n","#         min_val = 0\n","#         max_val = 255\n","#         # Scale the image in range [0,1)\n","#         image = (image-min_val)/(max_val-min_val)\n","\n","        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n","        # where M is the size of the image\n","        f = (shape[1]-1)*torch.rand(3)\n","        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n","        w = (torch.pi-0)*torch.rand(3)\n","\n","        # Sample the decay parameter from a l-exponential distribution\n","        sigma = torch.distributions.Exponential(1/l).sample((3,))\n","\n","        # Generate basis function\n","        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n","        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n","        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n","        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n","        noise = torch.stack([basis_r,basis_g,basis_b])\n","\n","        # Modify The Image\n","        modified_image = image+noise\n","        \n","        if len(sample) >= 2:\n","            label = sample[1]\n","            return torch.clip(modified_image,0,1), label\n","\n","        return torch.clip(modified_image,0,1)\n","\n","print(\"ok\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test Transformation\n","\n","import torch\n","import torchvision\n","import torch.nn.functional as F\n","import torchvision.transforms.functional as TF\n","import numpy as np\n","\n","from torchvision import transforms\n","\n","class GaussianNoise:\n","    '''\n","    This corruption adds noise to the image\n","    '''\n","    def __init__(self, sigma: float = 0.1):\n","        self.__sigma = sigma\n","\n","    def __call__(self, image: torch.Tensor):\n","        shape = image.shape\n","        noise = torch.normal(0.0, self.__sigma, shape)\n","\n","        return torch.clip(\n","            image + noise,\n","            0.0,\n","            1.0\n","        )\n","    \n","class GaussianBlur:\n","    '''\n","    This corruption simulates the image being out of focus\n","    '''\n","    def __init__(self, kernel_size = 7, std = 5):\n","        self.__corruption = torchvision.transforms.GaussianBlur(kernel_size,std)\n","    \n","    def __call__(self, image: torch.Tensor):\n","        return self.__corruption(image)\n","    \n","class Fog:\n","    '''\n","    Add fog effect to the image\n","    '''\n","    def __init__(self, density: float = 0.5):\n","        self.__density = density\n","\n","    def __call__(self, image: torch.Tensor):\n","        shape = image.shape\n","        # Generate white fog\n","        fog = torch.ones(shape)*self.__density\n","        fogged_image = image*(1-fog) + fog\n","\n","        return torch.clip(\n","            fogged_image,\n","            0.0,\n","            1.0\n","        )\n","\n","class MotionBlur:\n","    '''\n","    Corruption that simulates motion blur\n","    '''\n","    def __init__(self, kernel_size = 13, angle = 45):\n","        # Initialize a kernel with zeros\n","        kernel = torch.zeros(kernel_size, kernel_size)\n","        \n","        # Convert angle from degrees to radians\n","        angle_rad = angle * torch.pi / 180.0\n","        \n","        # Calculate center position\n","        center = kernel_size // 2\n","        \n","        # Calculate the direction vector components\n","        dx = np.cos(angle_rad)\n","        dy = np.sin(angle_rad)\n","        \n","        # Ensure dx and dy are not zero to avoid division by zero\n","        if dx == 0 and dy == 0:\n","            dx = 1.0  # Set arbitrary non-zero value to avoid division by zero\n","            dy = 0.0\n","        \n","        # Set the central line of the kernel to 1\n","        start = center - kernel_size // 2\n","        end = center + kernel_size // 2 + 1\n","        \n","        if dx == 0:\n","            kernel[:, center] = 1.0\n","        elif dy == 0:\n","            kernel[center, :] = 1.0\n","        else:\n","            slope = dy / dx\n","            for i in range(start, end):\n","                j = int(round(center + slope * (i - center)))\n","                kernel[i, j] = 1.0\n","        \n","        # Normalize the kernel\n","        kernel /= kernel.sum()\n","        self.__kernel = kernel.unsqueeze(0).unsqueeze(0)\n","    \n","    def __call__(self, image: torch.Tensor):\n","        # Separate the layes to apply the kernel layer-wise\n","        R = F.conv2d(image[0].unsqueeze(0), self.__kernel, padding='same')\n","        G = F.conv2d(image[1].unsqueeze(0), self.__kernel, padding='same')\n","        B = F.conv2d(image[2].unsqueeze(0), self.__kernel, padding='same')\n","\n","        blurred_image = torch.stack([\n","            R.squeeze(),\n","            G.squeeze(),\n","            B.squeeze()\n","        ])\n","\n","        return torch.clip(\n","            blurred_image,\n","            0.0,\n","            1.0\n","        )\n","    \n","class Brightness:\n","    def __init__(self, brightness = 1.5):\n","        self.__birghtness = brightness\n","\n","    def __call__(self, image: torch.Tensor):\n","        bright_image = transforms.functional.adjust_brightness(image, self.__birghtness)\n","\n","        return torch.clip(\n","            bright_image,\n","            0.0,\n","            1.0\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","id":"LIgECtVqMlCI","trusted":true},"outputs":[],"source":["# TRANSFORMATIONS\n","\n","from torchvision.transforms import v2\n","\n","img_dimensions = 224\n","\n","transform_for_test = None\n","\n","# Please note: the normalization of the dataset happens later on the code\n","\n","match data_augmentation_type:\n","    \n","    case 'nothing':\n","        \n","        img_train_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n","        ])\n","        img_validation_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n","        ])\n","        img_test_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n","        ])\n","    \n","    case 'geometric':\n","        \n","        img_train_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.RandomPerspective(distortion_scale=0.5, p=0.5),\n","            v2.Pad(padding=30),\n","            v2.RandomVerticalFlip(p=0.5),\n","            v2.RandomHorizontalFlip(p=0.5),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","        ])\n","\n","        img_validation_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","        ])\n","\n","        img_test_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","        ])    \n","  \n","    case 'patch_gaussian':\n","        \n","        img_train_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            PatchGaussian(),\n","        ])\n","\n","        img_validation_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","        ])\n","\n","        img_test_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","        ])\n","        \n","    case 'fourier_random_noise':\n","        \n","        img_train_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            FourierRandomNoise(),\n","            ])\n","\n","        img_validation_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            ])\n","\n","        img_test_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            ])\n","        \n","    case 'fourier_basis_augmentation':\n","        \n","        img_train_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            FourierBasisAugmentation(),\n","            ])\n","\n","        img_validation_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            ])\n","\n","        img_test_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            ])\n","\n","print('ok')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","_kg_hide-input":true,"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","id":"V1Q6ogjksMqE","trusted":true},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as F\n","from torchvision.tv_tensors import BoundingBoxes\n","\n","# def show(imgs):\n","\n","#     if not isinstance(imgs, list):\n","#         imgs = [imgs]\n","#     fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n","#     for i, img in enumerate(imgs):\n","#         img = img.detach()\n","#         img = F.to_pil_image(img)\n","#         axs[0, i].imshow(np.asarray(img))\n","#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","\n","class ShipsDataset(torch.utils.data.Dataset):\n","    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n","        self.file_list = sorted(file_list, key = lambda f: f.split('/')[-1])\n","        self.targets = sorted(targets, key=lambda d: d['image_id'])\n","        self.transform = transforms\n","\n","    def __len__(self):\n","        self.filelength = len(self.file_list)\n","        return self.filelength\n","\n","    def __getitem__(self, idx):\n","        \n","        try:\n","            label = self.targets[idx]\n","        except:\n","            print(f\"Tried to access {idx = } which DOESN'T EXIST on rcnn_targets!\")\n","        \n","        assert self.file_list[idx].split('/')[-1] == label['image_id'], f\"Bounding Box mismatch for {idx = }, file: {self.file_list[idx].split('/')[-1]} and label: {label['image_id']}\"\n","        \n","        try:\n","            image = read_image(self.file_list[idx])    # numpy tensor\n","            image = F.convert_image_dtype(image, torch.float) # Images must be FloatTensor with values in [0, 1]\n","        except RuntimeError as e:\n","            Warning(f'Errore con {self.file_list[idx]}')\n","#             self.targets[idx]['labels'] = torch.tensor([])\n","            return None, self.targets[idx]\n","\n","      #  print(self.file_list[idx])\n","      #  print(self.targets[idx])\n","        \n","        try:\n","            label['boxes'] = BoundingBoxes(data=label['boxes'], format='XYXY', canvas_size=tuple(image.size()[-2:]))\n","        except IndexError as e:\n","            Warning(f'Errore con {idx = }')\n","            plt.imshow(F.convert_image_dtype(image).permute(1, 2, 0))\n","            plt.show()\n","\n","        if self.transform:\n","            if label['boxes'].numel():\n","                image, label = self.transform(image, label)\n","                # print(\"type of label:\", type(label))\n","            else:\n","                image = self.transform(image)\n","            \n","        return image, label\n","\n","print('ok')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","id":"YW9039lzlK5S","trusted":true},"outputs":[],"source":["from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","\n","def custom_collate_fn(batch):\n","    # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n","    # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n","    # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n","    return batch\n","\n","#if not test_only:\n","\n","from sklearn.model_selection import train_test_split\n","from torchvision import tv_tensors\n","from torch.utils.data import SubsetRandomSampler\n","\n","# DATASET_DIR = os.path.join(\".\")\n","TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n","TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n","# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n","\n","data_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n","ship_dataset_train = ShipsDataset(data_list, transforms = img_train_transforms, targets=torch.load('/kaggle/input/targets-rcnn/rcnn_targets.pt'))\n","ship_dataset_val = ShipsDataset(data_list, transforms = img_validation_transforms, targets=torch.load('/kaggle/input/targets-rcnn/rcnn_targets.pt'))\n","ship_dataset_test = ShipsDataset(data_list, transforms = img_test_transforms, targets=torch.load('/kaggle/input/targets-rcnn/rcnn_targets.pt'))\n","\n","# Fix the generator for reproducibility, remove once we understand that it works\n","generator = torch.Generator().manual_seed(42)\n","(\n","    train_dataset,\n","    _,\n","    _,\n","    _\n",") = torch.utils.data.random_split(ship_dataset_train, [train_percentage, val_percentage, test_percentage, 1 - train_percentage - val_percentage - test_percentage], generator)\n","\n","(\n","    _,\n","    val_dataset,\n","    _,\n","    _\n",") = torch.utils.data.random_split(ship_dataset_val, [train_percentage, val_percentage, test_percentage, 1 - train_percentage - val_percentage - test_percentage], generator)\n","\n","(\n","    _,\n","    _,\n","    test_dataset,\n","    _\n",") = torch.utils.data.random_split(ship_dataset_test, [train_percentage, val_percentage, test_percentage, 1 - train_percentage - val_percentage - test_percentage], generator)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n","\n","# print(f\"Whole dataset size: {len(ship_dataset)}\")\n","print(f\"train loader size: {len(train_loader)} batches ({train_percentage:.2f}%)\")\n","print(f\"validation loader size: {len(val_loader)} batches ({val_percentage:.2f}%)\")\n","print(f\"test loader size: {len(test_loader)} batches ({test_percentage:.2f}%)\")\n","\n","# logger.info(f\"Whole dataset size: {len(ship_dataset)}\")\n","logger.info(f\"train loader size: {len(train_loader)} batches ({train_percentage:.2f}%)\")\n","logger.info(f\"validation loader size: {len(val_loader)} batches ({val_percentage:.2f}%)\")\n","logger.info(f\"test loader size: {len(test_loader)} batches ({test_percentage:.2f}%)\")\n","\n","# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n","# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"\n","\n","print('ok')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if save_dataset:\n","    # Save loaders\n","    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n","    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n","    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n","\n","    print('Dataset Loaders saved succesfully!')\n","    logger.info(\"Finished saving Dataset Loaders\")\n","else:\n","    print('Skipping saving dataset')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_mean_std(loader):\n","    # Compute the mean and standard deviation of all pixels in the dataset  \n","    print(\"computing mean and std of this dataset split...\")\n","    \n","    nimages = 0\n","    mean = 0.\n","    var = 0.\n","    for i, batch in enumerate(loader):\n","        \n","        if i % 5 == 0:\n","            print(\"batch: \", i)\n","        \n","        inputs = []\n","        for el in batch:      \n","            inputs.append(el[0])\n","        batch = torch.stack(inputs, dim=0)\n","        # Rearrange batch to be the shape of [B, C, W * H]\n","        batch = batch.view(batch.size(0), batch.size(1), -1)\n","        # Update total number of images\n","        nimages += batch.size(0)\n","        # Compute mean and var\n","        mean += batch.mean(2).sum(0) \n","        var += batch.var(2).sum(0)\n","        \n","    mean /= nimages\n","    var /= nimages\n","    std = torch.sqrt(var)\n","    print(\"Done\")\n","    \n","    return mean, std\n","\n","\n","def new_model():\n","    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","    for module in model_rcnn.backbone.body.modules():\n","        if isinstance(module, nn.Conv2d):\n","            # Insert batch normalization after convolutional layers\n","            module = nn.Sequential(\n","                module,\n","                nn.BatchNorm2d(module.out_channels),\n","                nn.ReLU(inplace=True)\n","            )\n","\n","    for name, param in model_rcnn.named_parameters():\n","          param.requires_grad = False\n","\n","    num_classes = 2 # background, ship\n","    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n","    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    return model_rcnn\n","\n","\n","def save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, mean_std, model_name=\"model.tar\"):\n","    \"\"\"\n","        epoch: last trained epoch\n","    \"\"\"\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'training_losses': training_losses,\n","        'validation_losses': validation_losses,\n","        'lrs': lrs,\n","        'mean_std': mean_std\n","    }, os.path.join(model_filepath, model_name))\n","    \n","    print(\"Saved model\")\n","\n","\n","model_rcnn = new_model()\n","print(\"ok match data_augmentation_type, new_model, save_checkpoint\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Normalmente andrebbero calcolati rispetto il dataset. Per ora facciamo delle prove,\n","# e teniamo dati pre-calcolati (pur sapendo non siano accuratissimi)\n","\n","if calculate_mean_std:\n","    logger.info(\"Calculated correct mean, std\")\n","    image_mean_train, image_std_train = get_mean_std(train_loader)\n","    image_mean_val, image_std_val = get_mean_std(test_loader)\n","    image_mean_test, image_std_test = get_mean_std(val_loader)\n","else:\n","    logger.info(\"Using approximated mean, std\")\n","    match data_augmentation_type:\n","        case 'nothing':\n","            image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n","            image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n","\n","            image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n","            image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n","\n","            image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n","            image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n","\n","        case 'geometric':\n","            image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n","            image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n","\n","            image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n","            image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n","\n","            image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n","            image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n","\n","        case 'patch_gaussian':\n","            image_mean_train = torch.Tensor([0.0941, 0.0936, 0.0942])\n","            image_std_train = torch.Tensor([0.1021, 0.1032, 0.1025])\n","\n","            image_mean_val = torch.Tensor([0.0921, 0.0912, 0.0928])\n","            image_std_val = torch.Tensor([0.1022, 0.1025, 0.1026])\n","\n","            image_mean_test = torch.Tensor([0.0979, 0.0949, 0.0937])\n","            image_std_test = torch.Tensor([0.1061, 0.1042, 0.1011])\n","\n","        case 'fourier_random_noise':\n","            image_mean_train = torch.Tensor([0., 0., 0.])\n","            image_std_train = torch.Tensor([0., 0., 0.])\n","\n","            image_mean_val = torch.Tensor([0., 0., 0.])\n","            image_std_val = torch.Tensor([0., 0., 0.])\n","\n","            image_mean_test = torch.Tensor([0., 0., 0.])\n","            image_std_test = torch.Tensor([0., 0., 0.])\n","\n","        case 'fourier_basis_augmentation':\n","            image_mean_train = torch.Tensor([0.0913, 0.0944, 0.0934])\n","            image_std_train = torch.Tensor([0.0988, 0.1006, 0.1007])\n","\n","            image_mean_val = torch.Tensor([0.0949, 0.0938, 0.0963])\n","            image_std_val = torch.Tensor([0.1025, 0.1029, 0.1040])\n","\n","            image_mean_test = torch.Tensor([0.0940, 0.0942, 0.0947])\n","            image_std_test = torch.Tensor([0.1032, 0.1043, 0.1020])\n","\n","\n","print(f\"{image_mean_train = }, {image_std_train = }\")\n","print(f\"{image_mean_val = }, {image_std_val = }\")\n","print(f\"{image_mean_test = }, {image_std_test = }\")\n","print(f\"{data_augmentation_type = }\")\n","\n","logger.info(f\"{image_mean_train = }, {image_std_train = }\")\n","logger.info(f\"{image_mean_val = }, {image_std_val = }\")\n","logger.info(f\"{image_mean_test = }, {image_std_test = }\")\n","logger.info(f\"{data_augmentation_type = }\")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","id":"Mv8b06EulUK2","trusted":true},"outputs":[],"source":["# TRAIN\n","\n","import matplotlib.pyplot as plt\n","from torchvision.utils import draw_bounding_boxes\n","from torchvision.models.detection.transform import GeneralizedRCNNTransform\n","\n","def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n","\n","    model.transform.image_mean = image_mean_train\n","    model.transform.image_std = image_std_train\n","    model._skip_resize = True\n","\n","    for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n","\n","        training_loss = 0.0\n","        batch_cumsum = 0\n","        model.train()\n","\n","        for i, batch in enumerate(train_loader):\n","            logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n","#                 if i % 50 == 0:\n","#                     print(f\"epoch {epoch} batch {i}\")\n","            batch_cumsum += len(batch) # needed to compute the training loss later\n","            optimizer.zero_grad()\n","\n","            inputs = []\n","            targets = []\n","\n","            for el in batch:       # el = (image,dict) when transforms are active\n","\n","                el_dict = {\n","                    \"boxes\": el[1][\"boxes\"].to(device),\n","                    \"labels\": el[1][\"labels\"].to(device)\n","                }\n","\n","                if not el_dict[\"labels\"].numel():\n","                    # filtering out empty images (model does not accept empty targets)\n","                    continue\n","                else:\n","                  #  print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n","                  #  print(f'el_dict has {el_dict[\"boxes\"] = }')\n","\n","                    result = (el_dict[\"boxes\"][:, 0] == el_dict[\"boxes\"][:, 2]) | (el_dict[\"boxes\"][:, 1] == el_dict[\"boxes\"][:, 3])\n","                    if (result.any()):\n","                        print(\"Skipped an element because bboxes were aligned\")\n","                        continue\n","\n","                    image = el[0].to(device)\n","                    el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n","\n","                    inputs.append(image)\n","                    targets.append(el_dict)\n","\n","                    # Print images during training\n","                    if print_images_during_training:\n","                        num = len(el_dict[\"boxes\"])\n","\n","                        img = draw_bounding_boxes(\n","                            (image*256).byte(),\n","                            el_dict[\"boxes\"],\n","                            width = 1,\n","                            colors = 'yellow',\n","                            # font='arial',\n","                            font_size = 15\n","                        )\n","\n","                        fig, ax = plt.subplots()\n","                        fig.set_size_inches(16,9)\n","                        fig.tight_layout(pad=5)\n","                        ax.imshow(img.byte().permute(1, 2, 0))\n","                        plt.show()\n","                        plt.close()\n","\n","                    # print(f\"{el = }\")\n","                    # Example el\n","                    # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n","                    #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n","                    #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n","                    #          ...,\n","                    #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n","                    #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n","                    #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n","\n","                    #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n","                    #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n","                    #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n","                    #          ...,\n","                    #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n","                    #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n","                    #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n","\n","                    #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n","                    #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n","                    #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n","                    #          ...,\n","                    #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n","                    #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n","                    #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n","                    #         [0.2331, 0.2643, 0.3268, 0.3060],\n","                    #         [0.2435, 0.2995, 0.4062, 0.3724],\n","                    #         [0.7188, 0.6198, 0.8281, 0.6784],\n","                    #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n","\n","            if len(inputs) == 0:\n","                continue\n","\n","            output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n","\n","            \"\"\" EXAMPLE :\n","                {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n","                 'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n","                 'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n","\n","                 How losses are computed:\n","\n","                 -loss_classifier-\n","                 classification_loss = F.cross_entropy(class_logits, labels)\n","\n","                 -loss_box_reg-\n","                 box_loss = F.smooth_l1_loss(\n","                    box_regression[sampled_pos_inds_subset, labels_pos],\n","                    regression_targets[sampled_pos_inds_subset],\n","                    beta=1 / 9,\n","                    reduction=\"sum\",\n","                )\n","                box_loss = box_loss / labels.numel()\n","\n","                -loss_rpn_box_reg-\n","                box_loss = F.smooth_l1_loss(\n","                pred_bbox_deltas[sampled_pos_inds],\n","                regression_targets[sampled_pos_inds],\n","                beta=1 / 9,\n","                reduction=\"sum\",\n","                ) / (sampled_inds.numel())\n","\n","                -loss_objectness-\n","                objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n","\n","             \"\"\"\n","\n","            loss = sum(loss for loss in output.values())\n","            loss.backward()\n","            optimizer.step()\n","            training_loss += loss.data.item() * len(batch)\n","\n","        lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n","        scheduler.step() # changes LR\n","        training_loss /= batch_cumsum\n","        training_losses.append(training_loss)\n","\n","        # VALIDATION\n","        model.transform.image_mean = image_mean_val\n","        model.transform.image_std = image_std_val\n","\n","        model.train()\n","        num_correct = 0\n","        num_examples = 0\n","        valid_loss = 0\n","\n","        with torch.no_grad():\n","            for i,batch in enumerate(val_loader):\n","                if i % 20 == 0:\n","                    print(\"(VAL) batch\", i)\n","\n","                inputs = []\n","                targets = []\n","\n","                for el in batch:       # el = (image,labels)\n","\n","                    el_dict = {\n","                    \"boxes\": el[1][\"boxes\"].to(device),\n","                    \"labels\": el[1][\"labels\"].to(device)\n","                    }\n","\n","                    if not el_dict[\"labels\"].numel():\n","                        # filtering out empty images (model does not accept empty targets)\n","                        continue\n","                    else:\n","                    #    print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n","                    #    print(f'el_dict has {el_dict[\"boxes\"] = }')\n","\n","                        image = el[0].to(device)\n","                        el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n","\n","                        inputs.append(image)\n","                        targets.append(el_dict)\n","\n","\n","                    '''\n","                    if el[1]['boxes'].size()[0] != 0:\n","                        inputs.append(el[0].to(device))\n","                        targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n","                    '''\n","\n","                if len(inputs) == 0:\n","                    continue\n","\n","                output = model(inputs, targets)\n","\n","                loss = sum(loss for loss in output.values())\n","                valid_loss += loss.data.item() *len(batch)\n","\n","        valid_loss /= len(val_loader.dataset)\n","        validation_losses.append(valid_loss)\n","\n","        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.10f}'.format(epoch, training_loss,\n","        valid_loss, lrs[-1]))\n","\n","        logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.10f}'.format(epoch, training_loss,\n","        valid_loss, lrs[-1]))\n","\n","        save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, mean_std=[image_mean_train, image_std_train,image_mean_val, image_std_val, image_mean_test, image_std_test])\n","\n","print(f\"{data_augmentation_type = }\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#### START MODEL TRAINING\n","if not test_only:\n","    \n","    model = new_model()\n","    model.to(device)\n","    torch.compile(model)\n","    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n","\n","#     scheduler = torch.optim.lr_scheduler.StepLR(\n","#         optimizer,\n","#         gamma = 0.9,\n","#         step_size = 5,\n","#     )\n","\n","    scheduler = torch.optim.lr_scheduler.StepLR(\n","        optimizer,\n","        gamma = 7.5e-1,\n","        step_size = 1,\n","    )\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    \n","    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n","    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n","    print(f\"{device = }\")\n","    \n","    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n","    \n","# plots\n","#     fig, ax = plt.subplots()\n","#     ax.plot(lrs)    \n","#     ax.set(xlabel='epoch', ylabel='learning rate value')\n","#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n","#     print(f\"{lrs = }\")\n","#     logger.info(f\"{lrs = }\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fine_tuning = True\n","\n","if fine_tuning:\n","    model_dict = torch.load('/kaggle/working/'+model_filepath+'/model.tar')\n","    model = new_model()\n","    model.load_state_dict(model_dict['model_state_dict'])\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print(\"model is now using cuda\")\n","    \n","    for name, param in model.named_parameters():\n","        param.requires_grad = True\n","        \n","    model = model.to(device)\n","    optimizer = optim.Adam(params = model.parameters(), lr = 1e-6)\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(\n","        optimizer,\n","        lr_lambda = lambda epoch: 1.0 ** epoch,\n","    )\n","    criterion = nn.CrossEntropyLoss()\n","    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=5, device=device)\n","    \n","print('finished')\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torchvision.transforms.functional as F\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from torchvision.ops import batched_nms\n","\n","def test(model, test_loader, device=torch.device(\"cpu\")):\n","    # Normally targets should be None\n","    \n","    model.transform.image_mean  = image_mean_test\n","    model.transform.image_std = image_std_test\n","    model._skip_resize = True\n","    \n","    model.eval()\n","    num_correct = 0\n","    num_examples = 0\n","    test_loss = 0\n","    metric =  MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n","    mAP = 0    \n","    \n","    for i, batch in enumerate(test_loader):\n","        print(f\"Batch: {i}\")\n","        logger.info(f\"TEST, Batch; {i}\")\n","        \n","        inputs = []\n","        targets = []\n","        \n","        for el in batch:       # el = (image,dict)\n","            if el[0].numel() and el[1]['labels'].numel(): # We're considering non-empty elements\n","                inputs.append(el[0].to(device))\n","                targets.append(el[1])\n","                \n","        if len(inputs) == 0:\n","            continue\n","        \n","        output = model(inputs)\n","        \n","\n","        \"\"\"\n","        scores come from RoIHeads class:\n","        pred_scores = F.softmax(class_logits, -1)\n","        after deleting empy boxes, low scored boxes and applying non-max suppression\n","        \"\"\"\n","        \n","        output_nms = []\n","        \n","        for el in inputs:\n","            el.cpu()\n","        \n","        for dic in targets:\n","            dic[\"boxes\"] = dic[\"boxes\"].to(torch.device(\"cpu\"))\n","            dic[\"labels\"] = dic[\"labels\"].to(torch.device(\"cpu\"))\n","        \n","        for i,dic in enumerate(output):\n","            dic[\"boxes\"] = dic[\"boxes\"].to(torch.device(\"cpu\"))\n","            dic[\"labels\"] = dic[\"labels\"].to(torch.device(\"cpu\"))\n","            dic[\"scores\"] = dic[\"scores\"].to(torch.device(\"cpu\"))\n","            \n","            # performs non-maximum suppression \n","            idxs = batched_nms(dic[\"boxes\"],dic[\"scores\"],dic['labels'],0.2)\n","            \n","            dic_nms = {}\n","            list_boxes = []\n","            list_labels = []\n","            list_scores = []\n","            \n","            for n in idxs:\n","                if dic['scores'][n] > 0.6:\n","                    list_boxes.append(dic['boxes'][n])\n","                    list_labels.append(dic['labels'][n])\n","                    list_scores.append(dic['scores'][n])\n","            \n","            \n","            if len(list_boxes)>0:\n","                dic_nms['boxes'] = torch.vstack(list_boxes)\n","                dic_nms['labels'] = torch.tensor(list_labels)\n","                dic_nms['scores'] = torch.tensor(list_scores)\n","            else:\n","                dic_nms['boxes'] = torch.tensor([])\n","                dic_nms['labels'] = torch.tensor([])\n","                dic_nms['scores'] = torch.tensor([])\n","            \n","            output_nms.append(dic_nms)\n","           # print(dic_nms)\n","            \n","            if print_images_during_test:\n","                num = len(dic_nms[\"boxes\"])\n","                image = inputs[i]\n","                if not dic_nms['boxes'].numel():\n","                    img = (image*256).byte().cpu()\n","                else:\n","                    img = draw_bounding_boxes(\n","                        (image*256).byte().cpu(),\n","                        dic_nms[\"boxes\"],\n","                        width = 1,\n","                        colors = 'yellow',\n","                        # font='arial',\n","                        font_size = 15\n","                    )\n","\n","                fig, ax = plt.subplots()\n","                fig.set_size_inches(16,9)\n","                fig.tight_layout(pad=5)\n","                ax.imshow(img.byte().permute(1, 2, 0))\n","                plt.show()\n","                plt.close()\n","            \n","        \n","        torch.cuda.empty_cache()\n","        \n","        res = metric(output,targets)\n","        mAP += res['map_75']\n","        #print(res)\n","\n","        \n","    mAP /= len(test_loader)\n","    #log.info(f\"TEST, scored {mAP:.10f}\")\n","    print(f\"TEST, scored {mAP:.10f}\")\n","\n","print(\"ok\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# START MODEL TEST\n","\n","# if model:\n","#     del model\n","#     torch.cuda.empty_cache()\n","\n","\n","if do_model_test or test_only:\n","    checkpoint = torch.load('/kaggle/working/'+model_filepath+'/model.tar') # LUDO\n","    #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n","    # test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n","    \n","    model = new_model()\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print(\"model is now using cuda\")\n","\n","    test(model.to(device), test_loader, device)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":868324,"sourceId":9988,"sourceType":"competition"},{"datasetId":5181471,"isSourceIdPinned":true,"sourceId":8713159,"sourceType":"datasetVersion"},{"datasetId":5248227,"sourceId":8741262,"sourceType":"datasetVersion"},{"datasetId":5277501,"sourceId":8780114,"sourceType":"datasetVersion"},{"datasetId":5277502,"sourceId":8780115,"sourceType":"datasetVersion"},{"datasetId":5277505,"sourceId":8780118,"sourceType":"datasetVersion"},{"datasetId":5289535,"sourceId":8796721,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
