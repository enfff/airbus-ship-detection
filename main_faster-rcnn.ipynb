{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8585841,"sourceType":"datasetVersion","datasetId":5084559},{"sourceId":8649784,"sourceType":"datasetVersion","datasetId":5026303},{"sourceId":8650287,"sourceType":"datasetVersion","datasetId":5181471}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n## MAIN CONFIGURATIONS\nmodel_id = '1'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 10  # Number of epochs the model will train for\nbatch_size = 32\ndataset_percentage = 0.01 # Which percentage of the dataset to use. 0.03 means 4.6k images in the training\ninit_lr = 1e-4 # Initial Learning Rate\ndata_augmentation_type = 'none'    # Which data augmentation tecnique are we using?\n                                    # 'none' :     only image resize\n                                    # 'noaug':     basic geometric transforms\n                                    # 'fourier':   fourier transforms\n\n## WHAT WILL THIS SESSION DO?\ntest_only = False # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\ndo_model_test = False # Tests the model after training\ncreate_log_file = True\nplot_data = True # Plot training data of '{model_filepath}/model.tar'\nprint_images_during_training = False\n\n\nmodel_filepath = f\"model_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\n# !tree # Prints folder structure\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:25:17.722471Z","iopub.execute_input":"2024-06-10T07:25:17.722931Z","iopub.status.idle":"2024-06-10T07:25:17.733910Z","shell.execute_reply.started":"2024-06-10T07:25:17.722895Z","shell.execute_reply":"2024-06-10T07:25:17.732737Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_none_id1'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:25:17.736317Z","iopub.execute_input":"2024-06-10T07:25:17.736724Z","iopub.status.idle":"2024-06-10T07:25:25.239024Z","shell.execute_reply.started":"2024-06-10T07:25:17.736693Z","shell.execute_reply":"2024-06-10T07:25:25.237721Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install torchmetrics\n!pip install torchmetrics[detection]","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:25:25.240672Z","iopub.execute_input":"2024-06-10T07:25:25.241328Z","iopub.status.idle":"2024-06-10T07:25:57.754292Z","shell.execute_reply.started":"2024-06-10T07:25:25.241280Z","shell.execute_reply":"2024-06-10T07:25:57.752730Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2+cpu)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2+cpu)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nCollecting pycocotools>2.0.0 (from torchmetrics[detection])\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2+cpu)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-10T07:25:57.756420Z","iopub.execute_input":"2024-06-10T07:25:57.756948Z","iopub.status.idle":"2024-06-10T07:25:57.765121Z","shell.execute_reply.started":"2024-06-10T07:25:57.756905Z","shell.execute_reply":"2024-06-10T07:25:57.763881Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"device = device(type='cpu')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\n\nif log_filepath:\n    print(log_filepath)\nelse:\n    print(\"No logging\")","metadata":{"_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-06-10T07:25:57.769764Z","iopub.execute_input":"2024-06-10T07:25:57.770493Z","iopub.status.idle":"2024-06-10T07:25:57.786206Z","shell.execute_reply.started":"2024-06-10T07:25:57.770448Z","shell.execute_reply":"2024-06-10T07:25:57.784772Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_none_id1/log.txt'\nmodels/model_none_id1/log.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# FOURIER DATA AUGMENTATION\n\nimport torch.fft as fft\nimport torchvision\nimport random\n\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n        # Scale the image in range [0,1)\n        min_val = 0\n        max_val = 255\n        image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n        min_val = 0\n        max_val = 255\n        # Scale the image in range [0,1)\n        image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:25:57.788452Z","iopub.execute_input":"2024-06-10T07:25:57.788907Z","iopub.status.idle":"2024-06-10T07:25:57.827537Z","shell.execute_reply.started":"2024-06-10T07:25:57.788869Z","shell.execute_reply":"2024-06-10T07:25:57.825262Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n        \n        img_train_transforms = v2.Compose([\n             v2.RandomRotation(50),\n             v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n             v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n    \n    case 'fourier':\n        \n        img_train_transforms = v2.Compose([\n             FourierRandomNoise(),\n             PatchGaussian(),\n             FourierBasisAugmentation(),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n    \n    case 'none':\n        img_train_transforms = v2.Compose([\n                v2.Resize((img_dimensions, img_dimensions)),\n                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n            ])\n        img_validation_transforms = v2.Compose([\n                v2.Resize((img_dimensions, img_dimensions)),\n                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n            ])\n        img_test_transforms = v2.Compose([\n                v2.Resize((img_dimensions, img_dimensions)),\n                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n            ])\n\nprint('ok')","metadata":{"_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-06-10T07:25:57.829282Z","iopub.execute_input":"2024-06-10T07:25:57.829740Z","iopub.status.idle":"2024-06-10T07:25:58.119856Z","shell.execute_reply.started":"2024-06-10T07:25:57.829699Z","shell.execute_reply":"2024-06-10T07:25:58.118295Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = sorted(file_list, key = lambda f: f.split('/')[-1])\n        self.targets = sorted(targets, key=lambda d: d['image_id'])\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        \n        try:\n            image = read_image(self.file_list[idx])    # numpy tensor\n        except RuntimeError as e:\n            Warning(f'Errore con {self.file_list[idx]}')\n            self.targets[idx]['labels'] = torch.tensor([])\n            return None, self.targets[idx]\n        \n        image = F.convert_image_dtype(image)\n        \n      #  print(self.file_list[idx])\n      #  print(self.targets[idx])\n        \n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        try:\n            label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n            label['boxes'] = torch.Tensor(label['boxes'])\n            label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(image.permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            image, label = self.transform(image, label)\n\n        return image, label\n\nprint('ok')","metadata":{"_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","_kg_hide-input":true,"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","id":"V1Q6ogjksMqE","execution":{"iopub.status.busy":"2024-06-10T07:25:58.121670Z","iopub.execute_input":"2024-06-10T07:25:58.122128Z","iopub.status.idle":"2024-06-10T07:25:58.147721Z","shell.execute_reply.started":"2024-06-10T07:25:58.122087Z","shell.execute_reply":"2024-06-10T07:25:58.146277Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"if not test_only:\n\n    from sklearn.model_selection import train_test_split\n    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n    from torchvision import tv_tensors\n    from torch.utils.data import SubsetRandomSampler\n\n    # DATASET_DIR = os.path.join(\".\")\n    TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n    TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n    # print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\n    data_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n#     train_list, test_list = train_test_split(train_list, test_size = float(1 - dataset_percentage)) # check first cell\n#     train_list, val_list = train_test_split(train_list, test_size = 0.2)\n#     test_list, _ = train_test_split(test_list, test_size = 0.99)\n#     test_list, _ = train_test_split(test_list, test_size = 0.5)\n    \n   # train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/targets-rcnn/rcnn_targets_modified.npy', allow_pickle='TRUE'))\n    ship_dataset = ShipsDataset(data_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/targets-rcnn/rcnn_targets_modified.npy', allow_pickle='TRUE'))\n#     test_data = ShipsDataset(test_list, transforms = img_test_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n#     val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\n    def custom_collate_fn(batch):\n        # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n        # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n        # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n\n        return batch\n\n    # Fix the generator for reproducibility, remove once we understand that it works\n   # generator = torch.Generator().manual_seed(42)\n    # Split the dataset into train and test\n #   train_dataset, test_dataset = torch.utils.data.random_split(ship_dataset, [0.7, 0.3], generator)\n#    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n #   test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n#     val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n#     test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n     \n    dataset_size = 192_556\n    indices = list(range(0, dataset_size))\n    np.random.shuffle(indices)\n    test_split_index = int(np.floor(0.1 * dataset_size))\n    train_idx, test_idx = indices[test_split_index:], indices[:test_split_index]\n    \n    val_split_index = int(np.floor(0.2 * len(train_idx)))\n    train_idx, val_idx =  train_idx[val_split_index:], train_idx[:val_split_index]\n\n    train_sampler = SubsetRandomSampler(train_idx)\n    train_loader = torch.utils.data.DataLoader(dataset = ship_dataset, batch_size = batch_size, shuffle = False, sampler = train_sampler, collate_fn=custom_collate_fn)\n    \n    val_sampler = SubsetRandomSampler(val_idx)\n    val_loader = torch.utils.data.DataLoader(dataset = ship_dataset, batch_size = batch_size, shuffle = False, sampler = val_sampler, collate_fn=custom_collate_fn)\n    \n    test_sampler = SubsetRandomSampler(test_idx)\n    test_loader = torch.utils.data.DataLoader(dataset = ship_dataset, batch_size = batch_size, shuffle = False, sampler = test_sampler, collate_fn=custom_collate_fn)\n\n     \n    print(f\"{int(np.floor(len(train_loader)/batch_size))} batch TRAINING, {len(train_loader)} images\")\n    print(f\"{int(np.floor(len(val_loader)/batch_size))} batch VALIDATION, {len(val_loader)} images\")\n    print(f\"{int(np.floor(len(test_loader)/batch_size))} batch TEST, {len(test_loader)} images\")\n\n\n    # https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n    # La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n    # /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"\n\nprint('ok')","metadata":{"_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-06-10T07:38:00.114639Z","iopub.execute_input":"2024-06-10T07:38:00.115062Z","iopub.status.idle":"2024-06-10T07:38:03.287013Z","shell.execute_reply.started":"2024-06-10T07:38:00.115028Z","shell.execute_reply":"2024-06-10T07:38:03.285883Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"135 batch TRAINING, 4333 images\n33 batch VALIDATION, 1084 images\n18 batch TEST, 602 images\nok\n","output_type":"stream"}]},{"cell_type":"code","source":" if not test_only:\n    # Save loaders\n    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\n    print('Dataset Loaders saved succesfully!')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:05.563625Z","iopub.execute_input":"2024-06-10T07:26:05.563987Z","iopub.status.idle":"2024-06-10T07:26:05.569508Z","shell.execute_reply.started":"2024-06-10T07:26:05.563958Z","shell.execute_reply":"2024-06-10T07:26:05.567990Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\ndef get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:05.570954Z","iopub.execute_input":"2024-06-10T07:26:05.571306Z","iopub.status.idle":"2024-06-10T07:26:05.606858Z","shell.execute_reply.started":"2024-06-10T07:26:05.571276Z","shell.execute_reply":"2024-06-10T07:26:05.605455Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0893, 0.0827, 0.0817]) original size\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0879, 0.0811, 0.0800]) 224x224\n\n# image_mean_train, image_std_train = get_mean_std(train_loader)\n# image_mean_val, image_std_val = get_mean_std(val_loader)\n# image_mean_test, image_std_test = get_mean_std(test_loader)\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n\n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n    \n    case 'fourier':\n        \n        image_mean_train = torch.Tensor([0.0954, 0.0930, 0.0948])\n        image_std_train = torch.Tensor([0.1039, 0.1016, 0.1036])\n\n        image_mean_val = torch.Tensor([0.1925, 0.2728, 0.3091])\n        image_std_val = torch.Tensor([0.0881, 0.0804, 0.0795])\n\n        image_mean_test = torch.tensor([0.1985, 0.2808, 0.3167])\n        image_std_test = torch.tensor([0.0846, 0.0770, 0.0768])\n    \n  #  case 'none':\n        \n       # image_mean_train, image_std_train = get_mean_std(train_loader)\n        \n\n\n\nimage_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\nimage_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\nimage_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\nimage_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\nimage_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\nimage_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n\nprint(f\"{image_mean_train = }, {image_std_train =}\")\nprint(f\"{image_mean_val = }, {image_std_val =}\")\nprint(f\"{image_mean_test = }, {image_std_test =}\")\nprint(f\"{data_augmentation_type}\")\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:05.608452Z","iopub.execute_input":"2024-06-10T07:26:05.608855Z","iopub.status.idle":"2024-06-10T07:26:05.736716Z","shell.execute_reply.started":"2024-06-10T07:26:05.608823Z","shell.execute_reply":"2024-06-10T07:26:05.735519Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"image_mean_train = tensor([0.1543, 0.2125, 0.2388]), image_std_train =tensor([0.1429, 0.1588, 0.1657])\nimage_mean_val = tensor([0.1541, 0.2128, 0.2395]), image_std_val =tensor([0.1415, 0.1594, 0.1676])\nimage_mean_test = tensor([0.2114, 0.2936, 0.3265]), image_std_test =tensor([0.0816, 0.0745, 0.0731])\nnone\nok\n","output_type":"stream"}]},{"cell_type":"code","source":"def new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\nmodel_rcnn = new_model()\n\nprint('ok')","metadata":{"_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","id":"5J9M_bnAxnDk","execution":{"iopub.status.busy":"2024-06-10T07:26:05.740870Z","iopub.execute_input":"2024-06-10T07:26:05.741238Z","iopub.status.idle":"2024-06-10T07:26:08.272227Z","shell.execute_reply.started":"2024-06-10T07:26:05.741208Z","shell.execute_reply":"2024-06-10T07:26:08.270753Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 121MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")\n\nprint(\"ok\")","metadata":{"_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","id":"Du5q6_RRCmD4","execution":{"iopub.status.busy":"2024-06-10T07:26:08.273484Z","iopub.execute_input":"2024-06-10T07:26:08.273818Z","iopub.status.idle":"2024-06-10T07:26:08.282697Z","shell.execute_reply.started":"2024-06-10T07:26:08.273791Z","shell.execute_reply":"2024-06-10T07:26:08.281379Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes\n\nif not test_only:\n    \n    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n\n    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n\n        model.transform.image_mean = image_mean_train\n        model.transform.image_std = image_std_train\n        model._skip_resize = True\n\n        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n\n            training_loss = 0.0\n            batch_cumsum = 0\n            model.train()\n\n            for i, batch in enumerate(train_loader):\n                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n                print(f\"epoch {epoch} batch {i}\")\n                batch_cumsum += len(batch) # needed to compute the training loss later\n                optimizer.zero_grad()\n                \n                inputs = []\n                targets = []\n                \n                for el in batch:       # el = (image,dict) when transforms are active\n                    \n                    el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                    }\n                    \n                    if not el_dict[\"labels\"].numel():\n                        # filtering out empty images (model does not accept empty targets)\n                        continue\n                    else:\n                      #  print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                      #  print(f'el_dict has {el_dict[\"boxes\"] = }')\n                        \n                        image = el[0].to(device)\n                        el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n                        \n                        inputs.append(image)\n                        targets.append(el_dict)\n                        \n                        # Print images during training\n                        if print_images_during_training:\n                            num = len(el_dict[\"boxes\"])\n\n                            img = draw_bounding_boxes(\n                                (image*256).byte(),\n                                el_dict[\"boxes\"]*224,\n                                width = 1,\n                                colors = 'yellow',\n                                # font='arial',\n                                font_size = 15\n                            )\n\n                            fig, ax = plt.subplots()\n                            fig.set_size_inches(16,9)\n                            fig.tight_layout(pad=5)\n                            ax.imshow(img.byte().permute(1, 2, 0))\n                            plt.show()\n                            plt.close()\n\n                        # print(f\"{el = }\")\n                        # Example el\n                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                        #          ...,\n                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                        #          ...,\n                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                        #          ...,\n                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n\n                if len(inputs) == 0:\n                    continue\n\n                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n                \"\"\" EXAMPLE :\n                    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n                     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n                     'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n\n                     How losses are computed:\n\n                     -loss_classifier-\n                     classification_loss = F.cross_entropy(class_logits, labels)\n\n                     -loss_box_reg-\n                     box_loss = F.smooth_l1_loss(\n                        box_regression[sampled_pos_inds_subset, labels_pos],\n                        regression_targets[sampled_pos_inds_subset],\n                        beta=1 / 9,\n                        reduction=\"sum\",\n                    )\n                    box_loss = box_loss / labels.numel()\n\n                    -loss_rpn_box_reg-\n                    box_loss = F.smooth_l1_loss(\n                    pred_bbox_deltas[sampled_pos_inds],\n                    regression_targets[sampled_pos_inds],\n                    beta=1 / 9,\n                    reduction=\"sum\",\n                    ) / (sampled_inds.numel())\n\n                    -loss_objectness-\n                    objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n                 \"\"\"\n\n                loss = sum(loss for loss in output.values())\n                loss.backward()\n                optimizer.step()\n                training_loss += loss.data.item() * len(batch)\n\n            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n            scheduler.step() # changes LR\n            training_loss /= batch_cumsum\n            training_losses.append(training_loss)\n            # save_checkpoint(epoch, model, optimizer, scheduler, training_loss, lrs)\n\n            # VALIDATION\n            model.transform.image_mean = image_mean_val\n            model.transform.image_std = image_std_val\n\n            model.train()\n            num_correct = 0\n            num_examples = 0\n            valid_loss = 0\n\n            with torch.no_grad():\n                for i,batch in enumerate(val_loader):\n                    print(\"batch\", i)\n                    inputs = []\n                    targets = []\n\n                    for el in batch:       # el = (image,labels)\n                        \n                        el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                        }\n\n                        if not el_dict[\"labels\"].numel():\n                            # filtering out empty images (model does not accept empty targets)\n                            continue\n                        else:\n                        #    print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                        #    print(f'el_dict has {el_dict[\"boxes\"] = }')\n\n                            image = el[0].to(device)\n                            el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n\n                            inputs.append(image)\n                            targets.append(el_dict)\n            \n                        \n                        '''\n                        if el[1]['boxes'].size()[0] != 0:\n                            inputs.append(el[0].to(device))\n                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n                        '''\n                   \n                    if len(inputs) == 0:\n                        continue\n\n                    output = model(inputs, targets)\n\n                    loss = sum(loss for loss in output.values())\n                    valid_loss += loss.data.item() *len(batch)\n\n            valid_loss /= len(val_loader.dataset)\n            validation_losses.append(valid_loss)\n\n            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs)\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5\nprint('ok')","metadata":{"_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-06-10T07:26:08.284422Z","iopub.execute_input":"2024-06-10T07:26:08.285024Z","iopub.status.idle":"2024-06-10T07:26:08.322493Z","shell.execute_reply.started":"2024-06-10T07:26:08.284971Z","shell.execute_reply":"2024-06-10T07:26:08.321220Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n    # plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:08.324045Z","iopub.execute_input":"2024-06-10T07:26:08.324484Z","iopub.status.idle":"2024-06-10T07:26:29.678818Z","shell.execute_reply.started":"2024-06-10T07:26:08.324450Z","shell.execute_reply":"2024-06-10T07:26:29.677090Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Beginning training, num_epochs = 10, data_augmentation_type = 'none', batch_size = 32\ndevice = device(type='cpu')\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"epoch 0 batch 0\n","output_type":"stream"},{"name":"stderr","text":"Process ForkProcess-2:\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nProcess ForkProcess-1:\nProcess ForkProcess-4:\nProcess ForkProcess-3:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n    res = self._recv_bytes()\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n    buf = self._recv(4)\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning training, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_augmentation_type\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# plots\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     fig, ax = plt.subplots()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     ax.plot(lrs)    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     print(f\"{lrs = }\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     logger.info(f\"{lrs = }\")\u001b[39;00m\n","Cell \u001b[0;32mIn[17], line 104\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs, validation_losses, training_losses, epochs, device, start_from_epoch)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# NOTE: output is a dict with already computed losses within!\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" EXAMPLE :\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m \"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mvalues())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m             degen_bb: List[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m boxes[bb_idx]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     95\u001b[0m             torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[1;32m     96\u001b[0m                 \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll bounding boxes should have positive height and width.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found invalid box \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdegen_bb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for target at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m             )\n\u001b[0;32m--> 101\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/backbone_utils.py:58\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[1;32m     57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody(x)\n\u001b[0;32m---> 58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/ops/feature_pyramid_network.py:196\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m     inner_top_down \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(last_inner, size\u001b[38;5;241m=\u001b[39mfeat_shape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m     last_inner \u001b[38;5;241m=\u001b[39m inner_lateral \u001b[38;5;241m+\u001b[39m inner_top_down\n\u001b[0;32m--> 196\u001b[0m     results\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_from_layer_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     results, names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_blocks(results, x, names)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/ops/feature_pyramid_network.py:169\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.get_result_from_layer_blocks\u001b[0;34m(self, x, idx)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_blocks):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m idx:\n\u001b[0;32m--> 169\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Plot data\n\n# if plot_data:\n    \n#     import matplotlib.pyplot as plt\n    \n#     pass","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:29.679934Z","iopub.status.idle":"2024-06-10T07:26:29.680328Z","shell.execute_reply.started":"2024-06-10T07:26:29.680141Z","shell.execute_reply":"2024-06-10T07:26:29.680156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che è stata creata contenente il modello e i log\n\n# from IPython.display import FileLink\n# !zip -r file.zip {model_filepath}\n# FileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:29.682552Z","iopub.status.idle":"2024-06-10T07:26:29.682979Z","shell.execute_reply.started":"2024-06-10T07:26:29.682773Z","shell.execute_reply":"2024-06-10T07:26:29.682796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchmetrics.detection import MeanAveragePrecision\nimport torchvision.transforms.functional as F\n\ndef test(model, test_loader, device=torch.device(\"cpu\")): \n    \n    model.transform.image_mean  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in enumerate(test_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,dict)\n            if el[0] != None and not el[1]['labels'].numel() :\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n        # print(type(model(torch.cuda.FloatTensor(inputs))))\n#         print(\"out :\\n\", output)\n#         print(\"target :\\n\",targets)\n        #     # Example output\n        #     {'boxes': tensor([[\n        #       0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        \n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(device)\n            dic[\"labels\"] = dic[\"labels\"].to(device)\n            dic[\"scores\"] = dic[\"scores\"].to(device)\n            \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:29.685605Z","iopub.status.idle":"2024-06-10T07:26:29.686170Z","shell.execute_reply.started":"2024-06-10T07:26:29.685896Z","shell.execute_reply":"2024-06-10T07:26:29.685920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n\nif do_model_test or test_only:\n#     checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n    test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n    \n    model = new_model()\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)\n\n# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n# #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\n# model.load_state_dict(checkpoint['model_state_dict'])\n# test(model.to(device), test_loader, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:29.688147Z","iopub.status.idle":"2024-06-10T07:26:29.688722Z","shell.execute_reply.started":"2024-06-10T07:26:29.688440Z","shell.execute_reply":"2024-06-10T07:26:29.688462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN AGAIN (Continue training)\n\nimport pickle\n\ntrain_again = True\n\nif train_again:    \n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n    print(\"Loadeders and model loaded succesfully\") \n    \n#     print(f\"{device = }\")\n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch']\n    # Resume training from a specific epoch\n    # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    \n    # Il file salvato model.tar contiene optiimzer, scheduler, loss e tanto altro\n\n    logger.info(f\"Continuing training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T07:26:29.690382Z","iopub.status.idle":"2024-06-10T07:26:29.690933Z","shell.execute_reply.started":"2024-06-10T07:26:29.690648Z","shell.execute_reply":"2024-06-10T07:26:29.690670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n\nsimo e' intelligente\n\nsimo e' intelligente, \n\nsimo e' buono, ludo e' bella,\nenf e' attraente, simo e' buonosimo e' intelligente, \nludo e' intelligente\n\nsimo e' intelligente\nludo e' buona\n\nenf e' attraente\n\nludo e' buona, \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{}}]}