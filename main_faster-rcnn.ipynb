{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8438193,"sourceType":"datasetVersion","datasetId":5026303}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# MAIN CONFIGURATIONS\ncreate_log_file = True\nsave_to_drive = False\nmodel_id = '0'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 10\nbatch_size = 32\ndata_augmentation_type = 'noaug'  # Which data augmentation tecnique are we using?\n                                  # 'noaug':     no data augmentation\n\n# !tree # Prints folder structure\n\ntest_only = False # when true it doesn't train the model, but it just tests an existing one\n\nmodel_filepath = f\"model_epochs{str(num_epochs)}_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"_uuid":"f66e8f23-9459-4a78-b42d-592a8b72fd93","_cell_guid":"1eb3601c-59c5-4c49-8c24-91184a27a7e1","collapsed":false,"id":"NFc7Y_31k39Q","outputId":"13bf00d1-bde8-49aa-b24b-2e49f08ab1ab","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:30:26.145739Z","iopub.execute_input":"2024-05-19T08:30:26.146366Z","iopub.status.idle":"2024-05-19T08:30:26.161736Z","shell.execute_reply.started":"2024-05-19T08:30:26.146308Z","shell.execute_reply":"2024-05-19T08:30:26.160641Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_epochs10_noaug_id0'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\n\n# !pip install torchsummary\n# from torchsummary import summary","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","collapsed":false,"id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:30:26.163589Z","iopub.execute_input":"2024-05-19T08:30:26.164358Z","iopub.status.idle":"2024-05-19T08:30:26.179238Z","shell.execute_reply.started":"2024-05-19T08:30:26.164320Z","shell.execute_reply":"2024-05-19T08:30:26.176194Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    logger = logging.getLogger('RootLogger')\n    log_filepath = datetime.now().strftime(\"%m-%d_%H.%M.%S\")\n    log_filepath = os.path.join(model_filepath, f\"log_{log_filepath}\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","collapsed":false,"id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:30:26.181696Z","iopub.execute_input":"2024-05-19T08:30:26.183035Z","iopub.status.idle":"2024-05-19T08:30:26.200962Z","shell.execute_reply.started":"2024-05-19T08:30:26.182971Z","shell.execute_reply":"2024-05-19T08:30:26.199542Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_epochs10_noaug_id0/log_05-19_08.30.26.txt'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\n# Normalize to the ImageNet mean and standard deviation\n# Could calculate it for the cats/dogs data set, but the ImageNet\n# values give acceptable results here.\nimg_train_transforms = v2.Compose([\n    # transforms.RandomRotation(50),\n    # transforms.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n    # transforms.RandomHorizontalFlip(p=0.5),\n    v2.Resize((img_dimensions, img_dimensions)),\n    # transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])\n\nimg_validation_transforms = v2.Compose([\n    v2.Resize((img_dimensions, img_dimensions)),\n    # transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","collapsed":false,"id":"LIgECtVqMlCI","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:30:26.203360Z","iopub.execute_input":"2024-05-19T08:30:26.203790Z","iopub.status.idle":"2024-05-19T08:30:26.219488Z","shell.execute_reply.started":"2024-05-19T08:30:26.203747Z","shell.execute_reply":"2024-05-19T08:30:26.217042Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      # mask[0, r, c+j] = 1\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        # try:\n        label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n        label['boxes'] = torch.Tensor(label['boxes'])\n        label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        # except IndexError as e:\n        #     Warning(f'Errore con {idx = }')\n        #     plt.imshow(image.permute(1, 2, 0))\n        #     plt.show()\n\n        if self.transform:\n            image = self.transform(image, label)\n\n            # prova ad indagare da qui\n            # image = self.transform(image)\n            # image = image.numpy()\n            # return image, label\n            # print(f\"{image = }\")\n            # print(f\"{label = }\")\n\n        return image, label","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","collapsed":false,"id":"V1Q6ogjksMqE","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:30:26.221455Z","iopub.execute_input":"2024-05-19T08:30:26.222669Z","iopub.status.idle":"2024-05-19T08:30:26.244085Z","shell.execute_reply.started":"2024-05-19T08:30:26.222621Z","shell.execute_reply":"2024-05-19T08:30:26.241407Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision import tv_tensors\n\n# DATASET_DIR = os.path.join(\".\")\nTRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\nTEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\ntrain_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\ntrain_list, test_list = train_test_split(train_list, test_size = 0.99)\ntrain_list, val_list = train_test_split(train_list, test_size = 0.2)\ntest_list, _ = train_test_split(test_list, test_size = 0.7)\n\n\n# train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('rcnn_targets.npy', allow_pickle='TRUE'))\n# # test_data = ShipsDataset(train_list, transforms = img_train_transforms)\n# val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\ntest_data = ShipsDataset(test_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\nval_data = ShipsDataset(val_list, transforms = img_train_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\nval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\n\nprint(len(train_data),len(train_loader))\nprint(len(val_data), len(val_loader))\n\nmodel_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n\n# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n# La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","collapsed":false,"id":"YW9039lzlK5S","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:30:26.247352Z","iopub.execute_input":"2024-05-19T08:30:26.248060Z","iopub.status.idle":"2024-05-19T08:32:31.113755Z","shell.execute_reply.started":"2024-05-19T08:30:26.248023Z","shell.execute_reply":"2024-05-19T08:32:31.112505Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"1540 49\n385 13\n","output_type":"stream"}]},{"cell_type":"code","source":"## STEP 1. freeze backbone layers, add final layers and train the network\n\nfor name, param in model_rcnn.named_parameters():\n      param.requires_grad = False\n\nnum_classes = 2 # background, ship\nin_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\nmodel_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","collapsed":false,"id":"5J9M_bnAxnDk","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:32:31.115665Z","iopub.execute_input":"2024-05-19T08:32:31.116868Z","iopub.status.idle":"2024-05-19T08:32:31.124058Z","shell.execute_reply.started":"2024-05-19T08:32:31.116831Z","shell.execute_reply":"2024-05-19T08:32:31.122919Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"\n# How to save in google drive something else\n# if save_to_drive:\n#   with open('/content/drive/MyDrive/MLVM_project/file.txt', 'w') as f:\n#     f.write('content')\n\nprint(f\"{model_filepath = }\")\n\ndef save_checkpoint(epoch, model, optimizer, train_loss, val_loss=0, model_name=\"model.tar\"):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","collapsed":false,"id":"Du5q6_RRCmD4","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:32:31.125867Z","iopub.execute_input":"2024-05-19T08:32:31.126697Z","iopub.status.idle":"2024-05-19T08:32:31.140400Z","shell.execute_reply.started":"2024-05-19T08:32:31.126663Z","shell.execute_reply":"2024-05-19T08:32:31.138874Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_epochs10_noaug_id0'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\nimport gc\n\ndef train(model, optimizer, loss_fn, train_loader, val_loader, epochs=1, device=\"cpu\"):\n \n    for epoch in range(epochs):\n        training_loss = 0.0\n        batch_cumsum = 0\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n            logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n            print(f\"epoch {epoch} batch {i}\")\n            batch_cumsum += len(batch) # needed to compute the training loss later\n            optimizer.zero_grad()\n            # inputs, targets = batch\n            \"\"\" inputs = [img for i,el in enumerate(batch)]\n            targets = [lab for img,lab in batch] \"\"\"\n\n            # filtering out empty images (model does not accept empty targets)\n            inputs = []\n            targets = []\n            for el in batch:       # el = (image,labels)\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0][0])\n                    targets.append(el[0][1])\n                    # print(f\"{el = }\")\n                    # Example el\n                    # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                    #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                    #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                    #          ...,\n                    #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                    #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                    #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                    #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                    #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                    #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                    #          ...,\n                    #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                    #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                    #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                    #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                    #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                    #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                    #          ...,\n                    #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                    #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                    #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                    #         [0.2331, 0.2643, 0.3268, 0.3060],\n                    #         [0.2435, 0.2995, 0.4062, 0.3724],\n                    #         [0.7188, 0.6198, 0.8281, 0.6784],\n                    #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n                    # el = (tensor([[[0.0880, 0.0855, 0.0718,  ..., 0.0976, 0.1902, 0.0165],\n            if len(inputs) == 0:\n                continue\n            \n           # inputs = inputs.to(device)\n           # targets = targets.to(device)\n            output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n            \"\"\" EXAMPLE :\n            {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n             'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n             'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)} \"\"\"\n          \n            loss = sum(loss for loss in output.values())\n            #train_loss_list.append(loss.detach().cpu().numpy())\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item() * len(inputs)\n            \n            del inputs\n            del targets\n            gc.collect()\n            \n            if i % 5 == 0:\n                save_checkpoint(epoch, model, optimizer, training_loss/batch_cumsum)\n        \n        # VALIDATION\n           \n        model.train()\n        num_correct = 0\n        num_examples = 0\n        valid_loss = 0\n\n        for i,batch in enumerate(val_loader):\n            print(\"batch\", i)\n            inputs = []\n            targets = []\n\n            for el in batch:       # el = (image,labels)\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0][0])\n                    targets.append(el[0][1])\n            \n            if len(inputs) == 0:\n                continue\n            # inputs = inputs.to(device)\n            output = model(inputs, targets)\n            # print(f\"{output = }\")\n            # targets = targets.to(device)\n            loss = output['loss_box_reg']\n\n            valid_loss += loss.data.item() \n\n           # correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n           # num_correct += torch.sum(correct).item()\n           # num_examples += correct.shape[0]\n\n            del inputs\n            del targets\n            gc.collect()\n\n        valid_loss /= len(val_loader.dataset)\n        print( 'Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.4f}'.format(epoch, training_loss,\n        valid_loss, 1))\n\n        logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.4f}'.format(epoch, training_loss,\n        valid_loss, 1))\n        \n        \n# TEST\n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","collapsed":false,"id":"Mv8b06EulUK2","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:32:31.143546Z","iopub.execute_input":"2024-05-19T08:32:31.143968Z","iopub.status.idle":"2024-05-19T08:32:31.162672Z","shell.execute_reply.started":"2024-05-19T08:32:31.143927Z","shell.execute_reply":"2024-05-19T08:32:31.161421Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","metadata":{"_uuid":"320e60fe-fabe-487b-91d3-a2b3d5965cba","_cell_guid":"3d46d08a-3a51-4452-b79e-882309e42a16","collapsed":false,"id":"LloRuEg8lWyf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:32:31.164478Z","iopub.execute_input":"2024-05-19T08:32:31.164880Z","iopub.status.idle":"2024-05-19T08:32:31.179561Z","shell.execute_reply.started":"2024-05-19T08:32:31.164846Z","shell.execute_reply":"2024-05-19T08:32:31.178537Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"model = model_rcnn.to(device)\ntorch.compile(model)\noptimizer = optim.Adam(params = model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()","metadata":{"_uuid":"5a384475-40a4-4c13-9c8a-8efeb96ed8f2","_cell_guid":"2f312861-b958-45c9-92e5-ed33d593194f","collapsed":false,"id":"2LUibV2Elccf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T08:32:31.180887Z","iopub.execute_input":"2024-05-19T08:32:31.181774Z","iopub.status.idle":"2024-05-19T08:32:31.206488Z","shell.execute_reply.started":"2024-05-19T08:32:31.181739Z","shell.execute_reply":"2024-05-19T08:32:31.205194Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# START MODEL TRAINING\nif not test_only:\n    train(model, optimizer, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)\n    torch.save(model.state_dict(), 'model_state_dict')","metadata":{"_uuid":"a79ed4b4-e470-4dfc-8f51-ef43b63f3ae5","_cell_guid":"620eeb9f-ac5f-4f99-b241-c62df61659a8","id":"COB3KM9Fx4i7","execution":{"iopub.status.busy":"2024-05-19T08:32:31.210324Z","iopub.execute_input":"2024-05-19T08:32:31.211707Z","iopub.status.idle":"2024-05-19T13:11:19.994860Z","shell.execute_reply.started":"2024-05-19T08:32:31.211657Z","shell.execute_reply":"2024-05-19T13:11:19.993095Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"batch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 0, Training Loss: 1625.3812, Validation Loss: 0.0002, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 1, Training Loss: 1592.7146, Validation Loss: 0.0002, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 2, Training Loss: 1581.1470, Validation Loss: 0.0007, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 3, Training Loss: 1580.7950, Validation Loss: 0.0001, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 4, Training Loss: 1578.6804, Validation Loss: 0.0003, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 5, Training Loss: 1579.2889, Validation Loss: 0.0003, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 6, Training Loss: 1579.6127, Validation Loss: 0.0003, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 7, Training Loss: 1578.3744, Validation Loss: 0.0003, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 8, Training Loss: 1579.4056, Validation Loss: 0.0002, accuracy = 1.0000\nbatch 0\nSaved model\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nSaved model\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nSaved model\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nSaved model\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nSaved model\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nSaved model\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nSaved model\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nSaved model\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nSaved model\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nSaved model\nbatch 46\nbatch 47\nbatch 48\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 9, Training Loss: 1578.0935, Validation Loss: 0.0002, accuracy = 1.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che è stata creata contenente il modello e i log\n\nif not test_only:\n    from IPython.display import FileLink\n    !zip -r file.zip {model_filepath}\n    FileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:12:32.649483Z","iopub.execute_input":"2024-05-19T13:12:32.651385Z","iopub.status.idle":"2024-05-19T13:12:53.115217Z","shell.execute_reply.started":"2024-05-19T13:12:32.651315Z","shell.execute_reply":"2024-05-19T13:12:53.113492Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"updating: models/model_epochs10_noaug_id0/ (stored 0%)\nupdating: models/model_epochs10_noaug_id0/log_05-19_08.30.26.txt (deflated 82%)\nupdating: models/model_epochs10_noaug_id0/model.tar (deflated 7%)\n","output_type":"stream"}]},{"cell_type":"code","source":"def test(model, test_loader, device=\"cpu\"):   \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    \n    for i,batch in enumerate(val_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,labels)\n            if el[1]['boxes'].size()[0] != 0:\n                inputs.append(el[0][0])\n                targets.append(el[0][1])\n        \n        # if len(inputs) == 0:\n            # continue\n        \n        # inputs = inputs.to(device)\n        output = model(inputs)\n        output = output[0]\n        targets = targets[0]\n        \n        #     # Example output\n        #     {'boxes': tensor([[ 0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        # loss = sum(output['scores'])\n        # print(f\"{output = }\")\n        # test_loss += loss * len(inputs)\n        # print(f\"{test_loss = }\")\n\n        # print(f\"{output['scores'] = }\")\n        \n        # res_softmax = torch.softmax(torch.FloatTensor(output['scores']), dim=-1)\n        # print(f\"{res_softmax = }\")\n        # print(f\"{targets = }\")\n        \n        # correct = torch.eq(torch.max(torch.softmax(torch.FloatTensor(output['scores']), dim=0), dim=0)[1], targets).view(-1)\n        # num_correct += torch.sum(correct).item()\n        # num_examples += correct.shape[0]\n                                    \n        \n        \n        del inputs\n        del targets\n        gc.collect()\n        \n    valid_loss /= len(test_loader.dataset)\n    print( 'Test Loss: {:.4f}, accuracy = {:.4f}'.format(\n    test_loss, 1))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:11:34.234129Z","iopub.execute_input":"2024-05-19T13:11:34.234669Z","iopub.status.idle":"2024-05-19T13:11:34.246008Z","shell.execute_reply.started":"2024-05-19T13:11:34.234617Z","shell.execute_reply":"2024-05-19T13:11:34.245113Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n# checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"))\n# model.load_state_dict(checkpoint['model_state_dict'])\n# test(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:11:34.249304Z","iopub.execute_input":"2024-05-19T13:11:34.249660Z","iopub.status.idle":"2024-05-19T13:11:34.263212Z","shell.execute_reply.started":"2024-05-19T13:11:34.249632Z","shell.execute_reply":"2024-05-19T13:11:34.261685Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for data in val_loader:\n#         images, labels = data[0].to(device), data[1].to(device)\n#         predictions = torch.argmax(model(images),dim=1)\n\n#         total += labels.size(0)\n#         correct += (predictions == labels).sum().item()\n\n# print('accuracy = {:f}'.format(correct / total))\n# print('correct: {:d}  total: {:d}'.format(correct, total))","metadata":{"_uuid":"ce6b85c5-91c9-488b-9ae0-74bd3514487f","_cell_guid":"93e16d46-9dc5-41c7-8837-52d4920e6149","collapsed":false,"id":"MLPxPQrile1o","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-19T13:11:34.265608Z","iopub.execute_input":"2024-05-19T13:11:34.266146Z","iopub.status.idle":"2024-05-19T13:11:34.279188Z","shell.execute_reply.started":"2024-05-19T13:11:34.266101Z","shell.execute_reply":"2024-05-19T13:11:34.278103Z"},"trusted":true},"execution_count":67,"outputs":[]}]}