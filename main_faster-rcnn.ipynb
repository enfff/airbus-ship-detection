{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8438193,"sourceType":"datasetVersion","datasetId":5026303},{"sourceId":8516397,"sourceType":"datasetVersion","datasetId":5084559}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# MAIN CONFIGURATIONS\ncreate_log_file = True\nsave_to_drive = False\nmodel_id = '1'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 20\nbatch_size = 32\n# init_lr = 0.1 # Initial Learning Rate\ndata_augmentation_type = 'noaug'  # Which data augmentation tecnique are we using?\n                                  # 'noaug':     no data augmentation\n\n# !tree # Prints folder structure\n\ntest_only = False # when true it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\n\nmodel_filepath = f\"model_epochs{str(num_epochs)}_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\nos.makedirs(model_filepath, exist_ok=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean everything\n# !rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:44:35.401892Z","iopub.execute_input":"2024-05-29T12:44:35.402329Z","iopub.status.idle":"2024-05-29T12:44:36.756875Z","shell.execute_reply.started":"2024-05-29T12:44:35.402297Z","shell.execute_reply":"2024-05-29T12:44:36.755637Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\n\n# !pip install torchsummary\n# from torchsummary import summary\n!pip install torchmetrics\n!pip install pycocotools faster-coco-eval\n!pip install torchmetrics[detection]\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-05-29T11:39:31.375228Z","iopub.execute_input":"2024-05-29T11:39:31.376159Z","iopub.status.idle":"2024-05-29T11:40:08.959114Z","shell.execute_reply.started":"2024-05-29T11:39:31.376117Z","shell.execute_reply":"2024-05-29T11:40:08.957848Z"},"trusted":true},"execution_count":202,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (2.0.7)\nRequirement already satisfied: faster-coco-eval in /opt/conda/lib/python3.10/site-packages (1.5.4)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (5.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (2.1.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->faster-coco-eval) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->faster-coco-eval) (2023.4)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->faster-coco-eval) (8.2.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nRequirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nRequirement already satisfied: pycocotools>2.0.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.0.7)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\ndevice = device(type='cuda')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    logger = logging.getLogger('RootLogger')\n    log_filepath = datetime.now().strftime(\"%m-%d_%H.%M.%S\")\n    log_filepath = os.path.join(model_filepath, f\"log_{log_filepath}\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\n\nprint('ok')","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-05-29T11:40:08.961924Z","iopub.execute_input":"2024-05-29T11:40:08.962827Z","iopub.status.idle":"2024-05-29T11:40:08.973546Z","shell.execute_reply.started":"2024-05-29T11:40:08.962794Z","shell.execute_reply":"2024-05-29T11:40:08.972581Z"},"trusted":true},"execution_count":203,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_epochs10_noaug_id1/log_05-29_11.40.08.txt'\nok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\n# Normalize to the ImageNet mean and standard deviation\n# Could calculate it for the cats/dogs data set, but the ImageNet\n# values give acceptable results here.\nimg_train_transforms = v2.Compose([\n     v2.RandomRotation(50),\n     v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n     v2.RandomHorizontalFlip(p=0.5),\n    v2.Resize((img_dimensions, img_dimensions)),\n     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n     #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])\n\nimg_validation_transforms = v2.Compose([\n    v2.Resize((img_dimensions, img_dimensions)),\n     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n     #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])\n\nprint('ok')","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-05-29T11:40:08.974675Z","iopub.execute_input":"2024-05-29T11:40:08.974954Z","iopub.status.idle":"2024-05-29T11:40:08.989068Z","shell.execute_reply.started":"2024-05-29T11:40:08.974931Z","shell.execute_reply":"2024-05-29T11:40:08.988181Z"},"trusted":true},"execution_count":204,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      # mask[0, r, c+j] = 1\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        # try:\n        label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n        label['boxes'] = torch.Tensor(label['boxes'])\n        label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        # except IndexError as e:\n        #     Warning(f'Errore con {idx = }')\n        #     plt.imshow(image.permute(1, 2, 0))\n        #     plt.show()\n\n        if self.transform:\n            image = self.transform(image, label)\n\n            # prova ad indagare da qui\n            # image = self.transform(image)\n            # image = image.numpy()\n            # return image, label\n            # print(f\"{image = }\")\n            # print(f\"{label = }\")\n\n        return image, label\n\nprint('ok')","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","id":"V1Q6ogjksMqE","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-29T11:40:08.992076Z","iopub.execute_input":"2024-05-29T11:40:08.992485Z","iopub.status.idle":"2024-05-29T11:40:09.013069Z","shell.execute_reply.started":"2024-05-29T11:40:08.992449Z","shell.execute_reply":"2024-05-29T11:40:09.012086Z"},"trusted":true},"execution_count":205,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\nfrom torchvision import tv_tensors\n\n# DATASET_DIR = os.path.join(\".\")\nTRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\nTEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\ntrain_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\ntrain_list, test_list = train_test_split(train_list, test_size = 0.97)\ntrain_list, val_list = train_test_split(train_list, test_size = 0.2)\ntest_list, _ = train_test_split(test_list, test_size = 0.99)\ntest_list, _ = train_test_split(test_list, test_size = 0.5)\n\ntrain_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\ntest_data = ShipsDataset(test_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\nval_data = ShipsDataset(val_list, transforms = img_train_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\nval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\n\n# torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pkl')).\n\n# Salva i DataLoader\n\nprint(len(train_data),len(train_loader))\nprint(len(val_data), len(val_loader))\nprint(len(test_loader))\n\n# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n# La documentazione non Ã¨ chiara sulla posizione dei punti per le ground-truth!\n# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-05-29T11:40:09.014307Z","iopub.execute_input":"2024-05-29T11:40:09.014616Z","iopub.status.idle":"2024-05-29T11:42:05.888049Z","shell.execute_reply.started":"2024-05-29T11:40:09.014580Z","shell.execute_reply":"2024-05-29T11:42:05.887042Z"},"trusted":true},"execution_count":206,"outputs":[{"name":"stdout","text":"153 5\n39 2\n3\n","output_type":"stream"}]},{"cell_type":"code","source":"#train_data = ShipsDataset(train_list, transforms = v2.Resize((224,224)), targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n#loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\n\ndef get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        inputs = []\n        for el in batch:      \n            inputs.append(el[0][0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:42:05.892920Z","iopub.execute_input":"2024-05-29T11:42:05.893249Z","iopub.status.idle":"2024-05-29T11:42:05.902233Z","shell.execute_reply.started":"2024-05-29T11:42:05.893220Z","shell.execute_reply":"2024-05-29T11:42:05.901332Z"},"trusted":true},"execution_count":207,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0893, 0.0827, 0.0817]) original size\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0879, 0.0811, 0.0800]) 224x224\n\n# image_mean_train, image_std_train = get_mean_std(train_loader)\n# image_mean_val, image_std_val = get_mean_std(val_loader)\n\nimage_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\nimage_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\nimage_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\nimage_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\nprint(f\"{image_mean_train = }, {image_std_train =}\")\nprint(f\"{image_mean_val = }, {image_std_val =}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:42:05.903725Z","iopub.execute_input":"2024-05-29T11:42:05.904099Z","iopub.status.idle":"2024-05-29T11:42:05.931518Z","shell.execute_reply.started":"2024-05-29T11:42:05.904065Z","shell.execute_reply":"2024-05-29T11:42:05.930638Z"},"trusted":true},"execution_count":208,"outputs":[{"name":"stdout","text":"image_mean_train = tensor([0.1543, 0.2125, 0.2388]), image_std_train =tensor([0.1429, 0.1588, 0.1657])\nimage_mean_val = tensor([0.1541, 0.2128, 0.2395]), image_std_val =tensor([0.1415, 0.1594, 0.1676])\n","output_type":"stream"}]},{"cell_type":"code","source":"## STEP 1. freeze backbone layers, add final layers and train the network\n\nmodel_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\nfor name, param in model_rcnn.named_parameters():\n      param.requires_grad = False\n\nnum_classes = 2 # background, ship\nin_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\nmodel_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nprint('ok')","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","id":"5J9M_bnAxnDk","execution":{"iopub.status.busy":"2024-05-29T11:42:05.932925Z","iopub.execute_input":"2024-05-29T11:42:05.933238Z","iopub.status.idle":"2024-05-29T11:42:06.779239Z","shell.execute_reply.started":"2024-05-29T11:42:05.933205Z","shell.execute_reply":"2024-05-29T11:42:06.778336Z"},"trusted":true},"execution_count":209,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# How to save in google drive something else\n# if save_to_drive:\n#   with open('/content/drive/MyDrive/MLVM_project/file.txt', 'w') as f:\n#     f.write('content')\n\nprint(f\"{model_filepath = }\")\n\ndef save_checkpoint(epoch, model, optimizer, scheduler, train_loss, val_loss=0, model_name=\"model.tar\"):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","id":"Du5q6_RRCmD4","execution":{"iopub.status.busy":"2024-05-29T11:42:06.780626Z","iopub.execute_input":"2024-05-29T11:42:06.781158Z","iopub.status.idle":"2024-05-29T11:42:06.790044Z","shell.execute_reply.started":"2024-05-29T11:42:06.781124Z","shell.execute_reply":"2024-05-29T11:42:06.789003Z"},"trusted":true},"execution_count":210,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_epochs10_noaug_id1'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\nimport gc\nfrom torchvision.models.detection.transform import GeneralizedRCNNTransform\n\nlrs = [] # list containing lrs\n\ndef train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, epochs=1, device=\"cpu\"):\n    \n    model.transform.image_mean = image_mean_train\n    model.transform.image_std = image_std_train\n    model._skip_resize = True\n    \n    for epoch in range(epochs):\n        \n        lrs.append(optimizer.param_groups[0][\"lr\"])\n        # lrs.append(scheduler.get_last_lr())\n        training_loss = 0.0\n        batch_cumsum = 0\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n            logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n            print(f\"epoch {epoch} batch {i}\")\n            batch_cumsum += len(batch) # needed to compute the training loss later\n            optimizer.zero_grad()\n            \n            # filtering out empty images (model does not accept empty targets)\n            inputs = []\n            targets = []\n            for el in batch:       # el = ((image,dict),dict) when transforms are active\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0][0].to(device))\n                    targets.append({\"boxes\": el[0][1][\"boxes\"].to(device),\"labels\": el[0][1][\"labels\"].to(device)})\n                    \n                    # print(f\"{el = }\")\n                    # Example el\n                    # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                    #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                    #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                    #          ...,\n                    #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                    #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                    #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                    #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                    #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                    #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                    #          ...,\n                    #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                    #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                    #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                    #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                    #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                    #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                    #          ...,\n                    #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                    #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                    #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                    #         [0.2331, 0.2643, 0.3268, 0.3060],\n                    #         [0.2435, 0.2995, 0.4062, 0.3724],\n                    #         [0.7188, 0.6198, 0.8281, 0.6784],\n                    #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n                    \n            if len(inputs) == 0:\n                continue\n         \n            output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n            \"\"\" EXAMPLE :\n            {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n             'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n             'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n             \n             How losses are computed:\n             \n             -loss_classifier-\n             classification_loss = F.cross_entropy(class_logits, labels)\n             \n             -loss_box_reg-\n             box_loss = F.smooth_l1_loss(\n                box_regression[sampled_pos_inds_subset, labels_pos],\n                regression_targets[sampled_pos_inds_subset],\n                beta=1 / 9,\n                reduction=\"sum\",\n            )\n            box_loss = box_loss / labels.numel()\n            \n            -loss_rpn_box_reg-\n            box_loss = F.smooth_l1_loss(\n            pred_bbox_deltas[sampled_pos_inds],\n            regression_targets[sampled_pos_inds],\n            beta=1 / 9,\n            reduction=\"sum\",\n            ) / (sampled_inds.numel())\n            \n            -loss_objectness-\n            objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n             \n             \"\"\"\n          \n            loss = sum(loss for loss in output.values())\n            loss.backward()\n            optimizer.step()\n            scheduler.step() # changes LR\n            training_loss += loss.data.item() * len(batch)\n            \n            del inputs\n            del targets\n            gc.collect()    \n        \n        training_loss /= batch_cumsum\n        save_checkpoint(epoch, model, optimizer, scheduler, training_loss)\n        \n        # VALIDATION\n        model.transform.image_mean = image_mean_val\n        model.transform.image_std = image_std_val\n        \n        model.train()\n        num_correct = 0\n        num_examples = 0\n        valid_loss = 0\n        \n        with torch.no_grad():\n            for i,batch in enumerate(val_loader):\n                print(\"batch\", i)\n                inputs = []\n                targets = []\n\n                for el in batch:       # el = (image,labels)\n                    if el[1]['boxes'].size()[0] != 0:\n                        inputs.append(el[0][0].to(device))\n                        targets.append({\"boxes\": el[0][1][\"boxes\"].to(device),\"labels\": el[0][1][\"labels\"].to(device)})\n\n                if len(inputs) == 0:\n                    continue\n                \n                output = model(inputs, targets)\n\n                loss = sum(loss for loss in output.values())\n                valid_loss += loss.data.item() *len(batch)\n\n                del inputs\n                del targets\n                gc.collect()\n\n        valid_loss /= len(val_loader.dataset)\n        \n        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.4f}'.format(epoch, training_loss,\n        valid_loss, optimizer.param_groups[0][\"lr\"]))\n\n        logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.4f}'.format(epoch, training_loss,\n        valid_loss, optimizer.param_groups[0][\"lr\"]))\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5\nprint('ok')","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-05-29T12:30:34.324852Z","iopub.execute_input":"2024-05-29T12:30:34.325204Z","iopub.status.idle":"2024-05-29T12:30:34.350504Z","shell.execute_reply.started":"2024-05-29T12:30:34.325169Z","shell.execute_reply":"2024-05-29T12:30:34.349687Z"},"trusted":true},"execution_count":227,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"{device = }\")\nmodel = model_rcnn.to(device)\ntorch.compile(model)\noptimizer = optim.Adam(params = model.parameters(), lr = 1e-2)\nscheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\ncriterion = nn.CrossEntropyLoss()","metadata":{"_uuid":"5a384475-40a4-4c13-9c8a-8efeb96ed8f2","_cell_guid":"2f312861-b958-45c9-92e5-ed33d593194f","id":"2LUibV2Elccf","execution":{"iopub.status.busy":"2024-05-29T12:30:34.352652Z","iopub.execute_input":"2024-05-29T12:30:34.352936Z","iopub.status.idle":"2024-05-29T12:30:34.368269Z","shell.execute_reply.started":"2024-05-29T12:30:34.352908Z","shell.execute_reply":"2024-05-29T12:30:34.367429Z"},"trusted":true},"execution_count":228,"outputs":[{"name":"stdout","text":"device = device(type='cuda')\n","output_type":"stream"}]},{"cell_type":"code","source":"# START MODEL TRAINING\n\nlogger.info(f\"Beginning training, {num_epochs = }, {device = }\")\nprint(f\"Beginning training, {num_epochs = }\")\n\nif not test_only:\n    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)\n    torch.save(model.state_dict(), 'model_state_dict')\n    \n    fig, ax = plt.subplots()\n    ax.plot(lrs)    \n    ax.set(xlabel='epoch', ylabel='learning rate value')\n    fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n    print(f\"{lrs = }\")\n    logger.info(f\"{lrs = }\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:30:34.369372Z","iopub.execute_input":"2024-05-29T12:30:34.369705Z","iopub.status.idle":"2024-05-29T12:35:34.360978Z","shell.execute_reply.started":"2024-05-29T12:30:34.369681Z","shell.execute_reply":"2024-05-29T12:35:34.360057Z"},"trusted":true},"execution_count":229,"outputs":[{"name":"stdout","text":"Beginning training, num_epochs = 10\nepoch 0 batch 0\nepoch 0 batch 1\nepoch 0 batch 2\nepoch 0 batch 3\nepoch 0 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 0, Training Loss: 4.6229, Validation Loss: 4.7816, lr: 0.1000\nepoch 1 batch 0\nepoch 1 batch 1\nepoch 1 batch 2\nepoch 1 batch 3\nepoch 1 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 1, Training Loss: 4.5571, Validation Loss: 4.7637, lr: 0.1000\nepoch 2 batch 0\nepoch 2 batch 1\nepoch 2 batch 2\nepoch 2 batch 3\nepoch 2 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 2, Training Loss: 4.5510, Validation Loss: 4.7196, lr: 0.1000\nepoch 3 batch 0\nepoch 3 batch 1\nepoch 3 batch 2\nepoch 3 batch 3\nepoch 3 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 3, Training Loss: 4.6876, Validation Loss: 4.8142, lr: 0.1000\nepoch 4 batch 0\nepoch 4 batch 1\nepoch 4 batch 2\nepoch 4 batch 3\nepoch 4 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 4, Training Loss: 4.5539, Validation Loss: 4.8033, lr: 0.1000\nepoch 5 batch 0\nepoch 5 batch 1\nepoch 5 batch 2\nepoch 5 batch 3\nepoch 5 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 5, Training Loss: 4.5814, Validation Loss: 4.7494, lr: 0.1000\nepoch 6 batch 0\nepoch 6 batch 1\nepoch 6 batch 2\nepoch 6 batch 3\nepoch 6 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 6, Training Loss: 4.5221, Validation Loss: 3.8569, lr: 0.1000\nepoch 7 batch 0\nepoch 7 batch 1\nepoch 7 batch 2\nepoch 7 batch 3\nepoch 7 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 7, Training Loss: 4.5376, Validation Loss: 4.8689, lr: 0.1000\nepoch 8 batch 0\nepoch 8 batch 1\nepoch 8 batch 2\nepoch 8 batch 3\nepoch 8 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 8, Training Loss: 4.6534, Validation Loss: 4.8488, lr: 0.1000\nepoch 9 batch 0\nepoch 9 batch 1\nepoch 9 batch 2\nepoch 9 batch 3\nepoch 9 batch 4\nSaved model\nbatch 0\nbatch 1\nEpoch: 9, Training Loss: 4.6563, Validation Loss: 4.9977, lr: 0.1000\nlrs = [0.03333333333333333, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che Ã¨ stata creata contenente il modello e i log\n\nfrom IPython.display import FileLink\n!zip -r file.zip /kaggle/working/{model_filepath}\nFileLink(r'file.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchmetrics.detection import MeanAveragePrecision\nimport torchvision.transforms.functional as F\n\ndef test(model, test_loader, device=\"cpu\"):   \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in enumerate(test_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = ((image,dict),dict)\n            if el[1]['boxes'].size()[0] != 0:\n                inputs.append(el[0][0].to(device))\n                targets.append(el[0][1])\n        \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n        # print(type(model(torch.cuda.FloatTensor(inputs))))\n        print(\"out :\\n\", output)\n        print(\"target :\\n\",targets)\n                \n        #     # Example output\n        #     {'boxes': tensor([[ 0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        res = metric(output.cpu(),targets)\n        mAP += res['map_75']\n        #print(res)\n        \n        del inputs\n        del targets\n        gc.collect()\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:28.191703Z","iopub.execute_input":"2024-05-29T11:48:28.192416Z","iopub.status.idle":"2024-05-29T11:48:28.207577Z","shell.execute_reply.started":"2024-05-29T11:48:28.192375Z","shell.execute_reply":"2024-05-29T11:48:28.206503Z"},"trusted":true},"execution_count":215,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# START MODEL TEST\n\n# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\ncheckpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # ENF\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\nif torch.cuda.is_available():\n    model.cuda()\n    print(\"model is now using cuda\")\n\ntest(model.to(device), test_loader)\n\n# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n# model.load_state_dict(checkpoint['model_state_dict'])\n\n# checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\n# model.load_state_dict(checkpoint['model_state_dict'])\n# # scheduler.load_state_dict(checkpoint['scheduler_state_dict']) # non serve nel test\n# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n# epoch = checkpoint['epoch']\n# loss = checkpoint['loss']\n# test(model.to(device), test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # TRAIN AGAIN\n# # FUNZIONE TUTTA DA RIFARE\n# import pickle\n\n# train_again = False\n\n# if train_again:\n#     model = torch.load(os.path.join(model_filepath, \"model.tar\"))\n#     model = model.to(device)\n#     torch.compile(model)\n#     # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    \n#     # Il file salvato model.tar contiene optiimzer, scheduler, loss e tanto altro\n\n#     # train(model, model.optimizer, model.scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:48:31.198930Z","iopub.status.idle":"2024-05-29T11:48:31.199454Z","shell.execute_reply.started":"2024-05-29T11:48:31.199185Z","shell.execute_reply":"2024-05-29T11:48:31.199208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for data in val_loader:\n#         images, labels = data[0].to(device), data[1].to(device)\n#         predictions = torch.argmax(model(images),dim=1)\n\n#         total += labels.size(0)\n#         correct += (predictions == labels).sum().item()\n\n# print('accuracy = {:f}'.format(correct / total))\n# print('correct: {:d}  total: {:d}'.format(correct, total))","metadata":{"_uuid":"ce6b85c5-91c9-488b-9ae0-74bd3514487f","_cell_guid":"93e16d46-9dc5-41c7-8837-52d4920e6149","id":"MLPxPQrile1o","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-21T07:41:13.044527Z","iopub.execute_input":"2024-05-21T07:41:13.045158Z","iopub.status.idle":"2024-05-21T07:41:13.054176Z","shell.execute_reply.started":"2024-05-21T07:41:13.045063Z","shell.execute_reply":"2024-05-21T07:41:13.052798Z"}}},{"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]}]}