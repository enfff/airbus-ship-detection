{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T09:03:06.526787Z","iopub.status.busy":"2024-06-10T09:03:06.525973Z","iopub.status.idle":"2024-06-10T09:03:06.541096Z","shell.execute_reply":"2024-06-10T09:03:06.540140Z","shell.execute_reply.started":"2024-06-10T09:03:06.526751Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["model_filepath = 'models/model_none_id0'\n"]}],"source":["import os\n","\n","## MAIN CONFIGURATIONS\n","model_id = '0'  # We will train multiple models with the same settings. Keep it as a string!\n","num_epochs = 10  # Number of epochs the model will train for\n","batch_size = 32\n","dataset_percentage_for_training = 0.1 # Which percentage of the dataset to use for training. 0.03 means 192_556 * 0.03 ~ 4.6k images in the training\n","init_lr = 1e-4 # Initial Learning Rate\n","data_augmentation_type = 'none'    # Which data augmentation tecnique are we using?\n","                                    # 'none' :     only image resize\n","                                    # 'noaug':     basic geometric transforms\n","                                    # 'fourier':   fourier transforms\n","\n","## WHAT WILL THIS SESSION DO?\n","test_only = False # When True it doesn't train the model, but it just tests an existing one\n","train_again = False # Trains the model again for num_epoch times\n","do_model_test = False # Tests the model after training\n","create_log_file = True\n","plot_data = True # Plot training data of '{model_filepath}/model.tar'\n","print_images_during_training = False\n","\n","model_filepath = f\"model_{data_augmentation_type}_id{model_id}\"\n","model_filepath = os.path.join(\"models\", model_filepath)\n","print(f\"{model_filepath = }\")\n","\n","# !tree # Prints folder structure\n","os.makedirs(model_filepath, exist_ok=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T09:03:06.545542Z","iopub.status.busy":"2024-06-10T09:03:06.545235Z","iopub.status.idle":"2024-06-10T09:03:12.330922Z","shell.execute_reply":"2024-06-10T09:03:12.329898Z","shell.execute_reply.started":"2024-06-10T09:03:06.545517Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.models as models\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import glob\n","import pandas as pd\n","from torchvision.io import read_image\n","from torchvision.transforms.functional import rotate\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T09:03:12.333292Z","iopub.status.busy":"2024-06-10T09:03:12.332804Z","iopub.status.idle":"2024-06-10T09:03:40.121016Z","shell.execute_reply":"2024-06-10T09:03:40.119882Z","shell.execute_reply.started":"2024-06-10T09:03:12.333264Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\n","Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\n","Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\n","Requirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2)\n","Collecting pycocotools>2.0.0 (from torchmetrics[detection])\n","  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\n","Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\n","Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hInstalling collected packages: pycocotools\n","Successfully installed pycocotools-2.0.7\n"]}],"source":["!pip install torchmetrics\n","!pip install torchmetrics[detection]"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","execution":{"iopub.execute_input":"2024-06-10T09:03:40.123404Z","iopub.status.busy":"2024-06-10T09:03:40.122945Z","iopub.status.idle":"2024-06-10T09:03:40.157985Z","shell.execute_reply":"2024-06-10T09:03:40.156816Z","shell.execute_reply.started":"2024-06-10T09:03:40.123358Z"},"id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["device = device(type='cuda')\n"]}],"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"{device = }\")"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","execution":{"iopub.execute_input":"2024-06-10T09:03:40.161020Z","iopub.status.busy":"2024-06-10T09:03:40.160227Z","iopub.status.idle":"2024-06-10T09:03:40.171589Z","shell.execute_reply":"2024-06-10T09:03:40.170577Z","shell.execute_reply.started":"2024-06-10T09:03:40.160991Z"},"id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["log_filepath = 'models/model_none_id0/log.txt'\n","models/model_none_id0/log.txt\n"]}],"source":["import logging\n","from datetime import datetime\n","\n","if create_log_file:\n","    log_filepath = \"\"\n","    logger = logging.getLogger('RootLogger')\n","    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n","    print(f\"{log_filepath = }\")\n","    \n","    logging.basicConfig(filename=log_filepath,\n","                        filemode='a',\n","                        format='%(asctime)s %(levelname)s %(message)s',\n","                        level=logging.INFO,\n","                        datefmt='%m-%d %H:%M:%S',\n","                        force=True)\n","else:\n","    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n","                        level=logging.INFO,\n","                        datefmt='%m-%d %H:%M:%S',\n","                        force=True)\n","\n","if log_filepath:\n","    print(log_filepath)\n","else:\n","    print(\"No logging\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T08:21:31.693515Z","iopub.status.busy":"2024-06-10T08:21:31.693032Z","iopub.status.idle":"2024-06-10T08:21:31.717020Z","shell.execute_reply":"2024-06-10T08:21:31.716083Z","shell.execute_reply.started":"2024-06-10T08:21:31.693480Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok\n"]}],"source":["# FOURIER DATA AUGMENTATION\n","\n","import torch.fft as fft\n","import torchvision\n","import random\n","\n","from torch import sin, cos\n","\n","class FourierRandomNoise(object):\n","        \n","    def __call__(self, *sample ):\n","        image = sample[0]\n","\n","        # Fourier Transform\n","        fourier = fft.rfftn(image)\n","        magnitude, angle = self.__polar_form(fourier)\n","\n","        # Apply Noise in the Frequency Domain\n","        noise = torch.rand(fourier.size())\n","        noised_magnitude = torch.mul(magnitude,noise)\n","\n","        # Inverse Fourier Transform\n","        fourier = self.__complex_form(noised_magnitude,angle)\n","        modified_image = fft.irfftn(fourier).byte()\n","        \n","        if len(sample) >= 2:\n","            label = sample[1]\n","            return modified_image, label\n","\n","        return modified_image\n","    \n","    def __polar_form(self, complex_tensor):\n","        return complex_tensor.abs(), complex_tensor.angle()\n","\n","    def __complex_form(self, magnitude, angle):\n","        return torch.polar(magnitude,angle)\n","    \n","\n","class PatchGaussian(object):\n","    \n","    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n","        '''\n","        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n","        The noise of the patch can be modified by specifying its variance\n","        '''\n","        \n","        image = sample[0]\n","        size = image.size()\n","        # Scale the image in range [0,1)\n","        min_val = 0\n","        max_val = 255\n","        image = (image-min_val)/(max_val-min_val)\n","\n","        # Define Gaussian patch\n","        patch = torch.empty(size).normal_(0,sigma_max)\n","        # Sample Corner Indices\n","        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n","        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n","        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n","        u = torch.stack([u,u,u])\n","        v = torch.stack([v,v,v])\n","        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n","        patch = mask*patch\n","        \n","        if len(sample) >= 2:\n","            label = sample[1]\n","            return torch.clip(image+patch,0,1), label\n","        \n","        return torch.clip(image+patch,0,1)\n","\n","class FourierBasisAugmentation(object):\n","    \n","    def __call__(self,*sample, l=0.3):\n","        '''\n","        Adds a Fourier Basis Function to the image\n","        '''\n","        image = sample[0]\n","        shape = image.size()\n","        min_val = 0\n","        max_val = 255\n","        # Scale the image in range [0,1)\n","        image = (image-min_val)/(max_val-min_val)\n","\n","        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n","        # where M is the size of the image\n","        f = (shape[1]-1)*torch.rand(3)\n","        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n","        w = (torch.pi-0)*torch.rand(3)\n","\n","        # Sample the decay parameter from a l-exponential distribution\n","        sigma = torch.distributions.Exponential(1/l).sample((3,))\n","\n","        # Generate basis function\n","        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n","        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n","        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n","        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n","        noise = torch.stack([basis_r,basis_g,basis_b])\n","\n","        # Modify The Image\n","        modified_image = image+noise\n","        \n","        if len(sample) >= 2:\n","            label = sample[1]\n","            return torch.clip(modified_image,0,1), label\n","\n","        return torch.clip(modified_image,0,1)\n","\n","print(\"ok\")"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","execution":{"iopub.execute_input":"2024-06-10T08:21:31.718753Z","iopub.status.busy":"2024-06-10T08:21:31.718479Z","iopub.status.idle":"2024-06-10T08:21:31.767458Z","shell.execute_reply":"2024-06-10T08:21:31.766604Z","shell.execute_reply.started":"2024-06-10T08:21:31.718730Z"},"id":"LIgECtVqMlCI","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok\n"]}],"source":["# TRANSFORMATIONS\n","\n","from torchvision.transforms import v2\n","\n","img_dimensions = 224\n","\n","match data_augmentation_type:\n","    \n","    case 'noaug':\n","        \n","        img_train_transforms = v2.Compose([\n","             v2.RandomRotation(50),\n","             v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n","             v2.RandomHorizontalFlip(p=0.5),\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n","            ])\n","\n","        img_validation_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n","            ])\n","\n","        img_test_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n","            ])\n","    \n","    case 'fourier':\n","        \n","        img_train_transforms = v2.Compose([\n","             FourierRandomNoise(),\n","             PatchGaussian(),\n","             FourierBasisAugmentation(),\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","            ])\n","\n","        img_validation_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n","            ])\n","\n","        img_test_transforms = v2.Compose([\n","            v2.Resize((img_dimensions, img_dimensions)),\n","             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n","             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n","            ])\n","    \n","    case 'none':\n","        img_train_transforms = v2.Compose([\n","                v2.Resize((img_dimensions, img_dimensions)),\n","                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n","            ])\n","        img_validation_transforms = v2.Compose([\n","                v2.Resize((img_dimensions, img_dimensions)),\n","                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n","            ])\n","        img_test_transforms = v2.Compose([\n","                v2.Resize((img_dimensions, img_dimensions)),\n","                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n","            ])\n","\n","print('ok')"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","_kg_hide-input":true,"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","execution":{"iopub.execute_input":"2024-06-10T08:21:31.768862Z","iopub.status.busy":"2024-06-10T08:21:31.768585Z","iopub.status.idle":"2024-06-10T08:21:31.782812Z","shell.execute_reply":"2024-06-10T08:21:31.781813Z","shell.execute_reply.started":"2024-06-10T08:21:31.768839Z"},"id":"V1Q6ogjksMqE","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok\n"]}],"source":["import torch\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as F\n","\n","def show(imgs, rotation=None):\n","\n","    if rotation:\n","          imgs = rotate(imgs, rotation)\n","\n","    if not isinstance(imgs, list):\n","        imgs = [imgs]\n","    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n","    for i, img in enumerate(imgs):\n","        img = img.detach()\n","        img = F.to_pil_image(img)\n","        axs[0, i].imshow(np.asarray(img))\n","        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n","\n","class ShipsDataset(torch.utils.data.Dataset):\n","    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n","        self.file_list = sorted(file_list, key = lambda f: f.split('/')[-1])\n","        self.targets = sorted(targets, key=lambda d: d['image_id'])\n","        self.transform = transforms\n","\n","    def __len__(self):\n","        self.filelength = len(self.file_list)\n","        return self.filelength\n","\n","    def __getitem__(self, idx):\n","        \n","        try:\n","            image = read_image(self.file_list[idx])    # numpy tensor\n","        except RuntimeError as e:\n","            Warning(f'Errore con {self.file_list[idx]}')\n","            self.targets[idx]['labels'] = torch.tensor([])\n","            return None, self.targets[idx]\n","        \n","        image = F.convert_image_dtype(image)\n","        \n","      #  print(self.file_list[idx])\n","      #  print(self.targets[idx])\n","        \n","        # Added this line to fix this problem (ENF) during training\n","        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n","\n","        label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n","        \n","        try:\n","            label['boxes'] = torch.Tensor(label['boxes'])\n","            label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n","        except IndexError as e:\n","            Warning(f'Errore con {idx = }')\n","            plt.imshow(image.permute(1, 2, 0))\n","            plt.show()\n","\n","        if self.transform:\n","            image, label = self.transform(image, label)\n","\n","        return image, label\n","\n","print('ok')"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","execution":{"iopub.execute_input":"2024-06-10T08:21:31.784752Z","iopub.status.busy":"2024-06-10T08:21:31.784219Z","iopub.status.idle":"2024-06-10T08:21:37.983148Z","shell.execute_reply":"2024-06-10T08:21:37.981954Z","shell.execute_reply.started":"2024-06-10T08:21:31.784720Z"},"id":"YW9039lzlK5S","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["135 batch TRAINING, 4333 images\n","33 batch VALIDATION, 1084 images\n","18 batch TEST, 602 images\n","ok\n"]}],"source":["if not test_only:\n","\n","    from sklearn.model_selection import train_test_split\n","    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n","    from torchvision import tv_tensors\n","    from torch.utils.data import SubsetRandomSampler\n","\n","    # DATASET_DIR = os.path.join(\".\")\n","    TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n","    TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n","    # print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n","\n","    data_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n","#     train_list, test_list = train_test_split(train_list, test_size = float(1 - dataset_percentage)) # check first cell\n","#     train_list, val_list = train_test_split(train_list, test_size = 0.2)\n","#     test_list, _ = train_test_split(test_list, test_size = 0.99)\n","#     test_list, _ = train_test_split(test_list, test_size = 0.5)\n","    \n","   # train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/targets-rcnn/rcnn_targets_modified.npy', allow_pickle='TRUE'))\n","    ship_dataset = ShipsDataset(data_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/targets-rcnn/rcnn_targets_modified.npy', allow_pickle='TRUE'))\n","#     test_data = ShipsDataset(test_list, transforms = img_test_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n","#     val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n","\n","    def custom_collate_fn(batch):\n","        # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n","        # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n","        # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n","\n","        return batch\n","\n","    # Fix the generator for reproducibility, remove once we understand that it works\n","   # generator = torch.Generator().manual_seed(42)\n","    # Split the dataset into train and test\n"," #   train_dataset, test_dataset = torch.utils.data.random_split(ship_dataset, [0.7, 0.3], generator)\n","#    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n"," #   test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n","#     val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n","#     test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n","     \n","    # Fix the generator for reproducibility, remove once we understand that it works\n","    generator = torch.Generator().manual_seed(42)\n","    used_train_percentage = 0.2\n","    # Split dataset to use only a certain part of it\n","    used_train_dataset, unused_train_dataset = torch.utils.data.random_split(ship_dataset, [used_train_percentage, 1.0-used_train_percentage], generator)\n","    # Split the used part into training and validation\n","    train_dataset, validation_dataset = torch.utils.data.random_split(used_train_dataset, [0.8, 0.2], generator)\n","    # Split the remaining part and take the resulting test dataset\n","    test_dataset, _ = torch.utils.data.random_split(unused_train_dataset, [0.4, 0.6], generator)\n","    # Create the dataloaders\n","    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n","    val_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n","    test_loader = train_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n","     \n","    # print(f\"{len(train_loader)} batches for TRAINING, {len(train_idx)} images\")\n","    # print(f\"{len(val_loader)} batches for VALIDATION, {len(val_idx)} images\")\n","    # print(f\"{len(test_loader)} batches for TEST, {len(test_idx)} images\")\n","\n","    # logger.info(f\"{len(train_loader)} batches for TRAINING, {len(train_idx)} images\")\n","    # logger.info(f\"{len(val_loader)} batches for VALIDATION, {len(val_idx)} images\")\n","    # logger.info(f\"{len(test_loader)} batches for TEST, {len(test_idx)} images\")\n","\n","    # https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n","    # La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n","    # /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"\n","\n","print('ok')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T08:21:37.985126Z","iopub.status.busy":"2024-06-10T08:21:37.984665Z","iopub.status.idle":"2024-06-10T08:21:43.657662Z","shell.execute_reply":"2024-06-10T08:21:43.656680Z","shell.execute_reply.started":"2024-06-10T08:21:37.985089Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Loaders saved succesfully!\n"]}],"source":[" if not test_only:\n","    # Save loaders\n","    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n","    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n","    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n","\n","    print('Dataset Loaders saved succesfully!')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T08:21:43.659228Z","iopub.status.busy":"2024-06-10T08:21:43.658931Z","iopub.status.idle":"2024-06-10T08:21:43.688039Z","shell.execute_reply":"2024-06-10T08:21:43.686634Z","shell.execute_reply.started":"2024-06-10T08:21:43.659203Z"},"trusted":true},"outputs":[],"source":["\n","def get_mean_std(loader):\n","    # Compute the mean and standard deviation of all pixels in the dataset  \n","    print(\"computing mean and std of this dataset split...\")\n","    nimages = 0\n","    mean = 0.\n","    var = 0.\n","    for i, batch in enumerate(loader):\n","        inputs = []\n","        for el in batch:      \n","            inputs.append(el[0])\n","        batch = torch.stack(inputs, dim=0)\n","        # Rearrange batch to be the shape of [B, C, W * H]\n","        batch = batch.view(batch.size(0), batch.size(1), -1)\n","        # Update total number of images\n","        nimages += batch.size(0)\n","        # Compute mean and var\n","        mean += batch.mean(2).sum(0) \n","        var += batch.var(2).sum(0)\n","        \n","    mean /= nimages\n","    var /= nimages\n","    std = torch.sqrt(var)\n","    print(\"Done\")\n","    \n","    return mean, std\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T08:21:43.689492Z","iopub.status.busy":"2024-06-10T08:21:43.689210Z","iopub.status.idle":"2024-06-10T08:21:43.844338Z","shell.execute_reply":"2024-06-10T08:21:43.843262Z","shell.execute_reply.started":"2024-06-10T08:21:43.689467Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["image_mean_train = tensor([0.1543, 0.2125, 0.2388]), image_std_train =tensor([0.1429, 0.1588, 0.1657])\n","image_mean_val = tensor([0.1541, 0.2128, 0.2395]), image_std_val =tensor([0.1415, 0.1594, 0.1676])\n","image_mean_test = tensor([0.2114, 0.2936, 0.3265]), image_std_test =tensor([0.0816, 0.0745, 0.0731])\n","none\n","ok\n"]}],"source":["\n","# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0893, 0.0827, 0.0817]) original size\n","# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0879, 0.0811, 0.0800]) 224x224\n","\n","# image_mean_train, image_std_train = get_mean_std(train_loader)\n","# image_mean_val, image_std_val = get_mean_std(val_loader)\n","# image_mean_test, image_std_test = get_mean_std(test_loader)\n","\n","match data_augmentation_type:\n","    \n","    case 'noaug':\n","\n","        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n","        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n","\n","        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n","        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n","\n","        image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n","        image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n","    \n","    case 'fourier':\n","        \n","        image_mean_train = torch.Tensor([0.0954, 0.0930, 0.0948])\n","        image_std_train = torch.Tensor([0.1039, 0.1016, 0.1036])\n","\n","        image_mean_val = torch.Tensor([0.1925, 0.2728, 0.3091])\n","        image_std_val = torch.Tensor([0.0881, 0.0804, 0.0795])\n","\n","        image_mean_test = torch.tensor([0.1985, 0.2808, 0.3167])\n","        image_std_test = torch.tensor([0.0846, 0.0770, 0.0768])\n","    \n","  #  case 'none':\n","        \n","       # image_mean_train, image_std_train = get_mean_std(train_loader)\n","        \n","\n","\n","\n","image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n","image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n","\n","image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n","image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n","\n","image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n","image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n","\n","print(f\"{image_mean_train = }, {image_std_train =}\")\n","print(f\"{image_mean_val = }, {image_std_val =}\")\n","print(f\"{image_mean_test = }, {image_std_test =}\")\n","print(f\"{data_augmentation_type}\")\n","\n","print('ok')"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","execution":{"iopub.execute_input":"2024-06-10T08:21:43.846291Z","iopub.status.busy":"2024-06-10T08:21:43.845729Z","iopub.status.idle":"2024-06-10T08:21:45.411811Z","shell.execute_reply":"2024-06-10T08:21:45.410720Z","shell.execute_reply.started":"2024-06-10T08:21:43.846255Z"},"id":"5J9M_bnAxnDk","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok new model\n"]}],"source":["def new_model():\n","    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","    for module in model_rcnn.backbone.body.modules():\n","        if isinstance(module, nn.Conv2d):\n","            # Insert batch normalization after convolutional layers\n","            module = nn.Sequential(\n","                module,\n","                nn.BatchNorm2d(module.out_channels),\n","                nn.ReLU(inplace=True)\n","            )\n","\n","    for name, param in model_rcnn.named_parameters():\n","          param.requires_grad = False\n","\n","    num_classes = 2 # background, ship\n","    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n","    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","    return model_rcnn\n","\n","model_rcnn = new_model()\n","\n","print('ok new model')"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","execution":{"iopub.execute_input":"2024-06-10T08:21:45.416195Z","iopub.status.busy":"2024-06-10T08:21:45.415857Z","iopub.status.idle":"2024-06-10T08:21:45.424047Z","shell.execute_reply":"2024-06-10T08:21:45.422818Z","shell.execute_reply.started":"2024-06-10T08:21:45.416167Z"},"id":"Du5q6_RRCmD4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok\n"]}],"source":["def save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, model_name=\"model.tar\"):\n","    \"\"\"\n","        epoch: last trained epoch\n","    \"\"\"\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'training_losses': training_losses,\n","        'validation_losses': validation_losses,\n","        'lrs': lrs\n","    }, os.path.join(model_filepath, model_name))\n","    print(\"Saved model\")\n","\n","print(\"ok\")"]},{"cell_type":"code","execution_count":17,"metadata":{"_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","execution":{"iopub.execute_input":"2024-06-10T08:22:10.638133Z","iopub.status.busy":"2024-06-10T08:22:10.637752Z","iopub.status.idle":"2024-06-10T08:22:10.666313Z","shell.execute_reply":"2024-06-10T08:22:10.665310Z","shell.execute_reply.started":"2024-06-10T08:22:10.638105Z"},"id":"Mv8b06EulUK2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ok\n"]}],"source":["# TRAIN\n","\n","import matplotlib.pyplot as plt\n","from torchvision.utils import draw_bounding_boxes\n","\n","if not test_only:\n","    \n","    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n","\n","    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n","\n","        model.transform.image_mean = image_mean_train\n","        model.transform.image_std = image_std_train\n","        model._skip_resize = True\n","\n","        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n","\n","            training_loss = 0.0\n","            batch_cumsum = 0\n","            model.train()\n","            \n","            for i, batch in enumerate(train_loader):\n","                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n","                print(f\"epoch {epoch} batch {i}\")\n","                batch_cumsum += len(batch) # needed to compute the training loss later\n","                optimizer.zero_grad()\n","                \n","                inputs = []\n","                targets = []\n","                \n","                for el in batch:       # el = (image,dict) when transforms are active\n","                    \n","                    el_dict = {\n","                        \"boxes\": el[1][\"boxes\"].to(device),\n","                        \"labels\": el[1][\"labels\"].to(device)\n","                    }\n","                    \n","                    if not el_dict[\"labels\"].numel():\n","                        # filtering out empty images (model does not accept empty targets)\n","                        continue\n","                    else:\n","                      #  print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n","                      #  print(f'el_dict has {el_dict[\"boxes\"] = }')\n","                        \n","                        image = el[0].to(device)\n","                        el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n","                        \n","                        inputs.append(image)\n","                        targets.append(el_dict)\n","                        \n","                        # Print images during training\n","                        if print_images_during_training:\n","                            num = len(el_dict[\"boxes\"])\n","\n","                            img = draw_bounding_boxes(\n","                                (image*256).byte(),\n","                                el_dict[\"boxes\"]*224,\n","                                width = 1,\n","                                colors = 'yellow',\n","                                # font='arial',\n","                                font_size = 15\n","                            )\n","\n","                            fig, ax = plt.subplots()\n","                            fig.set_size_inches(16,9)\n","                            fig.tight_layout(pad=5)\n","                            ax.imshow(img.byte().permute(1, 2, 0))\n","                            plt.show()\n","                            plt.close()\n","\n","                        # print(f\"{el = }\")\n","                        # Example el\n","                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n","                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n","                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n","                        #          ...,\n","                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n","                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n","                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n","\n","                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n","                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n","                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n","                        #          ...,\n","                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n","                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n","                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n","\n","                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n","                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n","                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n","                        #          ...,\n","                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n","                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n","                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n","                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n","                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n","                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n","                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n","\n","                if len(inputs) == 0:\n","                    continue\n","\n","                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n","\n","                \"\"\" EXAMPLE :\n","                    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n","                     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n","                     'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n","\n","                     How losses are computed:\n","\n","                     -loss_classifier-\n","                     classification_loss = F.cross_entropy(class_logits, labels)\n","\n","                     -loss_box_reg-\n","                     box_loss = F.smooth_l1_loss(\n","                        box_regression[sampled_pos_inds_subset, labels_pos],\n","                        regression_targets[sampled_pos_inds_subset],\n","                        beta=1 / 9,\n","                        reduction=\"sum\",\n","                    )\n","                    box_loss = box_loss / labels.numel()\n","\n","                    -loss_rpn_box_reg-\n","                    box_loss = F.smooth_l1_loss(\n","                    pred_bbox_deltas[sampled_pos_inds],\n","                    regression_targets[sampled_pos_inds],\n","                    beta=1 / 9,\n","                    reduction=\"sum\",\n","                    ) / (sampled_inds.numel())\n","\n","                    -loss_objectness-\n","                    objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n","\n","                 \"\"\"\n","\n","                loss = sum(loss for loss in output.values())\n","                loss.backward()\n","                optimizer.step()\n","                training_loss += loss.data.item() * len(batch)\n","\n","            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n","            scheduler.step() # changes LR\n","            training_loss /= batch_cumsum\n","            training_losses.append(training_loss)\n","            # save_checkpoint(epoch, model, optimizer, scheduler, training_loss, lrs)\n","\n","            # VALIDATION\n","            model.transform.image_mean = image_mean_val\n","            model.transform.image_std = image_std_val\n","\n","            model.train()\n","            num_correct = 0\n","            num_examples = 0\n","            valid_loss = 0\n","\n","            with torch.no_grad():\n","                for i,batch in enumerate(val_loader):\n","                    print(\"batch\", i)\n","                    inputs = []\n","                    targets = []\n","\n","                    for el in batch:       # el = (image,labels)\n","                        \n","                        el_dict = {\n","                        \"boxes\": el[1][\"boxes\"].to(device),\n","                        \"labels\": el[1][\"labels\"].to(device)\n","                        }\n","\n","                        if not el_dict[\"labels\"].numel():\n","                            # filtering out empty images (model does not accept empty targets)\n","                            continue\n","                        else:\n","                        #    print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n","                        #    print(f'el_dict has {el_dict[\"boxes\"] = }')\n","\n","                            image = el[0].to(device)\n","                            el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n","\n","                            inputs.append(image)\n","                            targets.append(el_dict)\n","            \n","                        \n","                        '''\n","                        if el[1]['boxes'].size()[0] != 0:\n","                            inputs.append(el[0].to(device))\n","                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n","                        '''\n","                   \n","                    if len(inputs) == 0:\n","                        continue\n","\n","                    output = model(inputs, targets)\n","\n","                    loss = sum(loss for loss in output.values())\n","                    valid_loss += loss.data.item() *len(batch)\n","\n","            valid_loss /= len(val_loader.dataset)\n","            validation_losses.append(valid_loss)\n","\n","            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n","            valid_loss, lrs[-1]))\n","\n","            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n","            valid_loss, lrs[-1]))\n","\n","            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs)\n","        \n","\n","# from torchvision.utils import draw_bounding_boxes\n","# score_threshold = .5\n","print('ok')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-10T08:22:13.546716Z","iopub.status.busy":"2024-06-10T08:22:13.546006Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Beginning training, num_epochs = 10, data_augmentation_type = 'none', batch_size = 32\n","device = device(type='cuda')\n","epoch 0 batch 0\n","epoch 0 batch 1\n","epoch 0 batch 2\n","epoch 0 batch 3\n","epoch 0 batch 4\n","epoch 0 batch 5\n","epoch 0 batch 6\n","epoch 0 batch 7\n","epoch 0 batch 8\n","epoch 0 batch 9\n","epoch 0 batch 10\n","epoch 0 batch 11\n","epoch 0 batch 12\n","epoch 0 batch 13\n","epoch 0 batch 14\n","epoch 0 batch 15\n","epoch 0 batch 16\n","epoch 0 batch 17\n","epoch 0 batch 18\n","epoch 0 batch 19\n","epoch 0 batch 20\n","epoch 0 batch 21\n","epoch 0 batch 22\n","epoch 0 batch 23\n","epoch 0 batch 24\n","epoch 0 batch 25\n","epoch 0 batch 26\n","epoch 0 batch 27\n","epoch 0 batch 28\n","epoch 0 batch 29\n","epoch 0 batch 30\n","epoch 0 batch 31\n","epoch 0 batch 32\n","epoch 0 batch 33\n","epoch 0 batch 34\n","epoch 0 batch 35\n","epoch 0 batch 36\n","epoch 0 batch 37\n","epoch 0 batch 38\n","epoch 0 batch 39\n","epoch 0 batch 40\n","epoch 0 batch 41\n","epoch 0 batch 42\n","epoch 0 batch 43\n","epoch 0 batch 44\n","epoch 0 batch 45\n","epoch 0 batch 46\n","epoch 0 batch 47\n","epoch 0 batch 48\n","epoch 0 batch 49\n","epoch 0 batch 50\n","epoch 0 batch 51\n","epoch 0 batch 52\n","epoch 0 batch 53\n","epoch 0 batch 54\n","epoch 0 batch 55\n","epoch 0 batch 56\n","epoch 0 batch 57\n","epoch 0 batch 58\n","epoch 0 batch 59\n","epoch 0 batch 60\n","epoch 0 batch 61\n","epoch 0 batch 62\n","epoch 0 batch 63\n","epoch 0 batch 64\n","epoch 0 batch 65\n","epoch 0 batch 66\n","epoch 0 batch 67\n","epoch 0 batch 68\n","epoch 0 batch 69\n","epoch 0 batch 70\n","epoch 0 batch 71\n","epoch 0 batch 72\n","epoch 0 batch 73\n","epoch 0 batch 74\n","epoch 0 batch 75\n","epoch 0 batch 76\n","epoch 0 batch 77\n","epoch 0 batch 78\n","epoch 0 batch 79\n","epoch 0 batch 80\n","epoch 0 batch 81\n","epoch 0 batch 82\n","epoch 0 batch 83\n","epoch 0 batch 84\n","epoch 0 batch 85\n","epoch 0 batch 86\n","epoch 0 batch 87\n","epoch 0 batch 88\n","epoch 0 batch 89\n","epoch 0 batch 90\n","epoch 0 batch 91\n","epoch 0 batch 92\n","epoch 0 batch 93\n","epoch 0 batch 94\n","epoch 0 batch 95\n","epoch 0 batch 96\n","epoch 0 batch 97\n","epoch 0 batch 98\n","epoch 0 batch 99\n","epoch 0 batch 100\n","epoch 0 batch 101\n","epoch 0 batch 102\n","epoch 0 batch 103\n","epoch 0 batch 104\n","epoch 0 batch 105\n","epoch 0 batch 106\n","epoch 0 batch 107\n","epoch 0 batch 108\n","epoch 0 batch 109\n","epoch 0 batch 110\n","epoch 0 batch 111\n","epoch 0 batch 112\n","epoch 0 batch 113\n","epoch 0 batch 114\n","epoch 0 batch 115\n","epoch 0 batch 116\n","epoch 0 batch 117\n","epoch 0 batch 118\n","epoch 0 batch 119\n","epoch 0 batch 120\n","epoch 0 batch 121\n","epoch 0 batch 122\n","epoch 0 batch 123\n","epoch 0 batch 124\n","epoch 0 batch 125\n","epoch 0 batch 126\n","epoch 0 batch 127\n","epoch 0 batch 128\n","epoch 0 batch 129\n","epoch 0 batch 130\n","epoch 0 batch 131\n","epoch 0 batch 132\n","epoch 0 batch 133\n","epoch 0 batch 134\n","epoch 0 batch 135\n","epoch 0 batch 136\n","epoch 0 batch 137\n","epoch 0 batch 138\n","epoch 0 batch 139\n","epoch 0 batch 140\n","epoch 0 batch 141\n","epoch 0 batch 142\n","epoch 0 batch 143\n","epoch 0 batch 144\n","epoch 0 batch 145\n","epoch 0 batch 146\n","epoch 0 batch 147\n","epoch 0 batch 148\n","epoch 0 batch 149\n","epoch 0 batch 150\n","epoch 0 batch 151\n","epoch 0 batch 152\n","epoch 0 batch 153\n","epoch 0 batch 154\n","epoch 0 batch 155\n","epoch 0 batch 156\n","epoch 0 batch 157\n","epoch 0 batch 158\n","epoch 0 batch 159\n","epoch 0 batch 160\n","epoch 0 batch 161\n","epoch 0 batch 162\n","epoch 0 batch 163\n","epoch 0 batch 164\n","epoch 0 batch 165\n","epoch 0 batch 166\n","epoch 0 batch 167\n","epoch 0 batch 168\n","epoch 0 batch 169\n","epoch 0 batch 170\n","epoch 0 batch 171\n","epoch 0 batch 172\n","epoch 0 batch 173\n","epoch 0 batch 174\n","epoch 0 batch 175\n","epoch 0 batch 176\n","epoch 0 batch 177\n","epoch 0 batch 178\n","epoch 0 batch 179\n","epoch 0 batch 180\n","epoch 0 batch 181\n","epoch 0 batch 182\n","epoch 0 batch 183\n","epoch 0 batch 184\n","epoch 0 batch 185\n","epoch 0 batch 186\n","epoch 0 batch 187\n","epoch 0 batch 188\n","epoch 0 batch 189\n","epoch 0 batch 190\n","epoch 0 batch 191\n","epoch 0 batch 192\n","epoch 0 batch 193\n","epoch 0 batch 194\n","epoch 0 batch 195\n","epoch 0 batch 196\n","epoch 0 batch 197\n","epoch 0 batch 198\n","epoch 0 batch 199\n","epoch 0 batch 200\n","epoch 0 batch 201\n","epoch 0 batch 202\n","epoch 0 batch 203\n","epoch 0 batch 204\n","epoch 0 batch 205\n","epoch 0 batch 206\n","epoch 0 batch 207\n","epoch 0 batch 208\n","epoch 0 batch 209\n","epoch 0 batch 210\n","epoch 0 batch 211\n","epoch 0 batch 212\n","epoch 0 batch 213\n","epoch 0 batch 214\n","epoch 0 batch 215\n","epoch 0 batch 216\n","epoch 0 batch 217\n","epoch 0 batch 218\n","epoch 0 batch 219\n","epoch 0 batch 220\n","epoch 0 batch 221\n","epoch 0 batch 222\n","epoch 0 batch 223\n","epoch 0 batch 224\n","epoch 0 batch 225\n","epoch 0 batch 226\n","epoch 0 batch 227\n","epoch 0 batch 228\n","epoch 0 batch 229\n","epoch 0 batch 230\n","epoch 0 batch 231\n","epoch 0 batch 232\n","epoch 0 batch 233\n","epoch 0 batch 234\n","epoch 0 batch 235\n","epoch 0 batch 236\n","epoch 0 batch 237\n","epoch 0 batch 238\n","epoch 0 batch 239\n","epoch 0 batch 240\n","epoch 0 batch 241\n","epoch 0 batch 242\n","epoch 0 batch 243\n","epoch 0 batch 244\n","epoch 0 batch 245\n","epoch 0 batch 246\n","epoch 0 batch 247\n","epoch 0 batch 248\n","epoch 0 batch 249\n","epoch 0 batch 250\n","epoch 0 batch 251\n","epoch 0 batch 252\n","epoch 0 batch 253\n","epoch 0 batch 254\n","epoch 0 batch 255\n","epoch 0 batch 256\n","epoch 0 batch 257\n","epoch 0 batch 258\n","epoch 0 batch 259\n","epoch 0 batch 260\n","epoch 0 batch 261\n","epoch 0 batch 262\n","epoch 0 batch 263\n","epoch 0 batch 264\n","epoch 0 batch 265\n","epoch 0 batch 266\n","epoch 0 batch 267\n","epoch 0 batch 268\n","epoch 0 batch 269\n","epoch 0 batch 270\n","epoch 0 batch 271\n","epoch 0 batch 272\n","epoch 0 batch 273\n","epoch 0 batch 274\n","epoch 0 batch 275\n","epoch 0 batch 276\n","epoch 0 batch 277\n","epoch 0 batch 278\n","epoch 0 batch 279\n","epoch 0 batch 280\n","epoch 0 batch 281\n","epoch 0 batch 282\n","epoch 0 batch 283\n","epoch 0 batch 284\n","epoch 0 batch 285\n","epoch 0 batch 286\n","epoch 0 batch 287\n","epoch 0 batch 288\n","epoch 0 batch 289\n","epoch 0 batch 290\n","epoch 0 batch 291\n","epoch 0 batch 292\n","epoch 0 batch 293\n","epoch 0 batch 294\n","epoch 0 batch 295\n","epoch 0 batch 296\n","epoch 0 batch 297\n","epoch 0 batch 298\n","epoch 0 batch 299\n","epoch 0 batch 300\n","epoch 0 batch 301\n","epoch 0 batch 302\n","epoch 0 batch 303\n","epoch 0 batch 304\n","epoch 0 batch 305\n","epoch 0 batch 306\n","epoch 0 batch 307\n","epoch 0 batch 308\n","epoch 0 batch 309\n","epoch 0 batch 310\n","epoch 0 batch 311\n","epoch 0 batch 312\n","epoch 0 batch 313\n","epoch 0 batch 314\n","epoch 0 batch 315\n","epoch 0 batch 316\n","epoch 0 batch 317\n","epoch 0 batch 318\n","epoch 0 batch 319\n","epoch 0 batch 320\n","epoch 0 batch 321\n","epoch 0 batch 322\n","epoch 0 batch 323\n","epoch 0 batch 324\n","epoch 0 batch 325\n","epoch 0 batch 326\n","epoch 0 batch 327\n","epoch 0 batch 328\n","epoch 0 batch 329\n","epoch 0 batch 330\n","epoch 0 batch 331\n","epoch 0 batch 332\n","epoch 0 batch 333\n","epoch 0 batch 334\n","epoch 0 batch 335\n","epoch 0 batch 336\n","epoch 0 batch 337\n","epoch 0 batch 338\n","epoch 0 batch 339\n","epoch 0 batch 340\n","epoch 0 batch 341\n","epoch 0 batch 342\n","epoch 0 batch 343\n","epoch 0 batch 344\n","epoch 0 batch 345\n","epoch 0 batch 346\n","epoch 0 batch 347\n","epoch 0 batch 348\n","epoch 0 batch 349\n","epoch 0 batch 350\n","epoch 0 batch 351\n","epoch 0 batch 352\n","epoch 0 batch 353\n","epoch 0 batch 354\n","epoch 0 batch 355\n","epoch 0 batch 356\n","epoch 0 batch 357\n","epoch 0 batch 358\n","epoch 0 batch 359\n","epoch 0 batch 360\n","epoch 0 batch 361\n","epoch 0 batch 362\n","epoch 0 batch 363\n","epoch 0 batch 364\n","epoch 0 batch 365\n","epoch 0 batch 366\n","epoch 0 batch 367\n","epoch 0 batch 368\n","epoch 0 batch 369\n","epoch 0 batch 370\n","epoch 0 batch 371\n","epoch 0 batch 372\n","epoch 0 batch 373\n","epoch 0 batch 374\n","epoch 0 batch 375\n","epoch 0 batch 376\n","epoch 0 batch 377\n","epoch 0 batch 378\n","epoch 0 batch 379\n","epoch 0 batch 380\n","epoch 0 batch 381\n","epoch 0 batch 382\n","epoch 0 batch 383\n","epoch 0 batch 384\n","epoch 0 batch 385\n","epoch 0 batch 386\n","epoch 0 batch 387\n","epoch 0 batch 388\n","epoch 0 batch 389\n","epoch 0 batch 390\n","epoch 0 batch 391\n"]}],"source":["#### START MODEL TRAINING\n","\n","if not test_only:\n","    \n","    model = new_model()\n","    model.to(device)\n","    torch.compile(model)\n","    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n","\n","    scheduler = torch.optim.lr_scheduler.StepLR(\n","        optimizer,\n","        gamma = 0.9,\n","        step_size = 5,\n","    )\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    \n","    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n","    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n","    print(f\"{device = }\")\n","    \n","    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n","    \n","    # plots\n","#     fig, ax = plt.subplots()\n","#     ax.plot(lrs)    \n","#     ax.set(xlabel='epoch', ylabel='learning rate value')\n","#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n","#     print(f\"{lrs = }\")\n","#     logger.info(f\"{lrs = }\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:21:51.171107Z","iopub.status.idle":"2024-06-10T08:21:51.171480Z","shell.execute_reply":"2024-06-10T08:21:51.171320Z","shell.execute_reply.started":"2024-06-10T08:21:51.171299Z"},"trusted":true},"outputs":[],"source":["# Plot data\n","\n","# if plot_data:\n","    \n","#     import matplotlib.pyplot as plt\n","    \n","#     pass"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:21:51.174763Z","iopub.status.idle":"2024-06-10T08:21:51.175089Z","shell.execute_reply":"2024-06-10T08:21:51.174942Z","shell.execute_reply.started":"2024-06-10T08:21:51.174923Z"},"trusted":true},"outputs":[],"source":["\n","from torchmetrics.detection import MeanAveragePrecision\n","import torchvision.transforms.functional as F\n","\n","def test(model, test_loader, device=torch.device(\"cpu\")): \n","    \n","    model.transform.image_mean  = image_mean_test\n","    model.transform.image_std = image_std_test\n","    model._skip_resize = True\n","    \n","    model.eval()\n","    num_correct = 0\n","    num_examples = 0\n","    test_loss = 0\n","    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n","    mAP = 0\n","    \n","    for i,batch in enumerate(test_loader):\n","        print(\"batch\", i)\n","        \n","        inputs = []\n","        targets = []\n","        \n","        for el in batch:       # el = (image,dict)\n","            if el[0] != None and not el[1]['labels'].numel() :\n","                inputs.append(el[0].to(device))\n","                targets.append(el[1])\n","                \n","        if len(inputs) == 0:\n","            continue\n","        \n","        output = model(inputs)\n","        # print(type(model(torch.cuda.FloatTensor(inputs))))\n","#         print(\"out :\\n\", output)\n","#         print(\"target :\\n\",targets)\n","        #     # Example output\n","        #     {'boxes': tensor([[\n","        #       0.3801,  0.3060,  3.5638,  3.0348],\n","        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n","        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n","        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n","        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n","        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n","        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n","        #     grad_fn=<IndexBackward0>)},\n","        \n","        \"\"\"\n","        scores come from RoIHeads class:\n","        pred_scores = F.softmax(class_logits, -1)\n","        after deleting empy boxes, low scored boxes and applying non-max suppression\n","        \"\"\"\n","        \n","        for dic in output:\n","            dic[\"boxes\"] = dic[\"boxes\"].to(device)\n","            dic[\"labels\"] = dic[\"labels\"].to(device)\n","            dic[\"scores\"] = dic[\"scores\"].to(device)\n","            \n","        res = metric(output,targets)\n","        mAP += res['map_75']\n","        #print(res)\n","\n","        \n","    mAP /= len(test_loader)  \n","    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n","\n","print(\"ok\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:21:51.176375Z","iopub.status.idle":"2024-06-10T08:21:51.176732Z","shell.execute_reply":"2024-06-10T08:21:51.176545Z","shell.execute_reply.started":"2024-06-10T08:21:51.176533Z"},"trusted":true},"outputs":[],"source":["# START MODEL TEST\n","\n","if do_model_test or test_only:\n","#     checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n","    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n","    test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n","    \n","    model = new_model()\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print(\"model is now using cuda\")\n","\n","    test(model.to(device), test_loader, device)\n","\n","# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n","# #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","# test(model.to(device), test_loader, device=device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-10T08:21:51.178544Z","iopub.status.idle":"2024-06-10T08:21:51.178889Z","shell.execute_reply":"2024-06-10T08:21:51.178743Z","shell.execute_reply.started":"2024-06-10T08:21:51.178730Z"},"trusted":true},"outputs":[],"source":["# TRAIN AGAIN (Continue training)\n","\n","import pickle\n","\n","if train_again:    \n","    # Load loaders\n","    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n","    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n","    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n","    print(\"Loadeders and model loaded succesfully\") \n","    \n","#     print(f\"{device = }\")\n","    \n","    model = new_model()\n","    \n","    # Load model from checkpoint\n","    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    \n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model = model.to(device)\n","    \n","    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    \n","    scheduler = torch.optim.lr_scheduler.StepLR(\n","        optimizer,\n","        gamma = 0.9,\n","        step_size = 5,\n","    )\n","    \n","    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","    \n","    training_losses = checkpoint['training_losses']\n","    validation_losses = checkpoint['validation_losses']\n","    lrs = checkpoint['lrs']\n","    epoch = checkpoint['epoch']\n","    # Resume training from a specific epoch\n","    # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n","    \n","    # Il file salvato model.tar contiene optiimzer, scheduler, loss e tanto altro\n","\n","    logger.info(f\"Continuing training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n","    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","\n","simo e' intelligente\n","\n","simo e' intelligente, \n","\n","simo e' buono, ludo e' bella,\n","enf e' attraente, simo e' buonosimo e' intelligente, \n","ludo e' intelligente\n","\n","simo e' intelligente\n","ludo e' buona\n","\n","enf e' attraente\n","\n","ludo e' buona, \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":868324,"sourceId":9988,"sourceType":"competition"},{"datasetId":5181471,"sourceId":8650287,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
