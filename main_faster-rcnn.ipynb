{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8438193,"sourceType":"datasetVersion","datasetId":5026303},{"sourceId":8516397,"sourceType":"datasetVersion","datasetId":5084559}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n## MAIN CONFIGURATIONS\nmodel_id = '1'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 10  # Number of epochs the model will train for\nbatch_size = 64\ndataset_percentage = 0.10 # Which percentage of the dataset to use. 0.03 means 4.6k images in the training\ninit_lr = 1e-4 # Initial Learning Rate\ndata_augmentation_type = 'fourier'    # Which data augmentation tecnique are we using?\n                                    # 'noaug':     no data augmentation\n                                    # 'fourier':   fourier transforms\n\n## WHAT WILL THIS SESSION DO?\ntest_only = False # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\ndo_model_test = False # Tests the model after training\ncreate_log_file = True\nplot_data = True # Plot training data of '{model_filepath}/model.tar'\n\n\nmodel_filepath = f\"model_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\n# !tree # Prints folder structure\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:06:14.980070Z","iopub.execute_input":"2024-06-06T09:06:14.981095Z","iopub.status.idle":"2024-06-06T09:06:14.995186Z","shell.execute_reply.started":"2024-06-06T09:06:14.981060Z","shell.execute_reply":"2024-06-06T09:06:14.994241Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_fourier_id1'\n","output_type":"stream"}]},{"cell_type":"code","source":"z\n\n# !pip install torchsummary\n# from torchsummary import summary\n# !pip install pycocotools faster-coco-eval\n# !pip install torchmetrics\n# !pip install torchmetrics[detection]","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-06T09:15:53.928958Z","iopub.execute_input":"2024-06-06T09:15:53.929368Z","iopub.status.idle":"2024-06-06T09:15:53.936410Z","shell.execute_reply.started":"2024-06-06T09:15:53.929319Z","shell.execute_reply":"2024-06-06T09:15:53.935375Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip install torchmetrics\n!pip install torchmetrics[detection]","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:15:56.480556Z","iopub.execute_input":"2024-06-06T09:15:56.480922Z","iopub.status.idle":"2024-06-06T09:16:21.587571Z","shell.execute_reply.started":"2024-06-06T09:15:56.480894Z","shell.execute_reply":"2024-06-06T09:16:21.586398Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2)\nCollecting pycocotools>2.0.0 (from torchmetrics[detection])\n  Using cached pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-06T09:16:21.589943Z","iopub.execute_input":"2024-06-06T09:16:21.590358Z","iopub.status.idle":"2024-06-06T09:16:21.597032Z","shell.execute_reply.started":"2024-06-06T09:16:21.590296Z","shell.execute_reply":"2024-06-06T09:16:21.596054Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"device = device(type='cuda')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\n\nif log_filepath:\n    print(log_filepath)\nelse:\n    print(\"No logging\")","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-06-06T09:16:21.598274Z","iopub.execute_input":"2024-06-06T09:16:21.598621Z","iopub.status.idle":"2024-06-06T09:16:21.609274Z","shell.execute_reply.started":"2024-06-06T09:16:21.598597Z","shell.execute_reply":"2024-06-06T09:16:21.608323Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_fourier_id1/log.txt'\nmodels/model_fourier_id1/log.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# FOURIER DATA AUGMENTATION\n\nimport torch.fft as fft\nimport torchvision\nimport random\n\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n        # Scale the image in range [0,1)\n        min_val = 0\n        max_val = 255\n        image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n        min_val = 0\n        max_val = 255\n        # Scale the image in range [0,1)\n        image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:16:21.611727Z","iopub.execute_input":"2024-06-06T09:16:21.612007Z","iopub.status.idle":"2024-06-06T09:16:21.654299Z","shell.execute_reply.started":"2024-06-06T09:16:21.611980Z","shell.execute_reply":"2024-06-06T09:16:21.653287Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n        \n        img_train_transforms = v2.Compose([\n             v2.RandomRotation(50),\n             v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n             v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n    \n    case 'fourier':\n        \n        img_train_transforms = v2.Compose([\n             FourierRandomNoise(),\n             PatchGaussian(),\n             FourierBasisAugmentation(),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n\nprint('ok')","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-06-06T09:16:21.655588Z","iopub.execute_input":"2024-06-06T09:16:21.655902Z","iopub.status.idle":"2024-06-06T09:16:21.669519Z","shell.execute_reply.started":"2024-06-06T09:16:21.655878Z","shell.execute_reply":"2024-06-06T09:16:21.668535Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        try:\n            label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n            label['boxes'] = torch.Tensor(label['boxes'])\n            label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(image.permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            image, label = self.transform(image, label)\n\n        return image, label\n\nprint('ok')","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","id":"V1Q6ogjksMqE","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-06T09:16:21.670853Z","iopub.execute_input":"2024-06-06T09:16:21.671179Z","iopub.status.idle":"2024-06-06T09:16:21.687222Z","shell.execute_reply.started":"2024-06-06T09:16:21.671149Z","shell.execute_reply":"2024-06-06T09:16:21.686311Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"if not test_only:\n\n    from sklearn.model_selection import train_test_split\n    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n    from torchvision import tv_tensors\n\n    # DATASET_DIR = os.path.join(\".\")\n    TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n    TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n    # print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\n    train_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n    train_list, test_list = train_test_split(train_list, test_size = float(1 - dataset_percentage)) # check first cell\n    train_list, val_list = train_test_split(train_list, test_size = 0.2)\n    test_list, _ = train_test_split(test_list, test_size = 0.99)\n    test_list, _ = train_test_split(test_list, test_size = 0.5)\n\n    train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n    test_data = ShipsDataset(test_list, transforms = img_test_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n    val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\n    def custom_collate_fn(batch):\n        # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n        # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n        # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n\n        return batch\n\n    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n    val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n    \n    print(len(train_data),len(train_loader))\n    print(len(val_data), len(val_loader))\n    print(len(test_loader))\n\n    # https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n    # La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n    # /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-06-06T09:16:21.688393Z","iopub.execute_input":"2024-06-06T09:16:21.688699Z","iopub.status.idle":"2024-06-06T09:18:16.520193Z","shell.execute_reply.started":"2024-06-06T09:16:21.688675Z","shell.execute_reply":"2024-06-06T09:18:16.519260Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"15404 241\n3851 61\n14\n","output_type":"stream"}]},{"cell_type":"code","source":"if not test_only:\n    # Save loaders\n    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\n    print('Dataset Loaders saved succesfully!')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:18:16.521350Z","iopub.execute_input":"2024-06-06T09:18:16.521613Z","iopub.status.idle":"2024-06-06T09:19:29.188342Z","shell.execute_reply.started":"2024-06-06T09:18:16.521589Z","shell.execute_reply":"2024-06-06T09:19:29.187460Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Dataset Loaders saved succesfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:19:29.189813Z","iopub.execute_input":"2024-06-06T09:19:29.190101Z","iopub.status.idle":"2024-06-06T09:19:29.198456Z","shell.execute_reply.started":"2024-06-06T09:19:29.190077Z","shell.execute_reply":"2024-06-06T09:19:29.197462Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0893, 0.0827, 0.0817]) original size\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0879, 0.0811, 0.0800]) 224x224\n\n# image_mean_train, image_std_train = get_mean_std(train_loader)\n# image_mean_val, image_std_val = get_mean_std(val_loader)\n# image_mean_test, image_std_test = get_mean_std(test_loader)\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n\n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n    \n    case 'fourier':\n        \n        image_mean_train = torch.Tensor([0.0954, 0.0930, 0.0948])\n        image_std_train = torch.Tensor([0.1039, 0.1016, 0.1036])\n\n        image_mean_val = torch.Tensor([0.1925, 0.2728, 0.3091])\n        image_std_val = torch.Tensor([0.0881, 0.0804, 0.0795])\n\n        image_mean_test = torch.tensor([0.1985, 0.2808, 0.3167])\n        image_std_test = torch.tensor([0.0846, 0.0770, 0.0768])\n\nprint(f\"{image_mean_train = }, {image_std_train =}\")\nprint(f\"{image_mean_val = }, {image_std_val =}\")\nprint(f\"{image_mean_test = }, {image_std_test =}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:19:46.959978Z","iopub.execute_input":"2024-06-06T09:19:46.960946Z","iopub.status.idle":"2024-06-06T09:19:47.030317Z","shell.execute_reply.started":"2024-06-06T09:19:46.960909Z","shell.execute_reply":"2024-06-06T09:19:47.029403Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"image_mean_train = tensor([0.0954, 0.0930, 0.0948]), image_std_train =tensor([0.1039, 0.1016, 0.1036])\nimage_mean_val = tensor([0.1925, 0.2728, 0.3091]), image_std_val =tensor([0.0881, 0.0804, 0.0795])\nimage_mean_test = tensor([0.1985, 0.2808, 0.3167]), image_std_test =tensor([0.0846, 0.0770, 0.0768])\n","output_type":"stream"}]},{"cell_type":"code","source":"def new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\nmodel_rcnn = new_model()\n\nprint('ok')","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","id":"5J9M_bnAxnDk","execution":{"iopub.status.busy":"2024-06-06T09:19:48.821960Z","iopub.execute_input":"2024-06-06T09:19:48.822317Z","iopub.status.idle":"2024-06-06T09:19:50.614914Z","shell.execute_reply.started":"2024-06-06T09:19:48.822289Z","shell.execute_reply":"2024-06-06T09:19:50.613978Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 163MB/s] \n","output_type":"stream"},{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")\n\nprint(\"ok\")","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","id":"Du5q6_RRCmD4","execution":{"iopub.status.busy":"2024-06-06T09:19:51.858502Z","iopub.execute_input":"2024-06-06T09:19:51.859109Z","iopub.status.idle":"2024-06-06T09:19:51.865973Z","shell.execute_reply.started":"2024-06-06T09:19:51.859080Z","shell.execute_reply":"2024-06-06T09:19:51.865013Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\nif not test_only:\n    \n    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n\n    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n\n        model.transform.image_mean = image_mean_train\n        model.transform.image_std = image_std_train\n        model._skip_resize = True\n\n        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n\n            training_loss = 0.0\n            batch_cumsum = 0\n            model.train()\n\n            for i, batch in enumerate(train_loader):\n                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n                print(f\"epoch {epoch} batch {i}\")\n                batch_cumsum += len(batch) # needed to compute the training loss later\n                optimizer.zero_grad()\n\n                # filtering out empty images (model does not accept empty targets)\n                inputs = []\n                targets = []\n                for el in batch:       # el = ((image,dict),dict) when transforms are active\n                    if el[1]['boxes'].size()[0] != 0:\n                        inputs.append(el[0].to(device))\n                        targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n\n                        # print(f\"{el = }\")\n                        # Example el\n                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                        #          ...,\n                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                        #          ...,\n                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                        #          ...,\n                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n\n                if len(inputs) == 0:\n                    continue\n\n                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n                \"\"\" EXAMPLE :\n                {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n                 'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n                 'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n\n                 How losses are computed:\n\n                 -loss_classifier-\n                 classification_loss = F.cross_entropy(class_logits, labels)\n\n                 -loss_box_reg-\n                 box_loss = F.smooth_l1_loss(\n                    box_regression[sampled_pos_inds_subset, labels_pos],\n                    regression_targets[sampled_pos_inds_subset],\n                    beta=1 / 9,\n                    reduction=\"sum\",\n                )\n                box_loss = box_loss / labels.numel()\n\n                -loss_rpn_box_reg-\n                box_loss = F.smooth_l1_loss(\n                pred_bbox_deltas[sampled_pos_inds],\n                regression_targets[sampled_pos_inds],\n                beta=1 / 9,\n                reduction=\"sum\",\n                ) / (sampled_inds.numel())\n\n                -loss_objectness-\n                objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n                 \"\"\"\n\n                loss = sum(loss for loss in output.values())\n                loss.backward()\n                optimizer.step()\n                training_loss += loss.data.item() * len(batch)\n\n    #             del inputs\n    #             del targets\n    #             gc.collect()    \n\n            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n            scheduler.step() # changes LR\n            training_loss /= batch_cumsum\n            training_losses.append(training_loss)\n            # save_checkpoint(epoch, model, optimizer, scheduler, training_loss, lrs)\n\n            # VALIDATION\n            model.transform.image_mean = image_mean_val\n            model.transform.image_std = image_std_val\n\n            model.train()\n            num_correct = 0\n            num_examples = 0\n            valid_loss = 0\n\n            with torch.no_grad():\n                for i,batch in enumerate(val_loader):\n                    print(\"batch\", i)\n                    inputs = []\n                    targets = []\n\n                    for el in batch:       # el = (image,labels)\n                        if el[1]['boxes'].size()[0] != 0:\n                            inputs.append(el[0].to(device))\n                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n\n                    if len(inputs) == 0:\n                        continue\n\n                    output = model(inputs, targets)\n\n                    loss = sum(loss for loss in output.values())\n                    valid_loss += loss.data.item() *len(batch)\n\n            valid_loss /= len(val_loader.dataset)\n            validation_losses.append(valid_loss)\n\n            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs)\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5\nprint('ok')","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-06-06T09:19:53.338357Z","iopub.execute_input":"2024-06-06T09:19:53.338792Z","iopub.status.idle":"2024-06-06T09:19:53.361515Z","shell.execute_reply.started":"2024-06-06T09:19:53.338763Z","shell.execute_reply":"2024-06-06T09:19:53.360601Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n    # plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:19:56.604757Z","iopub.execute_input":"2024-06-06T09:19:56.605645Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Beginning training, num_epochs = 10, data_augmentation_type = 'fourier'\ndevice = device(type='cuda')\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"epoch 0 batch 0\nepoch 0 batch 1\nepoch 0 batch 2\nepoch 0 batch 3\nepoch 0 batch 4\nepoch 0 batch 5\nepoch 0 batch 6\nepoch 0 batch 7\nepoch 0 batch 8\nepoch 0 batch 9\nepoch 0 batch 10\nepoch 0 batch 11\nepoch 0 batch 12\nepoch 0 batch 13\nepoch 0 batch 14\nepoch 0 batch 15\nepoch 0 batch 16\nepoch 0 batch 17\nepoch 0 batch 18\nepoch 0 batch 19\nepoch 0 batch 20\nepoch 0 batch 21\nepoch 0 batch 22\nepoch 0 batch 23\nepoch 0 batch 24\nepoch 0 batch 25\nepoch 0 batch 26\nepoch 0 batch 27\nepoch 0 batch 28\nepoch 0 batch 29\nepoch 0 batch 30\nepoch 0 batch 31\nepoch 0 batch 32\nepoch 0 batch 33\nepoch 0 batch 34\nepoch 0 batch 35\nepoch 0 batch 36\nepoch 0 batch 37\nepoch 0 batch 38\nepoch 0 batch 39\nepoch 0 batch 40\nepoch 0 batch 41\nepoch 0 batch 42\nepoch 0 batch 43\nepoch 0 batch 44\nepoch 0 batch 45\nepoch 0 batch 46\nepoch 0 batch 47\nepoch 0 batch 48\nepoch 0 batch 49\nepoch 0 batch 50\nepoch 0 batch 51\nepoch 0 batch 52\nepoch 0 batch 53\nepoch 0 batch 54\nepoch 0 batch 55\nepoch 0 batch 56\nepoch 0 batch 57\nepoch 0 batch 58\nepoch 0 batch 59\nepoch 0 batch 60\nepoch 0 batch 61\nepoch 0 batch 62\nepoch 0 batch 63\nepoch 0 batch 64\nepoch 0 batch 65\nepoch 0 batch 66\nepoch 0 batch 67\nepoch 0 batch 68\nepoch 0 batch 69\nepoch 0 batch 70\nepoch 0 batch 71\nepoch 0 batch 72\nepoch 0 batch 73\nepoch 0 batch 74\nepoch 0 batch 75\nepoch 0 batch 76\nepoch 0 batch 77\nepoch 0 batch 78\nepoch 0 batch 79\nepoch 0 batch 80\nepoch 0 batch 81\nepoch 0 batch 82\nepoch 0 batch 83\nepoch 0 batch 84\nepoch 0 batch 85\nepoch 0 batch 86\nepoch 0 batch 87\nepoch 0 batch 88\nepoch 0 batch 89\nepoch 0 batch 90\nepoch 0 batch 91\nepoch 0 batch 92\nepoch 0 batch 93\nepoch 0 batch 94\nepoch 0 batch 95\nepoch 0 batch 96\nepoch 0 batch 97\nepoch 0 batch 98\nepoch 0 batch 99\nepoch 0 batch 100\nepoch 0 batch 101\nepoch 0 batch 102\nepoch 0 batch 103\nepoch 0 batch 104\nepoch 0 batch 105\nepoch 0 batch 106\nepoch 0 batch 107\nepoch 0 batch 108\nepoch 0 batch 109\nepoch 0 batch 110\nepoch 0 batch 111\nepoch 0 batch 112\nepoch 0 batch 113\nepoch 0 batch 114\nepoch 0 batch 115\nepoch 0 batch 116\nepoch 0 batch 117\nepoch 0 batch 118\nepoch 0 batch 119\nepoch 0 batch 120\nepoch 0 batch 121\nepoch 0 batch 122\nepoch 0 batch 123\nepoch 0 batch 124\nepoch 0 batch 125\nepoch 0 batch 126\nepoch 0 batch 127\nepoch 0 batch 128\nepoch 0 batch 129\nepoch 0 batch 130\nepoch 0 batch 131\nepoch 0 batch 132\nepoch 0 batch 133\nepoch 0 batch 134\nepoch 0 batch 135\nepoch 0 batch 136\nepoch 0 batch 137\nepoch 0 batch 138\nepoch 0 batch 139\nepoch 0 batch 140\nepoch 0 batch 141\nepoch 0 batch 142\nepoch 0 batch 143\nepoch 0 batch 144\nepoch 0 batch 145\nepoch 0 batch 146\nepoch 0 batch 147\nepoch 0 batch 148\nepoch 0 batch 149\nepoch 0 batch 150\nepoch 0 batch 151\nepoch 0 batch 152\nepoch 0 batch 153\nepoch 0 batch 154\nepoch 0 batch 155\nepoch 0 batch 156\nepoch 0 batch 157\nepoch 0 batch 158\nepoch 0 batch 159\nepoch 0 batch 160\nepoch 0 batch 161\nepoch 0 batch 162\nepoch 0 batch 163\nepoch 0 batch 164\nepoch 0 batch 165\nepoch 0 batch 166\nepoch 0 batch 167\nepoch 0 batch 168\nepoch 0 batch 169\nepoch 0 batch 170\nepoch 0 batch 171\nepoch 0 batch 172\nepoch 0 batch 173\nepoch 0 batch 174\nepoch 0 batch 175\nepoch 0 batch 176\nepoch 0 batch 177\nepoch 0 batch 178\nepoch 0 batch 179\nepoch 0 batch 180\nepoch 0 batch 181\nepoch 0 batch 182\nepoch 0 batch 183\nepoch 0 batch 184\nepoch 0 batch 185\nepoch 0 batch 186\nepoch 0 batch 187\nepoch 0 batch 188\nepoch 0 batch 189\nepoch 0 batch 190\nepoch 0 batch 191\nepoch 0 batch 192\nepoch 0 batch 193\nepoch 0 batch 194\nepoch 0 batch 195\nepoch 0 batch 196\nepoch 0 batch 197\nepoch 0 batch 198\nepoch 0 batch 199\nepoch 0 batch 200\nepoch 0 batch 201\nepoch 0 batch 202\nepoch 0 batch 203\nepoch 0 batch 204\nepoch 0 batch 205\nepoch 0 batch 206\nepoch 0 batch 207\nepoch 0 batch 208\nepoch 0 batch 209\nepoch 0 batch 210\nepoch 0 batch 211\nepoch 0 batch 212\nepoch 0 batch 213\nepoch 0 batch 214\nepoch 0 batch 215\nepoch 0 batch 216\nepoch 0 batch 217\nepoch 0 batch 218\nepoch 0 batch 219\nepoch 0 batch 220\nepoch 0 batch 221\nepoch 0 batch 222\nepoch 0 batch 223\nepoch 0 batch 224\nepoch 0 batch 225\nepoch 0 batch 226\nepoch 0 batch 227\nepoch 0 batch 228\nepoch 0 batch 229\nepoch 0 batch 230\nepoch 0 batch 231\nepoch 0 batch 232\nepoch 0 batch 233\nepoch 0 batch 234\nepoch 0 batch 235\nepoch 0 batch 236\nepoch 0 batch 237\nepoch 0 batch 238\nepoch 0 batch 239\nepoch 0 batch 240\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nbatch 46\nbatch 47\nbatch 48\nbatch 49\nbatch 50\nbatch 51\nbatch 52\nbatch 53\nbatch 54\nbatch 55\nbatch 56\nbatch 57\nbatch 58\nbatch 59\nbatch 60\nEpoch: 0, Training Loss: 4.4905, Validation Loss: 4.5718, lr: 0.00010000\nSaved model\nepoch 1 batch 0\nepoch 1 batch 1\nepoch 1 batch 2\nepoch 1 batch 3\nepoch 1 batch 4\nepoch 1 batch 5\nepoch 1 batch 6\nepoch 1 batch 7\nepoch 1 batch 8\nepoch 1 batch 9\nepoch 1 batch 10\nepoch 1 batch 11\nepoch 1 batch 12\nepoch 1 batch 13\nepoch 1 batch 14\nepoch 1 batch 15\nepoch 1 batch 16\nepoch 1 batch 17\nepoch 1 batch 18\nepoch 1 batch 19\nepoch 1 batch 20\nepoch 1 batch 21\nepoch 1 batch 22\nepoch 1 batch 23\nepoch 1 batch 24\nepoch 1 batch 25\nepoch 1 batch 26\nepoch 1 batch 27\nepoch 1 batch 28\nepoch 1 batch 29\nepoch 1 batch 30\nepoch 1 batch 31\nepoch 1 batch 32\nepoch 1 batch 33\nepoch 1 batch 34\nepoch 1 batch 35\nepoch 1 batch 36\nepoch 1 batch 37\nepoch 1 batch 38\nepoch 1 batch 39\nepoch 1 batch 40\nepoch 1 batch 41\nepoch 1 batch 42\nepoch 1 batch 43\nepoch 1 batch 44\nepoch 1 batch 45\nepoch 1 batch 46\nepoch 1 batch 47\nepoch 1 batch 48\nepoch 1 batch 49\nepoch 1 batch 50\nepoch 1 batch 51\nepoch 1 batch 52\nepoch 1 batch 53\nepoch 1 batch 54\nepoch 1 batch 55\nepoch 1 batch 56\nepoch 1 batch 57\nepoch 1 batch 58\nepoch 1 batch 59\nepoch 1 batch 60\nepoch 1 batch 61\nepoch 1 batch 62\nepoch 1 batch 63\nepoch 1 batch 64\nepoch 1 batch 65\nepoch 1 batch 66\nepoch 1 batch 67\nepoch 1 batch 68\nepoch 1 batch 69\nepoch 1 batch 70\nepoch 1 batch 71\nepoch 1 batch 72\nepoch 1 batch 73\nepoch 1 batch 74\nepoch 1 batch 75\nepoch 1 batch 76\nepoch 1 batch 77\nepoch 1 batch 78\nepoch 1 batch 79\nepoch 1 batch 80\nepoch 1 batch 81\nepoch 1 batch 82\nepoch 1 batch 83\nepoch 1 batch 84\nepoch 1 batch 85\nepoch 1 batch 86\nepoch 1 batch 87\nepoch 1 batch 88\nepoch 1 batch 89\nepoch 1 batch 90\nepoch 1 batch 91\nepoch 1 batch 92\nepoch 1 batch 93\nepoch 1 batch 94\nepoch 1 batch 95\nepoch 1 batch 96\nepoch 1 batch 97\nepoch 1 batch 98\nepoch 1 batch 99\nepoch 1 batch 100\nepoch 1 batch 101\nepoch 1 batch 102\nepoch 1 batch 103\nepoch 1 batch 104\nepoch 1 batch 105\nepoch 1 batch 106\nepoch 1 batch 107\nepoch 1 batch 108\nepoch 1 batch 109\nepoch 1 batch 110\nepoch 1 batch 111\nepoch 1 batch 112\nepoch 1 batch 113\nepoch 1 batch 114\nepoch 1 batch 115\nepoch 1 batch 116\nepoch 1 batch 117\nepoch 1 batch 118\nepoch 1 batch 119\nepoch 1 batch 120\nepoch 1 batch 121\nepoch 1 batch 122\nepoch 1 batch 123\nepoch 1 batch 124\nepoch 1 batch 125\nepoch 1 batch 126\nepoch 1 batch 127\nepoch 1 batch 128\nepoch 1 batch 129\nepoch 1 batch 130\nepoch 1 batch 131\nepoch 1 batch 132\nepoch 1 batch 133\nepoch 1 batch 134\nepoch 1 batch 135\nepoch 1 batch 136\nepoch 1 batch 137\nepoch 1 batch 138\nepoch 1 batch 139\nepoch 1 batch 140\nepoch 1 batch 141\nepoch 1 batch 142\nepoch 1 batch 143\nepoch 1 batch 144\nepoch 1 batch 145\nepoch 1 batch 146\nepoch 1 batch 147\nepoch 1 batch 148\nepoch 1 batch 149\nepoch 1 batch 150\nepoch 1 batch 151\nepoch 1 batch 152\nepoch 1 batch 153\nepoch 1 batch 154\nepoch 1 batch 155\nepoch 1 batch 156\nepoch 1 batch 157\nepoch 1 batch 158\nepoch 1 batch 159\nepoch 1 batch 160\nepoch 1 batch 161\nepoch 1 batch 162\nepoch 1 batch 163\nepoch 1 batch 164\nepoch 1 batch 165\nepoch 1 batch 166\nepoch 1 batch 167\nepoch 1 batch 168\nepoch 1 batch 169\nepoch 1 batch 170\nepoch 1 batch 171\nepoch 1 batch 172\nepoch 1 batch 173\nepoch 1 batch 174\nepoch 1 batch 175\nepoch 1 batch 176\nepoch 1 batch 177\nepoch 1 batch 178\nepoch 1 batch 179\nepoch 1 batch 180\nepoch 1 batch 181\nepoch 1 batch 182\nepoch 1 batch 183\nepoch 1 batch 184\nepoch 1 batch 185\nepoch 1 batch 186\nepoch 1 batch 187\nepoch 1 batch 188\nepoch 1 batch 189\nepoch 1 batch 190\nepoch 1 batch 191\nepoch 1 batch 192\nepoch 1 batch 193\nepoch 1 batch 194\nepoch 1 batch 195\nepoch 1 batch 196\nepoch 1 batch 197\nepoch 1 batch 198\nepoch 1 batch 199\nepoch 1 batch 200\nepoch 1 batch 201\nepoch 1 batch 202\nepoch 1 batch 203\nepoch 1 batch 204\nepoch 1 batch 205\nepoch 1 batch 206\nepoch 1 batch 207\nepoch 1 batch 208\nepoch 1 batch 209\nepoch 1 batch 210\nepoch 1 batch 211\nepoch 1 batch 212\nepoch 1 batch 213\nepoch 1 batch 214\nepoch 1 batch 215\nepoch 1 batch 216\nepoch 1 batch 217\nepoch 1 batch 218\nepoch 1 batch 219\nepoch 1 batch 220\nepoch 1 batch 221\nepoch 1 batch 222\nepoch 1 batch 223\nepoch 1 batch 224\nepoch 1 batch 225\nepoch 1 batch 226\nepoch 1 batch 227\nepoch 1 batch 228\nepoch 1 batch 229\nepoch 1 batch 230\nepoch 1 batch 231\nepoch 1 batch 232\nepoch 1 batch 233\nepoch 1 batch 234\nepoch 1 batch 235\nepoch 1 batch 236\nepoch 1 batch 237\nepoch 1 batch 238\nepoch 1 batch 239\nepoch 1 batch 240\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nbatch 46\nbatch 47\nbatch 48\nbatch 49\nbatch 50\nbatch 51\nbatch 52\nbatch 53\nbatch 54\nbatch 55\nbatch 56\nbatch 57\nbatch 58\nbatch 59\nbatch 60\nEpoch: 1, Training Loss: 4.1305, Validation Loss: 4.5620, lr: 0.00010000\nSaved model\nepoch 2 batch 0\nepoch 2 batch 1\nepoch 2 batch 2\nepoch 2 batch 3\nepoch 2 batch 4\nepoch 2 batch 5\nepoch 2 batch 6\nepoch 2 batch 7\nepoch 2 batch 8\nepoch 2 batch 9\nepoch 2 batch 10\nepoch 2 batch 11\nepoch 2 batch 12\nepoch 2 batch 13\nepoch 2 batch 14\nepoch 2 batch 15\nepoch 2 batch 16\nepoch 2 batch 17\nepoch 2 batch 18\nepoch 2 batch 19\nepoch 2 batch 20\nepoch 2 batch 21\nepoch 2 batch 22\nepoch 2 batch 23\nepoch 2 batch 24\nepoch 2 batch 25\nepoch 2 batch 26\nepoch 2 batch 27\nepoch 2 batch 28\nepoch 2 batch 29\nepoch 2 batch 30\nepoch 2 batch 31\nepoch 2 batch 32\nepoch 2 batch 33\nepoch 2 batch 34\nepoch 2 batch 35\nepoch 2 batch 36\nepoch 2 batch 37\nepoch 2 batch 38\nepoch 2 batch 39\nepoch 2 batch 40\nepoch 2 batch 41\nepoch 2 batch 42\nepoch 2 batch 43\nepoch 2 batch 44\nepoch 2 batch 45\nepoch 2 batch 46\nepoch 2 batch 47\nepoch 2 batch 48\nepoch 2 batch 49\nepoch 2 batch 50\nepoch 2 batch 51\nepoch 2 batch 52\nepoch 2 batch 53\nepoch 2 batch 54\nepoch 2 batch 55\nepoch 2 batch 56\nepoch 2 batch 57\nepoch 2 batch 58\nepoch 2 batch 59\nepoch 2 batch 60\nepoch 2 batch 61\nepoch 2 batch 62\nepoch 2 batch 63\nepoch 2 batch 64\nepoch 2 batch 65\nepoch 2 batch 66\nepoch 2 batch 67\nepoch 2 batch 68\nepoch 2 batch 69\nepoch 2 batch 70\nepoch 2 batch 71\nepoch 2 batch 72\nepoch 2 batch 73\nepoch 2 batch 74\nepoch 2 batch 75\nepoch 2 batch 76\nepoch 2 batch 77\nepoch 2 batch 78\nepoch 2 batch 79\nepoch 2 batch 80\nepoch 2 batch 81\nepoch 2 batch 82\nepoch 2 batch 83\nepoch 2 batch 84\nepoch 2 batch 85\nepoch 2 batch 86\nepoch 2 batch 87\nepoch 2 batch 88\nepoch 2 batch 89\nepoch 2 batch 90\nepoch 2 batch 91\nepoch 2 batch 92\nepoch 2 batch 93\nepoch 2 batch 94\nepoch 2 batch 95\nepoch 2 batch 96\nepoch 2 batch 97\nepoch 2 batch 98\nepoch 2 batch 99\nepoch 2 batch 100\nepoch 2 batch 101\nepoch 2 batch 102\nepoch 2 batch 103\nepoch 2 batch 104\nepoch 2 batch 105\nepoch 2 batch 106\nepoch 2 batch 107\nepoch 2 batch 108\nepoch 2 batch 109\nepoch 2 batch 110\nepoch 2 batch 111\nepoch 2 batch 112\nepoch 2 batch 113\nepoch 2 batch 114\nepoch 2 batch 115\nepoch 2 batch 116\nepoch 2 batch 117\nepoch 2 batch 118\nepoch 2 batch 119\nepoch 2 batch 120\nepoch 2 batch 121\nepoch 2 batch 122\nepoch 2 batch 123\nepoch 2 batch 124\nepoch 2 batch 125\nepoch 2 batch 126\nepoch 2 batch 127\nepoch 2 batch 128\nepoch 2 batch 129\nepoch 2 batch 130\nepoch 2 batch 131\nepoch 2 batch 132\nepoch 2 batch 133\nepoch 2 batch 134\nepoch 2 batch 135\nepoch 2 batch 136\nepoch 2 batch 137\nepoch 2 batch 138\nepoch 2 batch 139\nepoch 2 batch 140\nepoch 2 batch 141\nepoch 2 batch 142\nepoch 2 batch 143\nepoch 2 batch 144\nepoch 2 batch 145\nepoch 2 batch 146\nepoch 2 batch 147\nepoch 2 batch 148\nepoch 2 batch 149\nepoch 2 batch 150\nepoch 2 batch 151\nepoch 2 batch 152\nepoch 2 batch 153\nepoch 2 batch 154\nepoch 2 batch 155\nepoch 2 batch 156\nepoch 2 batch 157\nepoch 2 batch 158\nepoch 2 batch 159\nepoch 2 batch 160\nepoch 2 batch 161\nepoch 2 batch 162\nepoch 2 batch 163\nepoch 2 batch 164\nepoch 2 batch 165\nepoch 2 batch 166\nepoch 2 batch 167\nepoch 2 batch 168\nepoch 2 batch 169\nepoch 2 batch 170\nepoch 2 batch 171\nepoch 2 batch 172\nepoch 2 batch 173\nepoch 2 batch 174\nepoch 2 batch 175\nepoch 2 batch 176\nepoch 2 batch 177\nepoch 2 batch 178\nepoch 2 batch 179\nepoch 2 batch 180\nepoch 2 batch 181\nepoch 2 batch 182\nepoch 2 batch 183\nepoch 2 batch 184\nepoch 2 batch 185\nepoch 2 batch 186\nepoch 2 batch 187\nepoch 2 batch 188\nepoch 2 batch 189\nepoch 2 batch 190\nepoch 2 batch 191\nepoch 2 batch 192\nepoch 2 batch 193\nepoch 2 batch 194\nepoch 2 batch 195\nepoch 2 batch 196\nepoch 2 batch 197\nepoch 2 batch 198\nepoch 2 batch 199\nepoch 2 batch 200\nepoch 2 batch 201\nepoch 2 batch 202\nepoch 2 batch 203\nepoch 2 batch 204\nepoch 2 batch 205\nepoch 2 batch 206\nepoch 2 batch 207\nepoch 2 batch 208\nepoch 2 batch 209\nepoch 2 batch 210\nepoch 2 batch 211\nepoch 2 batch 212\nepoch 2 batch 213\nepoch 2 batch 214\nepoch 2 batch 215\nepoch 2 batch 216\nepoch 2 batch 217\nepoch 2 batch 218\nepoch 2 batch 219\nepoch 2 batch 220\nepoch 2 batch 221\nepoch 2 batch 222\nepoch 2 batch 223\nepoch 2 batch 224\nepoch 2 batch 225\nepoch 2 batch 226\nepoch 2 batch 227\nepoch 2 batch 228\nepoch 2 batch 229\nepoch 2 batch 230\nepoch 2 batch 231\nepoch 2 batch 232\nepoch 2 batch 233\nepoch 2 batch 234\nepoch 2 batch 235\nepoch 2 batch 236\nepoch 2 batch 237\nepoch 2 batch 238\nepoch 2 batch 239\nepoch 2 batch 240\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nbatch 46\nbatch 47\nbatch 48\nbatch 49\nbatch 50\nbatch 51\nbatch 52\nbatch 53\nbatch 54\nbatch 55\nbatch 56\nbatch 57\nbatch 58\nbatch 59\nbatch 60\nEpoch: 2, Training Loss: 4.1271, Validation Loss: 4.5636, lr: 0.00010000\nSaved model\nepoch 3 batch 0\nepoch 3 batch 1\nepoch 3 batch 2\nepoch 3 batch 3\nepoch 3 batch 4\nepoch 3 batch 5\nepoch 3 batch 6\nepoch 3 batch 7\nepoch 3 batch 8\nepoch 3 batch 9\nepoch 3 batch 10\nepoch 3 batch 11\nepoch 3 batch 12\nepoch 3 batch 13\nepoch 3 batch 14\nepoch 3 batch 15\nepoch 3 batch 16\nepoch 3 batch 17\nepoch 3 batch 18\nepoch 3 batch 19\nepoch 3 batch 20\nepoch 3 batch 21\nepoch 3 batch 22\nepoch 3 batch 23\nepoch 3 batch 24\nepoch 3 batch 25\nepoch 3 batch 26\nepoch 3 batch 27\nepoch 3 batch 28\nepoch 3 batch 29\nepoch 3 batch 30\nepoch 3 batch 31\nepoch 3 batch 32\nepoch 3 batch 33\nepoch 3 batch 34\nepoch 3 batch 35\nepoch 3 batch 36\nepoch 3 batch 37\nepoch 3 batch 38\nepoch 3 batch 39\nepoch 3 batch 40\nepoch 3 batch 41\nepoch 3 batch 42\nepoch 3 batch 43\nepoch 3 batch 44\nepoch 3 batch 45\nepoch 3 batch 46\nepoch 3 batch 47\nepoch 3 batch 48\nepoch 3 batch 49\nepoch 3 batch 50\nepoch 3 batch 51\nepoch 3 batch 52\nepoch 3 batch 53\nepoch 3 batch 54\nepoch 3 batch 55\nepoch 3 batch 56\nepoch 3 batch 57\nepoch 3 batch 58\nepoch 3 batch 59\nepoch 3 batch 60\nepoch 3 batch 61\nepoch 3 batch 62\nepoch 3 batch 63\nepoch 3 batch 64\nepoch 3 batch 65\nepoch 3 batch 66\nepoch 3 batch 67\nepoch 3 batch 68\nepoch 3 batch 69\nepoch 3 batch 70\nepoch 3 batch 71\nepoch 3 batch 72\nepoch 3 batch 73\nepoch 3 batch 74\nepoch 3 batch 75\nepoch 3 batch 76\nepoch 3 batch 77\nepoch 3 batch 78\nepoch 3 batch 79\nepoch 3 batch 80\nepoch 3 batch 81\nepoch 3 batch 82\nepoch 3 batch 83\nepoch 3 batch 84\nepoch 3 batch 85\nepoch 3 batch 86\nepoch 3 batch 87\nepoch 3 batch 88\nepoch 3 batch 89\nepoch 3 batch 90\nepoch 3 batch 91\nepoch 3 batch 92\nepoch 3 batch 93\nepoch 3 batch 94\nepoch 3 batch 95\nepoch 3 batch 96\nepoch 3 batch 97\nepoch 3 batch 98\nepoch 3 batch 99\nepoch 3 batch 100\nepoch 3 batch 101\nepoch 3 batch 102\nepoch 3 batch 103\nepoch 3 batch 104\nepoch 3 batch 105\nepoch 3 batch 106\nepoch 3 batch 107\nepoch 3 batch 108\nepoch 3 batch 109\nepoch 3 batch 110\nepoch 3 batch 111\nepoch 3 batch 112\nepoch 3 batch 113\nepoch 3 batch 114\nepoch 3 batch 115\nepoch 3 batch 116\nepoch 3 batch 117\nepoch 3 batch 118\nepoch 3 batch 119\nepoch 3 batch 120\nepoch 3 batch 121\nepoch 3 batch 122\nepoch 3 batch 123\nepoch 3 batch 124\nepoch 3 batch 125\nepoch 3 batch 126\nepoch 3 batch 127\nepoch 3 batch 128\nepoch 3 batch 129\nepoch 3 batch 130\nepoch 3 batch 131\nepoch 3 batch 132\nepoch 3 batch 133\nepoch 3 batch 134\nepoch 3 batch 135\nepoch 3 batch 136\nepoch 3 batch 137\nepoch 3 batch 138\nepoch 3 batch 139\nepoch 3 batch 140\nepoch 3 batch 141\nepoch 3 batch 142\nepoch 3 batch 143\nepoch 3 batch 144\nepoch 3 batch 145\nepoch 3 batch 146\nepoch 3 batch 147\nepoch 3 batch 148\nepoch 3 batch 149\nepoch 3 batch 150\nepoch 3 batch 151\nepoch 3 batch 152\nepoch 3 batch 153\nepoch 3 batch 154\nepoch 3 batch 155\nepoch 3 batch 156\nepoch 3 batch 157\nepoch 3 batch 158\nepoch 3 batch 159\nepoch 3 batch 160\nepoch 3 batch 161\nepoch 3 batch 162\nepoch 3 batch 163\nepoch 3 batch 164\nepoch 3 batch 165\nepoch 3 batch 166\nepoch 3 batch 167\nepoch 3 batch 168\nepoch 3 batch 169\nepoch 3 batch 170\nepoch 3 batch 171\nepoch 3 batch 172\nepoch 3 batch 173\nepoch 3 batch 174\nepoch 3 batch 175\nepoch 3 batch 176\nepoch 3 batch 177\nepoch 3 batch 178\nepoch 3 batch 179\nepoch 3 batch 180\nepoch 3 batch 181\nepoch 3 batch 182\nepoch 3 batch 183\nepoch 3 batch 184\nepoch 3 batch 185\nepoch 3 batch 186\nepoch 3 batch 187\nepoch 3 batch 188\nepoch 3 batch 189\nepoch 3 batch 190\nepoch 3 batch 191\nepoch 3 batch 192\nepoch 3 batch 193\nepoch 3 batch 194\nepoch 3 batch 195\nepoch 3 batch 196\nepoch 3 batch 197\nepoch 3 batch 198\nepoch 3 batch 199\nepoch 3 batch 200\nepoch 3 batch 201\nepoch 3 batch 202\nepoch 3 batch 203\nepoch 3 batch 204\nepoch 3 batch 205\nepoch 3 batch 206\nepoch 3 batch 207\nepoch 3 batch 208\nepoch 3 batch 209\nepoch 3 batch 210\nepoch 3 batch 211\nepoch 3 batch 212\nepoch 3 batch 213\nepoch 3 batch 214\nepoch 3 batch 215\nepoch 3 batch 216\nepoch 3 batch 217\nepoch 3 batch 218\nepoch 3 batch 219\nepoch 3 batch 220\nepoch 3 batch 221\nepoch 3 batch 222\nepoch 3 batch 223\nepoch 3 batch 224\nepoch 3 batch 225\nepoch 3 batch 226\nepoch 3 batch 227\nepoch 3 batch 228\nepoch 3 batch 229\nepoch 3 batch 230\nepoch 3 batch 231\nepoch 3 batch 232\nepoch 3 batch 233\nepoch 3 batch 234\nepoch 3 batch 235\nepoch 3 batch 236\nepoch 3 batch 237\nepoch 3 batch 238\nepoch 3 batch 239\nepoch 3 batch 240\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nbatch 46\nbatch 47\nbatch 48\nbatch 49\nbatch 50\nbatch 51\nbatch 52\nbatch 53\nbatch 54\nbatch 55\nbatch 56\nbatch 57\nbatch 58\nbatch 59\nbatch 60\nEpoch: 3, Training Loss: 4.1211, Validation Loss: 4.5603, lr: 0.00010000\nSaved model\nepoch 4 batch 0\nepoch 4 batch 1\nepoch 4 batch 2\nepoch 4 batch 3\nepoch 4 batch 4\nepoch 4 batch 5\nepoch 4 batch 6\nepoch 4 batch 7\nepoch 4 batch 8\nepoch 4 batch 9\nepoch 4 batch 10\nepoch 4 batch 11\nepoch 4 batch 12\nepoch 4 batch 13\nepoch 4 batch 14\nepoch 4 batch 15\nepoch 4 batch 16\nepoch 4 batch 17\nepoch 4 batch 18\nepoch 4 batch 19\nepoch 4 batch 20\nepoch 4 batch 21\nepoch 4 batch 22\nepoch 4 batch 23\nepoch 4 batch 24\nepoch 4 batch 25\nepoch 4 batch 26\nepoch 4 batch 27\nepoch 4 batch 28\nepoch 4 batch 29\nepoch 4 batch 30\nepoch 4 batch 31\nepoch 4 batch 32\nepoch 4 batch 33\nepoch 4 batch 34\nepoch 4 batch 35\nepoch 4 batch 36\nepoch 4 batch 37\nepoch 4 batch 38\nepoch 4 batch 39\nepoch 4 batch 40\nepoch 4 batch 41\nepoch 4 batch 42\nepoch 4 batch 43\nepoch 4 batch 44\nepoch 4 batch 45\nepoch 4 batch 46\nepoch 4 batch 47\nepoch 4 batch 48\nepoch 4 batch 49\nepoch 4 batch 50\nepoch 4 batch 51\nepoch 4 batch 52\nepoch 4 batch 53\nepoch 4 batch 54\nepoch 4 batch 55\nepoch 4 batch 56\nepoch 4 batch 57\nepoch 4 batch 58\nepoch 4 batch 59\nepoch 4 batch 60\nepoch 4 batch 61\nepoch 4 batch 62\nepoch 4 batch 63\nepoch 4 batch 64\nepoch 4 batch 65\nepoch 4 batch 66\nepoch 4 batch 67\nepoch 4 batch 68\nepoch 4 batch 69\nepoch 4 batch 70\nepoch 4 batch 71\nepoch 4 batch 72\nepoch 4 batch 73\nepoch 4 batch 74\nepoch 4 batch 75\nepoch 4 batch 76\nepoch 4 batch 77\nepoch 4 batch 78\nepoch 4 batch 79\nepoch 4 batch 80\nepoch 4 batch 81\nepoch 4 batch 82\nepoch 4 batch 83\nepoch 4 batch 84\nepoch 4 batch 85\nepoch 4 batch 86\nepoch 4 batch 87\nepoch 4 batch 88\nepoch 4 batch 89\nepoch 4 batch 90\nepoch 4 batch 91\nepoch 4 batch 92\nepoch 4 batch 93\nepoch 4 batch 94\nepoch 4 batch 95\nepoch 4 batch 96\nepoch 4 batch 97\nepoch 4 batch 98\nepoch 4 batch 99\nepoch 4 batch 100\nepoch 4 batch 101\nepoch 4 batch 102\nepoch 4 batch 103\nepoch 4 batch 104\nepoch 4 batch 105\nepoch 4 batch 106\nepoch 4 batch 107\nepoch 4 batch 108\nepoch 4 batch 109\nepoch 4 batch 110\nepoch 4 batch 111\nepoch 4 batch 112\nepoch 4 batch 113\nepoch 4 batch 114\nepoch 4 batch 115\nepoch 4 batch 116\nepoch 4 batch 117\nepoch 4 batch 118\nepoch 4 batch 119\nepoch 4 batch 120\nepoch 4 batch 121\nepoch 4 batch 122\nepoch 4 batch 123\nepoch 4 batch 124\nepoch 4 batch 125\nepoch 4 batch 126\nepoch 4 batch 127\nepoch 4 batch 128\nepoch 4 batch 129\nepoch 4 batch 130\nepoch 4 batch 131\nepoch 4 batch 132\nepoch 4 batch 133\nepoch 4 batch 134\nepoch 4 batch 135\nepoch 4 batch 136\nepoch 4 batch 137\nepoch 4 batch 138\n","output_type":"stream"}]},{"cell_type":"code","source":"# Plot data\n\nif plot_data:\n    \n    import matplotlib.pyplot as plt\n    \n    pass","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:19:29.219226Z","iopub.status.idle":"2024-06-06T09:19:29.219589Z","shell.execute_reply.started":"2024-06-06T09:19:29.219404Z","shell.execute_reply":"2024-06-06T09:19:29.219419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che è stata creata contenente il modello e i log\n\n# from IPython.display import FileLink\n# !zip -r file.zip {model_filepath}\n# FileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:51.687623Z","iopub.status.idle":"2024-06-06T09:13:51.687947Z","shell.execute_reply.started":"2024-06-06T09:13:51.687789Z","shell.execute_reply":"2024-06-06T09:13:51.687803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchmetrics.detection import MeanAveragePrecision\nimport torchvision.transforms.functional as F\n\ndef test(model, test_loader, device=torch.device(\"cpu\")): \n    \n    model.transform.image_mean  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in enumerate(test_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = ((image,dict),dict)\n            if el[1]['boxes'].size()[0] != 0:\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n        # print(type(model(torch.cuda.FloatTensor(inputs))))\n#         print(\"out :\\n\", output)\n#         print(\"target :\\n\",targets)\n        #     # Example output\n        #     {'boxes': tensor([[\n        #       0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(device)\n            dic[\"labels\"] = dic[\"labels\"].to(device)\n            dic[\"scores\"] = dic[\"scores\"].to(device)\n            \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:51.689257Z","iopub.status.idle":"2024-06-06T09:13:51.689622Z","shell.execute_reply.started":"2024-06-06T09:13:51.689460Z","shell.execute_reply":"2024-06-06T09:13:51.689476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n\nif do_model_test or test_only:\n#     checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n    test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n    \n    model = new_model()\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)\n\n# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n# #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\n# model.load_state_dict(checkpoint['model_state_dict'])\n# test(model.to(device), test_loader, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:51.690914Z","iopub.status.idle":"2024-06-06T09:13:51.691236Z","shell.execute_reply.started":"2024-06-06T09:13:51.691079Z","shell.execute_reply":"2024-06-06T09:13:51.691093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN AGAIN (Continue training)\n\nimport pickle\n\ntrain_again = True\n\nif train_again:    \n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n    print(\"Loadeders and model loaded succesfully\") \n    \n#     print(f\"{device = }\")\n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch']\n    # Resume training from a specific epoch\n    # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    \n    # Il file salvato model.tar contiene optiimzer, scheduler, loss e tanto altro\n\n    logger.info(f\"Continuing training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T09:13:51.692515Z","iopub.status.idle":"2024-06-06T09:13:51.692847Z","shell.execute_reply.started":"2024-06-06T09:13:51.692677Z","shell.execute_reply":"2024-06-06T09:13:51.692692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n\nsimo e' intelligente\n\nsimo e' intelligente, \n\nsimo e' buono, ludo e' bella,\nenf e' attraente, simo e' buonosimo e' intelligente, \nludo e' intelligente\n\nsimo e' intelligente\nludo e' buona\n\nenf e' attraente\n\nludo e' buona, \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{}}]}