{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8438193,"sourceType":"datasetVersion","datasetId":5026303}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# MAIN CONFIGURATIONS\ncreate_log_file = True\nsave_to_drive = False\nmodel_id = '1'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 10\nbatch_size = 32\ndata_augmentation_type = 'noaug'  # Which data augmentation tecnique are we using?\n                                  # 'noaug':     no data augmentation\n\n# !tree # Prints folder structure\n\ntest_only = True # when true it doesn't train the model, but it just tests an existing one\n\nmodel_filepath = f\"model_epochs{str(num_epochs)}_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"_uuid":"f66e8f23-9459-4a78-b42d-592a8b72fd93","_cell_guid":"1eb3601c-59c5-4c49-8c24-91184a27a7e1","collapsed":false,"id":"NFc7Y_31k39Q","outputId":"13bf00d1-bde8-49aa-b24b-2e49f08ab1ab","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:16:25.214220Z","iopub.execute_input":"2024-05-20T22:16:25.214631Z","iopub.status.idle":"2024-05-20T22:16:25.223959Z","shell.execute_reply.started":"2024-05-20T22:16:25.214597Z","shell.execute_reply":"2024-05-20T22:16:25.222526Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_epochs10_noaug_id1'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\n\n# !pip install torchsummary\n# from torchsummary import summary","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","collapsed":false,"id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:16:25.231329Z","iopub.execute_input":"2024-05-20T22:16:25.231732Z","iopub.status.idle":"2024-05-20T22:16:25.242345Z","shell.execute_reply.started":"2024-05-20T22:16:25.231701Z","shell.execute_reply":"2024-05-20T22:16:25.240994Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    logger = logging.getLogger('RootLogger')\n    log_filepath = datetime.now().strftime(\"%m-%d_%H.%M.%S\")\n    log_filepath = os.path.join(model_filepath, f\"log_{log_filepath}\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","collapsed":false,"id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:16:25.248572Z","iopub.execute_input":"2024-05-20T22:16:25.249031Z","iopub.status.idle":"2024-05-20T22:16:25.262010Z","shell.execute_reply.started":"2024-05-20T22:16:25.248986Z","shell.execute_reply":"2024-05-20T22:16:25.260585Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_epochs10_noaug_id1/log_05-20_22.16.25.txt'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\n# Normalize to the ImageNet mean and standard deviation\n# Could calculate it for the cats/dogs data set, but the ImageNet\n# values give acceptable results here.\nimg_train_transforms = v2.Compose([\n    # transforms.RandomRotation(50),\n    # transforms.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n    # transforms.RandomHorizontalFlip(p=0.5),\n    v2.Resize((img_dimensions, img_dimensions)),\n    # transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])\n\nimg_validation_transforms = v2.Compose([\n    v2.Resize((img_dimensions, img_dimensions)),\n    # transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","collapsed":false,"id":"LIgECtVqMlCI","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:16:25.264544Z","iopub.execute_input":"2024-05-20T22:16:25.264990Z","iopub.status.idle":"2024-05-20T22:16:25.275622Z","shell.execute_reply.started":"2024-05-20T22:16:25.264941Z","shell.execute_reply":"2024-05-20T22:16:25.274257Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      # mask[0, r, c+j] = 1\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        # try:\n        label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n        label['boxes'] = torch.Tensor(label['boxes'])\n        label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        # except IndexError as e:\n        #     Warning(f'Errore con {idx = }')\n        #     plt.imshow(image.permute(1, 2, 0))\n        #     plt.show()\n\n        if self.transform:\n            image = self.transform(image, label)\n\n            # prova ad indagare da qui\n            # image = self.transform(image)\n            # image = image.numpy()\n            # return image, label\n            # print(f\"{image = }\")\n            # print(f\"{label = }\")\n\n        return image, label","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","collapsed":false,"id":"V1Q6ogjksMqE","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:16:25.277956Z","iopub.execute_input":"2024-05-20T22:16:25.278446Z","iopub.status.idle":"2024-05-20T22:16:25.298900Z","shell.execute_reply.started":"2024-05-20T22:16:25.278414Z","shell.execute_reply":"2024-05-20T22:16:25.297530Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision import tv_tensors\n\n# DATASET_DIR = os.path.join(\".\")\nTRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\nTEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\ntrain_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\ntrain_list, test_list = train_test_split(train_list, test_size = 0.99)\ntrain_list, val_list = train_test_split(train_list, test_size = 0.2)\ntest_list, _ = train_test_split(test_list, test_size = 0.7)\n\n\n# train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('rcnn_targets.npy', allow_pickle='TRUE'))\n# # test_data = ShipsDataset(train_list, transforms = img_train_transforms)\n# val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\ntest_data = ShipsDataset(test_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\nval_data = ShipsDataset(val_list, transforms = img_train_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\nval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\n\nprint(len(train_data),len(train_loader))\nprint(len(val_data), len(val_loader))\n\nmodel_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n\n# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n# La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","collapsed":false,"id":"YW9039lzlK5S","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:16:25.440655Z","iopub.execute_input":"2024-05-20T22:16:25.441066Z","iopub.status.idle":"2024-05-20T22:18:50.218749Z","shell.execute_reply.started":"2024-05-20T22:16:25.441035Z","shell.execute_reply":"2024-05-20T22:18:50.217606Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1540 49\n385 13\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 94.0MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"## STEP 1. freeze backbone layers, add final layers and train the network\n\nfor name, param in model_rcnn.named_parameters():\n      param.requires_grad = False\n\nnum_classes = 2 # background, ship\nin_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\nmodel_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","collapsed":false,"id":"5J9M_bnAxnDk","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:18:50.221086Z","iopub.execute_input":"2024-05-20T22:18:50.221461Z","iopub.status.idle":"2024-05-20T22:18:50.229640Z","shell.execute_reply.started":"2024-05-20T22:18:50.221430Z","shell.execute_reply":"2024-05-20T22:18:50.228475Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n# How to save in google drive something else\n# if save_to_drive:\n#   with open('/content/drive/MyDrive/MLVM_project/file.txt', 'w') as f:\n#     f.write('content')\n\nprint(f\"{model_filepath = }\")\n\ndef save_checkpoint(epoch, model, optimizer, train_loss, val_loss=0, model_name=\"model.tar\"):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","collapsed":false,"id":"Du5q6_RRCmD4","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:18:50.231042Z","iopub.execute_input":"2024-05-20T22:18:50.231365Z","iopub.status.idle":"2024-05-20T22:18:50.245486Z","shell.execute_reply.started":"2024-05-20T22:18:50.231338Z","shell.execute_reply":"2024-05-20T22:18:50.244215Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_epochs10_noaug_id1'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\nimport gc\n\ndef train(model, optimizer, loss_fn, train_loader, val_loader, epochs=1, device=\"cpu\"):\n \n    for epoch in range(epochs):\n        training_loss = 0.0\n        batch_cumsum = 0\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n            logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n            print(f\"epoch {epoch} batch {i}\")\n            batch_cumsum += len(batch) # needed to compute the training loss later\n            optimizer.zero_grad()\n            # inputs, targets = batch\n            \"\"\" inputs = [img for i,el in enumerate(batch)]\n            targets = [lab for img,lab in batch] \"\"\"\n\n            # filtering out empty images (model does not accept empty targets)\n            inputs = []\n            targets = []\n            for el in batch:       # el = (image,labels)\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0][0])\n                    targets.append(el[0][1])\n                    # print(f\"{el = }\")\n                    # Example el\n                    # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                    #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                    #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                    #          ...,\n                    #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                    #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                    #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                    #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                    #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                    #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                    #          ...,\n                    #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                    #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                    #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                    #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                    #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                    #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                    #          ...,\n                    #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                    #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                    #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                    #         [0.2331, 0.2643, 0.3268, 0.3060],\n                    #         [0.2435, 0.2995, 0.4062, 0.3724],\n                    #         [0.7188, 0.6198, 0.8281, 0.6784],\n                    #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n                    # el = (tensor([[[0.0880, 0.0855, 0.0718,  ..., 0.0976, 0.1902, 0.0165],\n            if len(inputs) == 0:\n                continue\n            \n           # inputs = inputs.to(device)\n           # targets = targets.to(device)\n            output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n            \"\"\" EXAMPLE :\n            {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n             'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n             'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)} \"\"\"\n          \n            loss = sum(loss for loss in output.values())\n            #train_loss_list.append(loss.detach().cpu().numpy())\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item() * len(inputs)\n            \n            del inputs\n            del targets\n            gc.collect()    \n        \n        save_checkpoint(epoch, model, optimizer, training_loss/batch_cumsum)\n        \n        # VALIDATION\n           \n        model.train()\n        num_correct = 0\n        num_examples = 0\n        valid_loss = 0\n\n        for i,batch in enumerate(val_loader):\n            print(\"batch\", i)\n            inputs = []\n            targets = []\n\n            for el in batch:       # el = (image,labels)\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0][0])\n                    targets.append(el[0][1])\n            \n            if len(inputs) == 0:\n                continue\n            # inputs = inputs.to(device)\n            output = model(inputs, targets)\n            # print(f\"{output = }\")\n            # targets = targets.to(device)\n            loss = output['loss_box_reg']\n\n            valid_loss += loss.data.item() \n\n           # correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n           # num_correct += torch.sum(correct).item()\n           # num_examples += correct.shape[0]\n\n            del inputs\n            del targets\n            gc.collect()\n\n        valid_loss /= len(val_loader.dataset)\n        print( 'Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.4f}'.format(epoch, training_loss,\n        valid_loss, 1))\n\n        logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.4f}'.format(epoch, training_loss,\n        valid_loss, 1))\n        \n        \n# TEST\n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","collapsed":false,"id":"Mv8b06EulUK2","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:18:50.247130Z","iopub.execute_input":"2024-05-20T22:18:50.247563Z","iopub.status.idle":"2024-05-20T22:18:50.271107Z","shell.execute_reply.started":"2024-05-20T22:18:50.247530Z","shell.execute_reply":"2024-05-20T22:18:50.269794Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","metadata":{"_uuid":"320e60fe-fabe-487b-91d3-a2b3d5965cba","_cell_guid":"3d46d08a-3a51-4452-b79e-882309e42a16","collapsed":false,"id":"LloRuEg8lWyf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:18:50.273853Z","iopub.execute_input":"2024-05-20T22:18:50.274302Z","iopub.status.idle":"2024-05-20T22:18:50.288848Z","shell.execute_reply.started":"2024-05-20T22:18:50.274259Z","shell.execute_reply":"2024-05-20T22:18:50.287471Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = model_rcnn.to(device)\ntorch.compile(model)\noptimizer = optim.Adam(params = model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()","metadata":{"_uuid":"5a384475-40a4-4c13-9c8a-8efeb96ed8f2","_cell_guid":"2f312861-b958-45c9-92e5-ed33d593194f","collapsed":false,"id":"2LUibV2Elccf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:18:50.290334Z","iopub.execute_input":"2024-05-20T22:18:50.290660Z","iopub.status.idle":"2024-05-20T22:18:56.867991Z","shell.execute_reply.started":"2024-05-20T22:18:50.290632Z","shell.execute_reply":"2024-05-20T22:18:56.866480Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"}]},{"cell_type":"code","source":"# START MODEL TRAINING\nif not test_only:\n    train(model, optimizer, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)\n    torch.save(model.state_dict(), 'model_state_dict')","metadata":{"_uuid":"a79ed4b4-e470-4dfc-8f51-ef43b63f3ae5","_cell_guid":"620eeb9f-ac5f-4f99-b241-c62df61659a8","id":"COB3KM9Fx4i7","execution":{"iopub.status.busy":"2024-05-20T22:18:56.870037Z","iopub.execute_input":"2024-05-20T22:18:56.870655Z","iopub.status.idle":"2024-05-20T22:18:56.877365Z","shell.execute_reply.started":"2024-05-20T22:18:56.870605Z","shell.execute_reply":"2024-05-20T22:18:56.876169Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che è stata creata contenente il modello e i log\n\nif not test_only:\n    from IPython.display import FileLink\n    !zip -r file.zip {model_filepath}\n    FileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T22:18:56.879004Z","iopub.execute_input":"2024-05-20T22:18:56.879521Z","iopub.status.idle":"2024-05-20T22:18:56.890172Z","shell.execute_reply.started":"2024-05-20T22:18:56.879484Z","shell.execute_reply":"2024-05-20T22:18:56.888827Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def test(model, test_loader, device=\"cpu\"):   \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    \n    for i,batch in enumerate(val_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,labels)\n            if el[1]['boxes'].size()[0] != 0:\n                inputs.append(el[0][0])\n                targets.append(el[0][1])\n        \n        # if len(inputs) == 0:\n            # continue\n        \n        # inputs = inputs.to(device)\n        output = model(inputs)\n        output = output[0]\n        targets = targets[0]\n                \n        #     # Example output\n        #     {'boxes': tensor([[ 0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        # loss = sum(output['scores'])\n        # print(f\"{output = }\")\n        # test_loss += loss * len(inputs)\n        # print(f\"{test_loss = }\")\n\n        # print(f\"{output['scores'] = }\")\n        \n        # res_softmax = torch.softmax(torch.FloatTensor(output['scores']), dim=-1)\n        # print(f\"{res_softmax = }\")\n        # print(f\"{targets = }\")\n        \n        # correct = torch.eq(torch.max(torch.softmax(torch.FloatTensor(output['scores']), dim=0), dim=0)[1], targets).view(-1)\n        # num_correct += torch.sum(correct).item()\n        # num_examples += correct.shape[0]\n                                    \n        \n        \n        del inputs\n        del targets\n        gc.collect()\n        \n    valid_loss /= len(test_loader.dataset)\n    print( 'Test Loss: {:.4f}, accuracy = {:.4f}'.format(\n    test_loss, 1))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T22:18:56.891661Z","iopub.execute_input":"2024-05-20T22:18:56.892131Z","iopub.status.idle":"2024-05-20T22:18:56.908644Z","shell.execute_reply.started":"2024-05-20T22:18:56.892096Z","shell.execute_reply":"2024-05-20T22:18:56.907390Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n# checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"))\n# model.load_state_dict(checkpoint['model_state_dict'])\n# test(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T22:18:56.910257Z","iopub.execute_input":"2024-05-20T22:18:56.910667Z","iopub.status.idle":"2024-05-20T22:18:56.925376Z","shell.execute_reply.started":"2024-05-20T22:18:56.910633Z","shell.execute_reply":"2024-05-20T22:18:56.924222Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for data in val_loader:\n#         images, labels = data[0].to(device), data[1].to(device)\n#         predictions = torch.argmax(model(images),dim=1)\n\n#         total += labels.size(0)\n#         correct += (predictions == labels).sum().item()\n\n# print('accuracy = {:f}'.format(correct / total))\n# print('correct: {:d}  total: {:d}'.format(correct, total))","metadata":{"_uuid":"ce6b85c5-91c9-488b-9ae0-74bd3514487f","_cell_guid":"93e16d46-9dc5-41c7-8837-52d4920e6149","collapsed":false,"id":"MLPxPQrile1o","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-20T22:18:56.926857Z","iopub.execute_input":"2024-05-20T22:18:56.927320Z","iopub.status.idle":"2024-05-20T22:18:56.941189Z","shell.execute_reply.started":"2024-05-20T22:18:56.927285Z","shell.execute_reply":"2024-05-20T22:18:56.939869Z"},"trusted":true},"execution_count":16,"outputs":[]}]}