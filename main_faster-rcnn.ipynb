{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8650287,"sourceType":"datasetVersion","datasetId":5181471},{"sourceId":8674817,"sourceType":"datasetVersion","datasetId":5198243}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n## MAIN CONFIGURATIONS\nnum_epochs = 10  # Number of epochs the model will train for\nbatch_size = 16\ninit_lr = 1e-4 # Initial Learning Rate\ndata_augmentation_type = 'nothing'\n# Which data augmentation tecnique are we using?\n# 'nothing': no transformations, only image resize\n# 'geometric': basic geometric transforms\n# 'patch_gaussian'\n# 'fourier_random_noise'\n# 'fourier_basis_augmentation'\n                                        \ntrain_percentage = 0.03 #  0.03 means 192_556 * 0.03 ~ 4.6k images in the training set\nval_percentage = train_percentage/3\ntest_percentage = val_percentage\n            \n## WHAT WILL THIS SESSION DO?\ntest_only = False # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\ndo_model_test = False # Tests the model after training\ncreate_log_file = True\n# plot_data = True # Plot training data of '{model_filepath}/model.tar'\nprint_images_during_training = True\n\nmodel_filepath = f\"model_{data_augmentation_type}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\n# !tree # Prints folder structure\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:23:37.807622Z","iopub.execute_input":"2024-06-12T14:23:37.807956Z","iopub.status.idle":"2024-06-12T14:23:37.823197Z","shell.execute_reply.started":"2024-06-12T14:23:37.807927Z","shell.execute_reply":"2024-06-12T14:23:37.822290Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_nothing'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\nimport tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:23:37.824810Z","iopub.execute_input":"2024-06-12T14:23:37.825065Z","iopub.status.idle":"2024-06-12T14:23:42.784908Z","shell.execute_reply.started":"2024-06-12T14:23:37.825042Z","shell.execute_reply":"2024-06-12T14:23:42.784109Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"try:\n    from torchmetrics.detection import MeanAveragePrecision\nexcept:\n    !pip install torchmetrics\n    !pip install torchmetrics[detection]\n    from torchmetrics.detection import MeanAveragePrecision\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:23:42.786032Z","iopub.execute_input":"2024-06-12T14:23:42.786467Z","iopub.status.idle":"2024-06-12T14:23:44.870241Z","shell.execute_reply.started":"2024-06-12T14:23:42.786435Z","shell.execute_reply":"2024-06-12T14:23:44.869218Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-12T14:23:44.872126Z","iopub.execute_input":"2024-06-12T14:23:44.872469Z","iopub.status.idle":"2024-06-12T14:23:44.903304Z","shell.execute_reply.started":"2024-06-12T14:23:44.872442Z","shell.execute_reply":"2024-06-12T14:23:44.902361Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"device = device(type='cuda')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\n\nif log_filepath:\n    print(log_filepath)\nelse:\n    print(\"No logging\")","metadata":{"_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-06-12T14:23:44.904670Z","iopub.execute_input":"2024-06-12T14:23:44.905049Z","iopub.status.idle":"2024-06-12T14:23:44.915912Z","shell.execute_reply.started":"2024-06-12T14:23:44.904985Z","shell.execute_reply":"2024-06-12T14:23:44.915101Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_nothing/log.txt'\nmodels/model_nothing/log.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.fft as fft\nimport torchvision\nimport random\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n        # Scale the image in range [0,1)\n        min_val = 0\n        max_val = 255\n        image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n        min_val = 0\n        max_val = 255\n        # Scale the image in range [0,1)\n        image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:23:44.917072Z","iopub.execute_input":"2024-06-12T14:23:44.917342Z","iopub.status.idle":"2024-06-12T14:23:45.069659Z","shell.execute_reply.started":"2024-06-12T14:23:44.917302Z","shell.execute_reply":"2024-06-12T14:23:45.068639Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\n# Please note: the noormalization of the dataset happens later on the code\n\nmatch data_augmentation_type:\n    \n    case 'nothing':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n    \n    case 'geometric':\n        \n        img_train_transforms = v2.Compose([\n            v2.RandomRotation(50),\n            v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n            v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])    \n  \n    case 'patch_gaussian':\n        \n        img_train_transforms = v2.Compose([\n            PatchGaussian(),\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n        \n    case 'fourier_random_noise':\n        \n        img_train_transforms = v2.Compose([\n            FourierRandomNoise(),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n        \n    case 'fourier_basis_augmentation':\n        \n        img_train_transforms = v2.Compose([\n             FourierBasisAugmentation(),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\nprint('ok')","metadata":{"_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-06-12T14:23:45.071112Z","iopub.execute_input":"2024-06-12T14:23:45.071435Z","iopub.status.idle":"2024-06-12T14:23:45.121879Z","shell.execute_reply.started":"2024-06-12T14:23:45.071409Z","shell.execute_reply":"2024-06-12T14:23:45.121097Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\nfrom torchvision.tv_tensors import BoundingBoxes\n\ndef show(imgs):\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = sorted(file_list, key = lambda f: f.split('/')[-1])\n        self.targets = sorted(targets, key=lambda d: d['image_id'])\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        \n        label = self.targets[idx]\n        \n        assert self.file_list[idx].split('/')[-1] == label['image_id'], f\"Bounding Box mismatch for {idx = }, file: {self.file_list[idx].split('/')[-1]} and label: {label['image_id']}\"\n        \n        try:\n            image = read_image(self.file_list[idx])    # numpy tensor\n        except RuntimeError as e:\n            Warning(f'Errore con {self.file_list[idx]}')\n#             self.targets[idx]['labels'] = torch.tensor([])\n            return None, self.targets[idx]\n        \n        image = F.convert_image_dtype(image) # To compensate for TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n        \n      #  print(self.file_list[idx])\n      #  print(self.targets[idx])\n        \n        try:\n            label['boxes'] = BoundingBoxes(data=label['boxes'], format='XYXY', canvas_size=tuple(image.size()[-2:]))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(image.permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            if label['boxes'].numel():\n                image, label = self.transform(image, label)\n                print(\"type of label:\", type(label))\n            else:\n                image = self.transform(image)\n            \n        return image, label\n\nprint('ok')","metadata":{"_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","_kg_hide-input":true,"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","id":"V1Q6ogjksMqE","execution":{"iopub.status.busy":"2024-06-12T14:23:45.122954Z","iopub.execute_input":"2024-06-12T14:23:45.123224Z","iopub.status.idle":"2024-06-12T14:23:45.138226Z","shell.execute_reply.started":"2024-06-12T14:23:45.123200Z","shell.execute_reply":"2024-06-12T14:23:45.137208Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"if not test_only:\n\n    from sklearn.model_selection import train_test_split\n    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n    from torchvision import tv_tensors\n    from torch.utils.data import SubsetRandomSampler\n\n    # DATASET_DIR = os.path.join(\".\")\n    TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n    TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n    # print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\n    data_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n    ship_dataset = ShipsDataset(data_list, transforms = img_train_transforms, targets=torch.load('/kaggle/input/rcnn-dataset-enf/rcnn_targets.pt'))\n    \n    def custom_collate_fn(batch):\n        # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n        # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n        # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n        return batch\n     \n    # Fix the generator for reproducibility, remove once we understand that it works\n    generator = torch.Generator().manual_seed(42)\n    (\n        train_dataset,\n        val_dataset,\n        test_dataset,\n        unused_dataset\n    ) = torch.utils.data.random_split(ship_dataset, [train_percentage, val_percentage, test_percentage, 1 - train_percentage - val_percentage - test_percentage], generator)\n\n    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n    val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n     \n    print(\"Whole dataset size:\",len(ship_dataset))\n    print(\"train loader size:\",len(train_loader),f\"batches ({train_percentage}%)\")\n    print(\"validation loader size: \",len(val_loader),f\"batches ({val_percentage}%)\")\n    print(\"test loader size:\",len(test_loader),f\"batches ({test_percentage}%)\")\n    \n    # https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n    # La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n    # /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"\n\nprint('ok')","metadata":{"_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-06-12T14:23:45.141054Z","iopub.execute_input":"2024-06-12T14:23:45.141353Z","iopub.status.idle":"2024-06-12T14:24:15.504332Z","shell.execute_reply.started":"2024-06-12T14:23:45.141311Z","shell.execute_reply":"2024-06-12T14:24:15.503036Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m data_list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TRAIN_DIR,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m ship_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mShipsDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_train_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/rcnn-dataset-enf/rcnn_targets.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_collate_fn\u001b[39m(batch):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n","Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mShipsDataset.__init__\u001b[0;34m(self, file_list, targets, transforms, target_transforms)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_list, targets, transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, target_transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(file_list, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m f: f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transforms\n","\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'str'"],"ename":"TypeError","evalue":"'<' not supported between instances of 'NoneType' and 'str'","output_type":"error"}]},{"cell_type":"code","source":" if not test_only:\n    # Save loaders\n    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\n    print('Dataset Loaders saved succesfully!')","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:24:15.505011Z","iopub.status.idle":"2024-06-12T14:24:15.505320Z","shell.execute_reply.started":"2024-06-12T14:24:15.505166Z","shell.execute_reply":"2024-06-12T14:24:15.505178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    \n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        \n        if i % 5 == 0:\n            print(\"batch: \", i)\n        \n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\n\ndef new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\n\ndef save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")\n\n\nmodel_rcnn = new_model()\nprint(\"ok match data_augmentation_type, new_model, save_checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:24:15.506611Z","iopub.status.idle":"2024-06-12T14:24:15.506906Z","shell.execute_reply.started":"2024-06-12T14:24:15.506758Z","shell.execute_reply":"2024-06-12T14:24:15.506770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match data_augmentation_type:\n    \n# Normalmente andrebbero calcolati rispetto il dataset. Per ora facciamo delle prove, e teniamo dati pre-calcolati (pur sapendo non siano accuratissimi)\n#     image_mean_train, image_std_train = get_mean_std(train_loader)\n#     image_mean_val, image_std_val = get_mean_std(test_loader)\n#     image_mean_test, image_std_test = get_mean_std(val_loader)\n    \n    case 'nothing':\n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n        \n    case 'geometric':\n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n    \n    case 'patch_gaussian':\n        image_mean_train = torch.Tensor([0.0941, 0.0936, 0.0942])\n        image_std_train = torch.Tensor([0.1021, 0.1032, 0.1025])\n        \n        image_mean_val = torch.Tensor([0.0921, 0.0912, 0.0928])\n        image_std_val = torch.Tensor([0.1022, 0.1025, 0.1026])\n        \n        image_mean_test = torch.Tensor([0.0979, 0.0949, 0.0937])\n        image_std_test = torch.Tensor([0.1061, 0.1042, 0.1011])\n\n    case 'fourier_random_noise':\n        image_mean_train = torch.Tensor([0., 0., 0.])\n        image_std_train = torch.Tensor([0., 0., 0.])\n        \n        image_mean_val = torch.Tensor([0., 0., 0.])\n        image_std_val = torch.Tensor([0., 0., 0.])\n        \n        image_mean_test = torch.Tensor([0., 0., 0.])\n        image_std_test = torch.Tensor([0., 0., 0.])\n        \n    case 'fourier_basis_augmentation':\n        image_mean_train = torch.Tensor([0.0913, 0.0944, 0.0934])\n        image_std_train = torch.Tensor([0.0988, 0.1006, 0.1007])\n        \n        image_mean_val = torch.Tensor([0.0949, 0.0938, 0.0963])\n        image_std_val = torch.Tensor([0.1025, 0.1029, 0.1040])\n        \n        image_mean_test = torch.Tensor([0.0940, 0.0942, 0.0947])\n        image_std_test = torch.Tensor([0.1032, 0.1043, 0.1020])\n\n\nprint(f\"{image_mean_train = }, {image_std_train = }\")\nprint(f\"{image_mean_val = }, {image_std_val = }\")\nprint(f\"{image_mean_test = }, {image_std_test = }\")\nprint(f\"{data_augmentation_type = }\")\n\nlogger.info(f\"{image_mean_train = }, {image_std_train = }\")\nlogger.info(f\"{image_mean_val = }, {image_std_val = }\")\nlogger.info(f\"{image_mean_test = }, {image_std_test = }\")\nlogger.info(f\"{data_augmentation_type = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:24:15.508205Z","iopub.status.idle":"2024-06-12T14:24:15.508536Z","shell.execute_reply.started":"2024-06-12T14:24:15.508371Z","shell.execute_reply":"2024-06-12T14:24:15.508384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes\n\nif not test_only:\n    \n    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n\n    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n\n        model.transform.image_mean = image_mean_train\n        model.transform.image_std = image_std_train\n        model._skip_resize = True\n\n        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n\n            training_loss = 0.0\n            batch_cumsum = 0\n            model.train()\n            \n            for i, batch in enumerate(train_loader):\n                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n                print(f\"epoch {epoch} batch {i}\")\n                batch_cumsum += len(batch) # needed to compute the training loss later\n                optimizer.zero_grad()\n                \n                inputs = []\n                targets = []\n                \n                for el in batch:       # el = (image,dict) when transforms are active\n                    \n                    el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                    }\n                    \n                    if not el_dict[\"labels\"].numel():\n                        # filtering out empty images (model does not accept empty targets)\n                        continue\n                    else:\n                      #  print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                      #  print(f'el_dict has {el_dict[\"boxes\"] = }')\n                        \n                        image = el[0].to(device)\n                        el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n                        \n                        inputs.append(image)\n                        targets.append(el_dict)\n                        \n                        # Print images during training\n                        if print_images_during_training:\n                            num = len(el_dict[\"boxes\"])\n\n                            img = draw_bounding_boxes(\n                                (image*256).byte(),\n                                el_dict[\"boxes\"],\n                                width = 1,\n                                colors = 'yellow',\n                                # font='arial',\n                                font_size = 15\n                            )\n\n                            fig, ax = plt.subplots()\n                            fig.set_size_inches(16,9)\n                            fig.tight_layout(pad=5)\n                            ax.imshow(img.byte().permute(1, 2, 0))\n                            plt.show()\n                            plt.close()\n\n                        # print(f\"{el = }\")\n                        # Example el\n                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                        #          ...,\n                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                        #          ...,\n                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                        #          ...,\n                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n\n                if len(inputs) == 0:\n                    continue\n\n                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n                \"\"\" EXAMPLE :\n                    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n                     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n                     'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n\n                     How losses are computed:\n\n                     -loss_classifier-\n                     classification_loss = F.cross_entropy(class_logits, labels)\n\n                     -loss_box_reg-\n                     box_loss = F.smooth_l1_loss(\n                        box_regression[sampled_pos_inds_subset, labels_pos],\n                        regression_targets[sampled_pos_inds_subset],\n                        beta=1 / 9,\n                        reduction=\"sum\",\n                    )\n                    box_loss = box_loss / labels.numel()\n\n                    -loss_rpn_box_reg-\n                    box_loss = F.smooth_l1_loss(\n                    pred_bbox_deltas[sampled_pos_inds],\n                    regression_targets[sampled_pos_inds],\n                    beta=1 / 9,\n                    reduction=\"sum\",\n                    ) / (sampled_inds.numel())\n\n                    -loss_objectness-\n                    objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n                 \"\"\"\n\n                loss = sum(loss for loss in output.values())\n                loss.backward()\n                optimizer.step()\n                training_loss += loss.data.item() * len(batch)\n\n            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n            scheduler.step() # changes LR\n            training_loss /= batch_cumsum\n            training_losses.append(training_loss)\n            # save_checkpoint(epoch, model, optimizer, scheduler, training_loss, lrs)\n\n            # VALIDATION\n            model.transform.image_mean = image_mean_val\n            model.transform.image_std = image_std_val\n\n            model.train()\n            num_correct = 0\n            num_examples = 0\n            valid_loss = 0\n\n            with torch.no_grad():\n                for i,batch in enumerate(val_loader):\n                    print(\"batch\", i)\n                    inputs = []\n                    targets = []\n\n                    for el in batch:       # el = (image,labels)\n                        \n                        el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                        }\n\n                        if not el_dict[\"labels\"].numel():\n                            # filtering out empty images (model does not accept empty targets)\n                            continue\n                        else:\n                        #    print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                        #    print(f'el_dict has {el_dict[\"boxes\"] = }')\n\n                            image = el[0].to(device)\n                            el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n\n                            inputs.append(image)\n                            targets.append(el_dict)\n            \n                        \n                        '''\n                        if el[1]['boxes'].size()[0] != 0:\n                            inputs.append(el[0].to(device))\n                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n                        '''\n                   \n                    if len(inputs) == 0:\n                        continue\n\n                    output = model(inputs, targets)\n\n                    loss = sum(loss for loss in output.values())\n                    valid_loss += loss.data.item() *len(batch)\n\n            valid_loss /= len(val_loader.dataset)\n            validation_losses.append(valid_loss)\n\n            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs)\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5\nprint('ok')","metadata":{"_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-06-12T14:24:15.509992Z","iopub.status.idle":"2024-06-12T14:24:15.510371Z","shell.execute_reply.started":"2024-06-12T14:24:15.510171Z","shell.execute_reply":"2024-06-12T14:24:15.510184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n    # plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:24:15.511933Z","iopub.status.idle":"2024-06-12T14:24:15.512233Z","shell.execute_reply.started":"2024-06-12T14:24:15.512084Z","shell.execute_reply":"2024-06-12T14:24:15.512096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms.functional as F\n\ndef test(model, test_loader, device=torch.device(\"cpu\")): \n    \n    model.transform.image_mean  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in tqdm(enumerate(test_loader)):\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,dict)\n            if el[0] != None and not el[1]['labels'].numel() :\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n        # print(type(model(torch.cuda.FloatTensor(inputs))))\n#         print(\"out :\\n\", output)\n#         print(\"target :\\n\",targets)\n        #     # Example output\n        #     {'boxes': tensor([[\n        #       0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        \n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(device)\n            dic[\"labels\"] = dic[\"labels\"].to(device)\n            dic[\"scores\"] = dic[\"scores\"].to(device)\n            \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:24:15.514089Z","iopub.status.idle":"2024-06-12T14:24:15.514414Z","shell.execute_reply.started":"2024-06-12T14:24:15.514239Z","shell.execute_reply":"2024-06-12T14:24:15.514251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n\nif do_model_test or test_only:\n#     checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n    test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n    \n    model = new_model()\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)\n\n# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n# #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\n# model.load_state_dict(checkpoint['model_state_dict'])\n# test(model.to(device), test_loader, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:24:15.515401Z","iopub.status.idle":"2024-06-12T14:24:15.515696Z","shell.execute_reply.started":"2024-06-12T14:24:15.515546Z","shell.execute_reply":"2024-06-12T14:24:15.515559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor.Tensor# TRAIN AGAIN (Continue training)\n\nimport pickle\n\nif train_again:    \n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n    print(\"Loadeders and model loaded succesfully\") \n    \n#     print(f\"{device = }\")\n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch']\n    # Resume training from a specific epoch\n    # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    \n    # Il file salvato model.tar contiene optimizer, scheduler, loss e tanto altro\n\n    logger.info(f\"Continuing training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T14:24:15.516796Z","iopub.status.idle":"2024-06-12T14:24:15.517095Z","shell.execute_reply.started":"2024-06-12T14:24:15.516947Z","shell.execute_reply":"2024-06-12T14:24:15.516959Z"},"trusted":true},"execution_count":null,"outputs":[]}]}