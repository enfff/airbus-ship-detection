{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8713159,"sourceType":"datasetVersion","datasetId":5181471,"isSourceIdPinned":true}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n## MAIN CONFIGURATIONS\nnum_epochs = 40  # Number of epochs the model will train for\nbatch_size = 32\ninit_lr = 1e-6 # Initial Learning Rate\ndata_augmentation_type = 'patch_gaussian'\n# Which data augmentation tecnique are we using?\n# 'nothing': no transformations, only image resize\n# 'geometric': basic geometric transforms\n# 'patch_gaussian': adds a gaussian patch to each\n# 'fourier_random_noise': currently doesn't work\n# 'fourier_basis_augmentation'\n                                        \ntrain_percentage = 0.20 #  0.03 means 192_556 * 0.03 ~ 4.6k images in the training set\nval_percentage = train_percentage/3\ntest_percentage = val_percentage\n\nassert (train_percentage + val_percentage + test_percentage) <= 1.0, \"Bad dataset split!\"\n\n# WHAT WILL THIS SESSION DO?\ntest_only = False # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\ndo_model_test = False # Tests the model after training\ncreate_log_file = True # self-explicatory\ncalculate_mean_std = True # Calculates the mean, and std of the datasets. When set to false, it uses the pre-calculated means, std.\nprint_images_during_training = False # self-explicatory\nsave_dataset = True # Saves the dataset to a file\n\nmodel_filepath = f\"model_{data_augmentation_type}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\n# !tree # Prints folder structure\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\nimport tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if do_model_test or test_only:\n#     try:\n#         from torchmetrics.detection import MeanAveragePrecision\n# #         metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n#     except:\n#         !pip install torchmetrics\n#         !pip install torchmetrics[detection]\n#         !pip install pycocotools\n#         !pip install faster-coco-eval\n        \n#         from torchmetrics.detection import MeanAveragePrecision\n# #         metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n\n# !pip install torchmetrics\n# !pip install torchmetrics[detection]\n# !pip install pycocotools\n# !pip install faster-coco-eval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)","metadata":{"_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.fft as fft\nimport torchvision\nimport random\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n#         # Scale the image in range [0,1)\n#         min_val = 0\n#         max_val = 255\n#         image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n#         min_val = 0\n#         max_val = 255\n#         # Scale the image in range [0,1)\n#         image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\n# Please note: the normalization of the dataset happens later on the code\n\nmatch data_augmentation_type:\n    \n    case 'nothing':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n    \n    case 'geometric':\n        \n        img_train_transforms = v2.Compose([\n            v2.RandomRotation(50),\n            v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n            v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])    \n  \n    case 'patch_gaussian':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            PatchGaussian(),\n        ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n        \n    case 'fourier_random_noise':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            FourierRandomNoise(),\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n        \n    case 'fourier_basis_augmentation':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            FourierBasisAugmentation(),\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\nprint('ok')","metadata":{"_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","id":"LIgECtVqMlCI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\nfrom torchvision.tv_tensors import BoundingBoxes\n\n# def show(imgs):\n\n#     if not isinstance(imgs, list):\n#         imgs = [imgs]\n#     fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n#     for i, img in enumerate(imgs):\n#         img = img.detach()\n#         img = F.to_pil_image(img)\n#         axs[0, i].imshow(np.asarray(img))\n#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = sorted(file_list, key = lambda f: f.split('/')[-1])\n        self.targets = sorted(targets, key=lambda d: d['image_id'])\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        \n        try:\n            label = self.targets[idx]\n        except:\n            print(f\"Tried to access {idx = } which DOESN'T EXIST on rcnn_targets!\")\n        \n        assert self.file_list[idx].split('/')[-1] == label['image_id'], f\"Bounding Box mismatch for {idx = }, file: {self.file_list[idx].split('/')[-1]} and label: {label['image_id']}\"\n        \n        try:\n            image = read_image(self.file_list[idx])    # numpy tensor\n            image = F.convert_image_dtype(image, torch.float) # Images must be FloatTensor with values in [0, 1]\n        except RuntimeError as e:\n            Warning(f'Errore con {self.file_list[idx]}')\n#             self.targets[idx]['labels'] = torch.tensor([])\n            return None, self.targets[idx]\n\n      #  print(self.file_list[idx])\n      #  print(self.targets[idx])\n        \n        try:\n            label['boxes'] = BoundingBoxes(data=label['boxes'], format='XYXY', canvas_size=tuple(image.size()[-2:]))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(F.convert_image_dtype(image).permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            if label['boxes'].numel():\n                image, label = self.transform(image, label)\n                # print(\"type of label:\", type(label))\n            else:\n                image = self.transform(image)\n            \n        return image, label\n\nprint('ok')","metadata":{"_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","_kg_hide-input":true,"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","id":"V1Q6ogjksMqE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n\ndef custom_collate_fn(batch):\n    # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n    # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n    # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n    return batch\n\nif not test_only:\n\n    from sklearn.model_selection import train_test_split\n    from torchvision import tv_tensors\n    from torch.utils.data import SubsetRandomSampler\n\n    # DATASET_DIR = os.path.join(\".\")\n    TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n    TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n    # print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\n    data_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n    ship_dataset = ShipsDataset(data_list, transforms = img_train_transforms, targets=torch.load('/kaggle/input/targets-rcnn/rcnn_targets.pt'))\n     \n    # Fix the generator for reproducibility, remove once we understand that it works\n    generator = torch.Generator().manual_seed(42)\n    (\n        train_dataset,\n        val_dataset,\n        test_dataset,\n        unused_dataset\n    ) = torch.utils.data.random_split(ship_dataset, [train_percentage, val_percentage, test_percentage, 1 - train_percentage - val_percentage - test_percentage], generator)\n\n    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n    val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n     \n    print(f\"Whole dataset size: {len(ship_dataset)}\")\n    print(f\"train loader size: {len(train_loader)} batches ({train_percentage:.2f}%)\")\n    print(f\"validation loader size: {len(val_loader)} batches ({val_percentage:.2f}%)\")\n    print(f\"test loader size: {len(test_loader)} batches ({test_percentage:.2f}%)\")\n    \n    logger.info(f\"Whole dataset size: {len(ship_dataset)}\")\n    logger.info(f\"train loader size: {len(train_loader)} batches ({train_percentage:.2f}%)\")\n    logger.info(f\"validation loader size: {len(val_loader)} batches ({val_percentage:.2f}%)\")\n    logger.info(f\"test loader size: {len(test_loader)} batches ({test_percentage:.2f}%)\")\n    \n    # https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n    # /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"\n\nprint('ok')","metadata":{"_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","id":"YW9039lzlK5S","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if save_dataset:\n    # Save loaders\n    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\n    print('Dataset Loaders saved succesfully!')\n    logger.info(\"Finished saving Dataset Loaders\")\nelse:\n    print('Skipping saving dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    \n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        \n        if i % 5 == 0:\n            print(\"batch: \", i)\n        \n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\n\ndef new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\n\ndef save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, mean_std, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs,\n        'mean_std': mean_std\n    }, os.path.join(model_filepath, model_name))\n    \n    print(\"Saved model\")\n\n\nmodel_rcnn = new_model()\nprint(\"ok match data_augmentation_type, new_model, save_checkpoint\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalmente andrebbero calcolati rispetto il dataset. Per ora facciamo delle prove,\n# e teniamo dati pre-calcolati (pur sapendo non siano accuratissimi)\n\nif calculate_mean_std:\n    logger.info(\"Calculated correct mean, std\")\n    image_mean_train, image_std_train = get_mean_std(train_loader)\n    image_mean_val, image_std_val = get_mean_std(test_loader)\n    image_mean_test, image_std_test = get_mean_std(val_loader)\nelse:\n    logger.info(\"Using approximated mean, std\")\n    match data_augmentation_type:\n        case 'nothing':\n            image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n            image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n            image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n            image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n            image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n            image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n\n        case 'geometric':\n            image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n            image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n            image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n            image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n            image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n            image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n\n        case 'patch_gaussian':\n            image_mean_train = torch.Tensor([0.0941, 0.0936, 0.0942])\n            image_std_train = torch.Tensor([0.1021, 0.1032, 0.1025])\n\n            image_mean_val = torch.Tensor([0.0921, 0.0912, 0.0928])\n            image_std_val = torch.Tensor([0.1022, 0.1025, 0.1026])\n\n            image_mean_test = torch.Tensor([0.0979, 0.0949, 0.0937])\n            image_std_test = torch.Tensor([0.1061, 0.1042, 0.1011])\n\n        case 'fourier_random_noise':\n            image_mean_train = torch.Tensor([0., 0., 0.])\n            image_std_train = torch.Tensor([0., 0., 0.])\n\n            image_mean_val = torch.Tensor([0., 0., 0.])\n            image_std_val = torch.Tensor([0., 0., 0.])\n\n            image_mean_test = torch.Tensor([0., 0., 0.])\n            image_std_test = torch.Tensor([0., 0., 0.])\n\n        case 'fourier_basis_augmentation':\n            image_mean_train = torch.Tensor([0.0913, 0.0944, 0.0934])\n            image_std_train = torch.Tensor([0.0988, 0.1006, 0.1007])\n\n            image_mean_val = torch.Tensor([0.0949, 0.0938, 0.0963])\n            image_std_val = torch.Tensor([0.1025, 0.1029, 0.1040])\n\n            image_mean_test = torch.Tensor([0.0940, 0.0942, 0.0947])\n            image_std_test = torch.Tensor([0.1032, 0.1043, 0.1020])\n\n\nprint(f\"{image_mean_train = }, {image_std_train = }\")\nprint(f\"{image_mean_val = }, {image_std_val = }\")\nprint(f\"{image_mean_test = }, {image_std_test = }\")\nprint(f\"{data_augmentation_type = }\")\n\nlogger.info(f\"{image_mean_train = }, {image_std_train = }\")\nlogger.info(f\"{image_mean_val = }, {image_std_val = }\")\nlogger.info(f\"{image_mean_test = }, {image_std_test = }\")\nlogger.info(f\"{data_augmentation_type = }\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes\n\nif not test_only:\n    \n    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n\n    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n\n        model.transform.image_mean = image_mean_train\n        model.transform.image_std = image_std_train\n        model._skip_resize = True\n\n        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n\n            training_loss = 0.0\n            batch_cumsum = 0\n            model.train()\n            \n            for i, batch in enumerate(train_loader):\n                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n                if i % 50 == 0:\n                    print(f\"epoch {epoch} batch {i}\")\n                batch_cumsum += len(batch) # needed to compute the training loss later\n                optimizer.zero_grad()\n                \n                inputs = []\n                targets = []\n                \n                for el in batch:       # el = (image,dict) when transforms are active\n                    \n                    el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                    }\n                    \n                    if not el_dict[\"labels\"].numel():\n                        # filtering out empty images (model does not accept empty targets)\n                        continue\n                    else:\n                      #  print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                      #  print(f'el_dict has {el_dict[\"boxes\"] = }')\n                        \n                        image = el[0].to(device)\n                        el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n                        \n                        inputs.append(image)\n                        targets.append(el_dict)\n                        \n                        # Print images during training\n                        if print_images_during_training:\n                            num = len(el_dict[\"boxes\"])\n\n                            img = draw_bounding_boxes(\n                                (image*256).byte(),\n                                el_dict[\"boxes\"],\n                                width = 1,\n                                colors = 'yellow',\n                                # font='arial',\n                                font_size = 15\n                            )\n\n                            fig, ax = plt.subplots()\n                            fig.set_size_inches(16,9)\n                            fig.tight_layout(pad=5)\n                            ax.imshow(img.byte().permute(1, 2, 0))\n                            plt.show()\n                            plt.close()\n\n                        # print(f\"{el = }\")\n                        # Example el\n                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                        #          ...,\n                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                        #          ...,\n                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                        #          ...,\n                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n\n                if len(inputs) == 0:\n                    continue\n\n                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n                \"\"\" EXAMPLE :\n                    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n                     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n                     'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n\n                     How losses are computed:\n\n                     -loss_classifier-\n                     classification_loss = F.cross_entropy(class_logits, labels)\n\n                     -loss_box_reg-\n                     box_loss = F.smooth_l1_loss(\n                        box_regression[sampled_pos_inds_subset, labels_pos],\n                        regression_targets[sampled_pos_inds_subset],\n                        beta=1 / 9,\n                        reduction=\"sum\",\n                    )\n                    box_loss = box_loss / labels.numel()\n\n                    -loss_rpn_box_reg-\n                    box_loss = F.smooth_l1_loss(\n                    pred_bbox_deltas[sampled_pos_inds],\n                    regression_targets[sampled_pos_inds],\n                    beta=1 / 9,\n                    reduction=\"sum\",\n                    ) / (sampled_inds.numel())\n\n                    -loss_objectness-\n                    objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n                 \"\"\"\n\n                loss = sum(loss for loss in output.values())\n                loss.backward()\n                optimizer.step()\n                training_loss += loss.data.item() * len(batch)\n\n            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n            scheduler.step() # changes LR\n            training_loss /= batch_cumsum\n            training_losses.append(training_loss)\n            \n            # VALIDATION\n            model.transform.image_mean = image_mean_val\n            model.transform.image_std = image_std_val\n\n            model.train()\n            num_correct = 0\n            num_examples = 0\n            valid_loss = 0\n\n            with torch.no_grad():\n                for i,batch in enumerate(val_loader):\n                    if i % 20 == 0:\n                        print(\"(VAL) batch\", i)\n                    \n                    inputs = []\n                    targets = []\n\n                    for el in batch:       # el = (image,labels)\n                        \n                        el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                        }\n\n                        if not el_dict[\"labels\"].numel():\n                            # filtering out empty images (model does not accept empty targets)\n                            continue\n                        else:\n                        #    print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                        #    print(f'el_dict has {el_dict[\"boxes\"] = }')\n\n                            image = el[0].to(device)\n                            el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n\n                            inputs.append(image)\n                            targets.append(el_dict)\n            \n                        \n                        '''\n                        if el[1]['boxes'].size()[0] != 0:\n                            inputs.append(el[0].to(device))\n                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n                        '''\n                   \n                    if len(inputs) == 0:\n                        continue\n\n                    output = model(inputs, targets)\n\n                    loss = sum(loss for loss in output.values())\n                    valid_loss += loss.data.item() *len(batch)\n\n            valid_loss /= len(val_loader.dataset)\n            validation_losses.append(valid_loss)\n\n            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.10f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.10f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, mean_std=[image_mean_train, image_std_train,image_mean_val, image_std_val, image_mean_test, image_std_test])\n\nprint(f\"{data_augmentation_type = }\")","metadata":{"_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","id":"Mv8b06EulUK2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n\n#     scheduler = torch.optim.lr_scheduler.StepLR(\n#         optimizer,\n#         gamma = 0.9,\n#         step_size = 5,\n#     )\n\n    scheduler = torch.optim.lr_scheduler.StepLR( # LR never changes\n        optimizer,\n        gamma = 0.9,\n        step_size = 15,\n    )\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n# plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms.functional as F\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n\ndef test(model, test_loader, device=torch.device(\"cpu\")):\n    # Normally targets should be None\n    \n    model.transform.image_mean  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    # metric =  MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0    \n    for i, batch in enumerate(test_loader):\n        print(f\"Batch: {i}\")\n        logger.info(f\"TEST, Batch; {i}\")\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,dict)\n            if el[0].numel() and el[1]['labels'].numel(): # We're considering non-empty elements\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n\n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        \n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(device)\n            dic[\"labels\"] = dic[\"labels\"].to(device)\n            dic[\"scores\"] = dic[\"scores\"].to(device)\n            \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)\n    mAPs.append(mAP)\n    log.info(f\"TEST, batch {i} scored {mAP:.10f}\")\n    print(f\"TEST, batch {i} scored {mAP:.10f}\")\n\nprint(\"ok\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n\nif do_model_test or test_only:\n#     checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n    test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n    \n    model = new_model()\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN AGAIN (Continue training)\n\nimport pickle\n\nif train_again:\n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n    print(\"Loadeders and model loaded succesfully\") \n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n    \n    criterion = nn.CrossEntropyLoss()\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch'] # Resume training from a specific epoch\n    \n    logger.info(f\"Continuing training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    logger.info(f\"Restarting from {epoch = }\")\n    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device, start_from_epoch=epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ludo e' intelligente\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simo e' buono\nludo e' buonasimo e' attraenteludo e' intelligente\n\n\nenf e' belloenf e' intelligente\n\nsimo e' bello\n\n\nsimo e' intelligenteenf e' attraente\n\n\n\n\n\n\n\n\n\n\n\n\nenf e' buono\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}