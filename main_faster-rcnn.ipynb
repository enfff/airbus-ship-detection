{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8438193,"sourceType":"datasetVersion","datasetId":5026303},{"sourceId":8516397,"sourceType":"datasetVersion","datasetId":5084559}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# MAIN CONFIGURATIONS\ncreate_log_file = True\nsave_to_drive = False\nmodel_id = '3'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 20\nbatch_size = 32\ndata_augmentation_type = 'noaug'  # Which data augmentation tecnique are we using?\n                                  # 'noaug':     no data augmentation\n\n# !tree # Prints folder structure\n\ntest_only = False # when true it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\n\nmodel_filepath = f\"model_epochs{str(num_epochs)}_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"_uuid":"f66e8f23-9459-4a78-b42d-592a8b72fd93","_cell_guid":"1eb3601c-59c5-4c49-8c24-91184a27a7e1","collapsed":false,"id":"NFc7Y_31k39Q","outputId":"13bf00d1-bde8-49aa-b24b-2e49f08ab1ab","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:18:42.167180Z","iopub.execute_input":"2024-05-25T18:18:42.167597Z","iopub.status.idle":"2024-05-25T18:18:42.176155Z","shell.execute_reply.started":"2024-05-25T18:18:42.167555Z","shell.execute_reply":"2024-05-25T18:18:42.174838Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_epochs20_noaug_id3'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\n\n# !pip install torchsummary\n# from torchsummary import summary\n!pip install torchmetrics\n!pip install pycocotools faster-coco-eval","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","collapsed":false,"id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:56:01.752447Z","iopub.execute_input":"2024-05-25T18:56:01.752856Z","iopub.status.idle":"2024-05-25T18:56:26.900696Z","shell.execute_reply.started":"2024-05-25T18:56:01.752819Z","shell.execute_reply":"2024-05-25T18:56:26.899045Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2+cpu)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (2.0.7)\nCollecting faster-coco-eval\n  Downloading faster_coco_eval-1.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (5.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (2.2.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from faster-coco-eval) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->faster-coco-eval) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->faster-coco-eval) (2023.4)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->faster-coco-eval) (8.2.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading faster_coco_eval-1.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (596 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faster-coco-eval\nSuccessfully installed faster-coco-eval-1.5.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    logger = logging.getLogger('RootLogger')\n    log_filepath = datetime.now().strftime(\"%m-%d_%H.%M.%S\")\n    log_filepath = os.path.join(model_filepath, f\"log_{log_filepath}\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","collapsed":false,"id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-21T07:38:50.407185Z","iopub.execute_input":"2024-05-21T07:38:50.407703Z","iopub.status.idle":"2024-05-21T07:38:50.420320Z","shell.execute_reply.started":"2024-05-21T07:38:50.407664Z","shell.execute_reply":"2024-05-21T07:38:50.419495Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_epochs20_noaug_id3/log_05-21_07.38.50.txt'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\n# Normalize to the ImageNet mean and standard deviation\n# Could calculate it for the cats/dogs data set, but the ImageNet\n# values give acceptable results here.\nimg_train_transforms = v2.Compose([\n    # transforms.RandomRotation(50),\n    # transforms.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n    # transforms.RandomHorizontalFlip(p=0.5),\n    v2.Resize((img_dimensions, img_dimensions)),\n    # transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])\n\nimg_validation_transforms = v2.Compose([\n    v2.Resize((img_dimensions, img_dimensions)),\n    # transforms.ToTensor(),\n    # transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n    ])","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","collapsed":false,"id":"LIgECtVqMlCI","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:17:53.976286Z","iopub.execute_input":"2024-05-25T18:17:53.976840Z","iopub.status.idle":"2024-05-25T18:17:53.983527Z","shell.execute_reply.started":"2024-05-25T18:17:53.976809Z","shell.execute_reply":"2024-05-25T18:17:53.982450Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      # mask[0, r, c+j] = 1\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        # try:\n        label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n        label['boxes'] = torch.Tensor(label['boxes'])\n        label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        # except IndexError as e:\n        #     Warning(f'Errore con {idx = }')\n        #     plt.imshow(image.permute(1, 2, 0))\n        #     plt.show()\n\n        if self.transform:\n            image = self.transform(image, label)\n\n            # prova ad indagare da qui\n            # image = self.transform(image)\n            # image = image.numpy()\n            # return image, label\n            # print(f\"{image = }\")\n            # print(f\"{label = }\")\n\n        return image, label","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","collapsed":false,"id":"V1Q6ogjksMqE","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:18:05.295170Z","iopub.execute_input":"2024-05-25T18:18:05.295539Z","iopub.status.idle":"2024-05-25T18:18:05.309984Z","shell.execute_reply.started":"2024-05-25T18:18:05.295513Z","shell.execute_reply":"2024-05-25T18:18:05.308543Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision import tv_tensors\n\n# DATASET_DIR = os.path.join(\".\")\nTRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\nTEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\ntrain_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\ntrain_list, test_list = train_test_split(train_list, test_size = 0.99)\ntrain_list, val_list = train_test_split(train_list, test_size = 0.2)\ntest_list, _ = train_test_split(test_list, test_size = 0.7)\n\ntrain_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\ntest_data = ShipsDataset(test_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\nval_data = ShipsDataset(val_list, transforms = img_train_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\nval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\n\n\"\"\"\nimport pickle\n\n# Salva i DataLoader\nwith open(os.path.join(model_filepath, 'train_loader.pkl'), 'wb') as f:\n    pickle.dump(train_loader, f)\n\nwith open(os.path.join(model_filepath, 'val_loader.pkl'), 'wb') as f:\n    pickle.dump(val_loader, f)\n\nwith open(os.path.join(model_filepath, 'test_loader.pkl'), 'wb') as f:\n    pickle.dump(test_loader, f)\n\n\"\"\"\n\n# print(len(train_data),len(train_loader))\n# print(len(val_data), len(val_loader))\n\nmodel_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n\n# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n# La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","collapsed":false,"id":"YW9039lzlK5S","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:38:05.544239Z","iopub.execute_input":"2024-05-25T18:38:05.544642Z","iopub.status.idle":"2024-05-25T18:40:01.868346Z","shell.execute_reply.started":"2024-05-25T18:38:05.544611Z","shell.execute_reply":"2024-05-25T18:40:01.865985Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 150MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"## STEP 1. freeze backbone layers, add final layers and train the network\n\nfor name, param in model_rcnn.named_parameters():\n      param.requires_grad = False\n\nnum_classes = 2 # background, ship\nin_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\nmodel_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","collapsed":false,"id":"5J9M_bnAxnDk","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:52:49.548509Z","iopub.execute_input":"2024-05-25T18:52:49.549187Z","iopub.status.idle":"2024-05-25T18:52:49.555796Z","shell.execute_reply.started":"2024-05-25T18:52:49.549155Z","shell.execute_reply":"2024-05-25T18:52:49.554591Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\n# How to save in google drive something else\n# if save_to_drive:\n#   with open('/content/drive/MyDrive/MLVM_project/file.txt', 'w') as f:\n#     f.write('content')\n\nprint(f\"{model_filepath = }\")\n\ndef save_checkpoint(epoch, model, optimizer, train_loss, val_loss=0, model_name=\"model.tar\"):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","collapsed":false,"id":"Du5q6_RRCmD4","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-21T07:43:22.023017Z","iopub.execute_input":"2024-05-21T07:43:22.023397Z","iopub.status.idle":"2024-05-21T07:43:22.031143Z","shell.execute_reply.started":"2024-05-21T07:43:22.023369Z","shell.execute_reply":"2024-05-21T07:43:22.030050Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_epochs20_noaug_id3'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\nimport gc\n\ndef train(model, optimizer, loss_fn, train_loader, val_loader, epochs=1, device=\"cpu\"):\n \n    for epoch in range(epochs):\n        training_loss = 0.0\n        batch_cumsum = 0\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n            logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n            print(f\"epoch {epoch} batch {i}\")\n            batch_cumsum += len(batch) # needed to compute the training loss later\n            optimizer.zero_grad()\n\n            # filtering out empty images (model does not accept empty targets)\n            inputs = []\n            targets = []\n            for el in batch:       # el = (image,labels)\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0][0])\n                    targets.append(el[0][1])\n                    # print(f\"{el = }\")\n                    # Example el\n                    # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                    #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                    #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                    #          ...,\n                    #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                    #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                    #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                    #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                    #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                    #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                    #          ...,\n                    #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                    #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                    #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                    #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                    #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                    #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                    #          ...,\n                    #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                    #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                    #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                    #         [0.2331, 0.2643, 0.3268, 0.3060],\n                    #         [0.2435, 0.2995, 0.4062, 0.3724],\n                    #         [0.7188, 0.6198, 0.8281, 0.6784],\n                    #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n                    # el = (tensor([[[0.0880, 0.0855, 0.0718,  ..., 0.0976, 0.1902, 0.0165],\n            if len(inputs) == 0:\n                continue\n            \n           # inputs = inputs.to(device)\n           # targets = targets.to(device)\n            output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n            \"\"\" EXAMPLE :\n            {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n             'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n             'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n             \n             How losses are computed:\n             \n             -loss_classifier-\n             classification_loss = F.cross_entropy(class_logits, labels)\n             \n             -loss_box_reg-\n             box_loss = F.smooth_l1_loss(\n                box_regression[sampled_pos_inds_subset, labels_pos],\n                regression_targets[sampled_pos_inds_subset],\n                beta=1 / 9,\n                reduction=\"sum\",\n            )\n            box_loss = box_loss / labels.numel()\n            \n            -loss_rpn_box_reg-\n            box_loss = F.smooth_l1_loss(\n            pred_bbox_deltas[sampled_pos_inds],\n            regression_targets[sampled_pos_inds],\n            beta=1 / 9,\n            reduction=\"sum\",\n            ) / (sampled_inds.numel())\n            \n            -loss_objectness-\n            objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n             \n             \"\"\"\n          \n            loss = sum(loss for loss in output.values())\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item() *len(batch)\n            \n            del inputs\n            del targets\n            gc.collect()    \n        \n        training_loss /= batch_cumsum\n        save_checkpoint(epoch, model, optimizer, training_loss)\n        \n        # VALIDATION\n           \n        model.train()\n        num_correct = 0\n        num_examples = 0\n        valid_loss = 0\n        \n        with torch.no_grad():\n            for i,batch in enumerate(val_loader):\n                print(\"batch\", i)\n                inputs = []\n                targets = []\n\n                for el in batch:       # el = (image,labels)\n                    if el[1]['boxes'].size()[0] != 0:\n                        inputs.append(el[0][0])\n                        targets.append(el[0][1])\n\n                if len(inputs) == 0:\n                    continue\n                # inputs = inputs.to(device)\n                output = model(inputs, targets)\n\n                # print(f\"{output = }\")\n                # targets = targets.to(device)\n                loss = sum(loss for loss in output.values())\n                valid_loss += loss.data.item() *len(batch)\n\n                del inputs\n                del targets\n                gc.collect()\n\n        valid_loss /= len(val_loader.dataset)\n        \n        print( 'Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, training_loss,\n        valid_loss))\n\n        logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, training_loss,\n        valid_loss))\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","collapsed":false,"id":"Mv8b06EulUK2","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-21T07:43:22.974762Z","iopub.execute_input":"2024-05-21T07:43:22.975161Z","iopub.status.idle":"2024-05-21T07:43:22.996747Z","shell.execute_reply.started":"2024-05-21T07:43:22.975133Z","shell.execute_reply":"2024-05-21T07:43:22.995436Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","metadata":{"_uuid":"320e60fe-fabe-487b-91d3-a2b3d5965cba","_cell_guid":"3d46d08a-3a51-4452-b79e-882309e42a16","collapsed":false,"id":"LloRuEg8lWyf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:44:22.805373Z","iopub.execute_input":"2024-05-25T18:44:22.805777Z","iopub.status.idle":"2024-05-25T18:44:22.810796Z","shell.execute_reply.started":"2024-05-25T18:44:22.805745Z","shell.execute_reply":"2024-05-25T18:44:22.809789Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = model_rcnn.to(device)\ntorch.compile(model)\noptimizer = optim.Adam(params = model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()","metadata":{"_uuid":"5a384475-40a4-4c13-9c8a-8efeb96ed8f2","_cell_guid":"2f312861-b958-45c9-92e5-ed33d593194f","collapsed":false,"id":"2LUibV2Elccf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-25T18:53:11.986422Z","iopub.execute_input":"2024-05-25T18:53:11.987111Z","iopub.status.idle":"2024-05-25T18:53:11.996368Z","shell.execute_reply.started":"2024-05-25T18:53:11.986991Z","shell.execute_reply":"2024-05-25T18:53:11.994947Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# START MODEL TRAINING\nif not test_only:\n    train(model, optimizer, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)\n    torch.save(model.state_dict(), 'model_state_dict')","metadata":{"_uuid":"a79ed4b4-e470-4dfc-8f51-ef43b63f3ae5","_cell_guid":"620eeb9f-ac5f-4f99-b241-c62df61659a8","id":"COB3KM9Fx4i7","execution":{"iopub.status.busy":"2024-05-21T07:43:57.058894Z","iopub.execute_input":"2024-05-21T07:43:57.059296Z","iopub.status.idle":"2024-05-21T18:14:44.178968Z","shell.execute_reply.started":"2024-05-21T07:43:57.059267Z","shell.execute_reply":"2024-05-21T18:14:44.177335Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"epoch 0 batch 0\nepoch 0 batch 1\nepoch 0 batch 2\nepoch 0 batch 3\nepoch 0 batch 4\nepoch 0 batch 5\nepoch 0 batch 6\nepoch 0 batch 7\nepoch 0 batch 8\nepoch 0 batch 9\nepoch 0 batch 10\nepoch 0 batch 11\nepoch 0 batch 12\nepoch 0 batch 13\nepoch 0 batch 14\nepoch 0 batch 15\nepoch 0 batch 16\nepoch 0 batch 17\nepoch 0 batch 18\nepoch 0 batch 19\nepoch 0 batch 20\nepoch 0 batch 21\nepoch 0 batch 22\nepoch 0 batch 23\nepoch 0 batch 24\nepoch 0 batch 25\nepoch 0 batch 26\nepoch 0 batch 27\nepoch 0 batch 28\nepoch 0 batch 29\nepoch 0 batch 30\nepoch 0 batch 31\nepoch 0 batch 32\nepoch 0 batch 33\nepoch 0 batch 34\nepoch 0 batch 35\nepoch 0 batch 36\nepoch 0 batch 37\nepoch 0 batch 38\nepoch 0 batch 39\nepoch 0 batch 40\nepoch 0 batch 41\nepoch 0 batch 42\nepoch 0 batch 43\nepoch 0 batch 44\nepoch 0 batch 45\nepoch 0 batch 46\nepoch 0 batch 47\nepoch 0 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 0, Training Loss: 1625.5893, Validation Loss: 0.0002, accuracy = 1.0000\nepoch 1 batch 0\nepoch 1 batch 1\nepoch 1 batch 2\nepoch 1 batch 3\nepoch 1 batch 4\nepoch 1 batch 5\nepoch 1 batch 6\nepoch 1 batch 7\nepoch 1 batch 8\nepoch 1 batch 9\nepoch 1 batch 10\nepoch 1 batch 11\nepoch 1 batch 12\nepoch 1 batch 13\nepoch 1 batch 14\nepoch 1 batch 15\nepoch 1 batch 16\nepoch 1 batch 17\nepoch 1 batch 18\nepoch 1 batch 19\nepoch 1 batch 20\nepoch 1 batch 21\nepoch 1 batch 22\nepoch 1 batch 23\nepoch 1 batch 24\nepoch 1 batch 25\nepoch 1 batch 26\nepoch 1 batch 27\nepoch 1 batch 28\nepoch 1 batch 29\nepoch 1 batch 30\nepoch 1 batch 31\nepoch 1 batch 32\nepoch 1 batch 33\nepoch 1 batch 34\nepoch 1 batch 35\nepoch 1 batch 36\nepoch 1 batch 37\nepoch 1 batch 38\nepoch 1 batch 39\nepoch 1 batch 40\nepoch 1 batch 41\nepoch 1 batch 42\nepoch 1 batch 43\nepoch 1 batch 44\nepoch 1 batch 45\nepoch 1 batch 46\nepoch 1 batch 47\nepoch 1 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 1, Training Loss: 1592.1575, Validation Loss: 0.0007, accuracy = 1.0000\nepoch 2 batch 0\nepoch 2 batch 1\nepoch 2 batch 2\nepoch 2 batch 3\nepoch 2 batch 4\nepoch 2 batch 5\nepoch 2 batch 6\nepoch 2 batch 7\nepoch 2 batch 8\nepoch 2 batch 9\nepoch 2 batch 10\nepoch 2 batch 11\nepoch 2 batch 12\nepoch 2 batch 13\nepoch 2 batch 14\nepoch 2 batch 15\nepoch 2 batch 16\nepoch 2 batch 17\nepoch 2 batch 18\nepoch 2 batch 19\nepoch 2 batch 20\nepoch 2 batch 21\nepoch 2 batch 22\nepoch 2 batch 23\nepoch 2 batch 24\nepoch 2 batch 25\nepoch 2 batch 26\nepoch 2 batch 27\nepoch 2 batch 28\nepoch 2 batch 29\nepoch 2 batch 30\nepoch 2 batch 31\nepoch 2 batch 32\nepoch 2 batch 33\nepoch 2 batch 34\nepoch 2 batch 35\nepoch 2 batch 36\nepoch 2 batch 37\nepoch 2 batch 38\nepoch 2 batch 39\nepoch 2 batch 40\nepoch 2 batch 41\nepoch 2 batch 42\nepoch 2 batch 43\nepoch 2 batch 44\nepoch 2 batch 45\nepoch 2 batch 46\nepoch 2 batch 47\nepoch 2 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 2, Training Loss: 1583.8639, Validation Loss: 0.0001, accuracy = 1.0000\nepoch 3 batch 0\nepoch 3 batch 1\nepoch 3 batch 2\nepoch 3 batch 3\nepoch 3 batch 4\nepoch 3 batch 5\nepoch 3 batch 6\nepoch 3 batch 7\nepoch 3 batch 8\nepoch 3 batch 9\nepoch 3 batch 10\nepoch 3 batch 11\nepoch 3 batch 12\nepoch 3 batch 13\nepoch 3 batch 14\nepoch 3 batch 15\nepoch 3 batch 16\nepoch 3 batch 17\nepoch 3 batch 18\nepoch 3 batch 19\nepoch 3 batch 20\nepoch 3 batch 21\nepoch 3 batch 22\nepoch 3 batch 23\nepoch 3 batch 24\nepoch 3 batch 25\nepoch 3 batch 26\nepoch 3 batch 27\nepoch 3 batch 28\nepoch 3 batch 29\nepoch 3 batch 30\nepoch 3 batch 31\nepoch 3 batch 32\nepoch 3 batch 33\nepoch 3 batch 34\nepoch 3 batch 35\nepoch 3 batch 36\nepoch 3 batch 37\nepoch 3 batch 38\nepoch 3 batch 39\nepoch 3 batch 40\nepoch 3 batch 41\nepoch 3 batch 42\nepoch 3 batch 43\nepoch 3 batch 44\nepoch 3 batch 45\nepoch 3 batch 46\nepoch 3 batch 47\nepoch 3 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 3, Training Loss: 1579.1417, Validation Loss: 0.0005, accuracy = 1.0000\nepoch 4 batch 0\nepoch 4 batch 1\nepoch 4 batch 2\nepoch 4 batch 3\nepoch 4 batch 4\nepoch 4 batch 5\nepoch 4 batch 6\nepoch 4 batch 7\nepoch 4 batch 8\nepoch 4 batch 9\nepoch 4 batch 10\nepoch 4 batch 11\nepoch 4 batch 12\nepoch 4 batch 13\nepoch 4 batch 14\nepoch 4 batch 15\nepoch 4 batch 16\nepoch 4 batch 17\nepoch 4 batch 18\nepoch 4 batch 19\nepoch 4 batch 20\nepoch 4 batch 21\nepoch 4 batch 22\nepoch 4 batch 23\nepoch 4 batch 24\nepoch 4 batch 25\nepoch 4 batch 26\nepoch 4 batch 27\nepoch 4 batch 28\nepoch 4 batch 29\nepoch 4 batch 30\nepoch 4 batch 31\nepoch 4 batch 32\nepoch 4 batch 33\nepoch 4 batch 34\nepoch 4 batch 35\nepoch 4 batch 36\nepoch 4 batch 37\nepoch 4 batch 38\nepoch 4 batch 39\nepoch 4 batch 40\nepoch 4 batch 41\nepoch 4 batch 42\nepoch 4 batch 43\nepoch 4 batch 44\nepoch 4 batch 45\nepoch 4 batch 46\nepoch 4 batch 47\nepoch 4 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 4, Training Loss: 1579.2180, Validation Loss: 0.0003, accuracy = 1.0000\nepoch 5 batch 0\nepoch 5 batch 1\nepoch 5 batch 2\nepoch 5 batch 3\nepoch 5 batch 4\nepoch 5 batch 5\nepoch 5 batch 6\nepoch 5 batch 7\nepoch 5 batch 8\nepoch 5 batch 9\nepoch 5 batch 10\nepoch 5 batch 11\nepoch 5 batch 12\nepoch 5 batch 13\nepoch 5 batch 14\nepoch 5 batch 15\nepoch 5 batch 16\nepoch 5 batch 17\nepoch 5 batch 18\nepoch 5 batch 19\nepoch 5 batch 20\nepoch 5 batch 21\nepoch 5 batch 22\nepoch 5 batch 23\nepoch 5 batch 24\nepoch 5 batch 25\nepoch 5 batch 26\nepoch 5 batch 27\nepoch 5 batch 28\nepoch 5 batch 29\nepoch 5 batch 30\nepoch 5 batch 31\nepoch 5 batch 32\nepoch 5 batch 33\nepoch 5 batch 34\nepoch 5 batch 35\nepoch 5 batch 36\nepoch 5 batch 37\nepoch 5 batch 38\nepoch 5 batch 39\nepoch 5 batch 40\nepoch 5 batch 41\nepoch 5 batch 42\nepoch 5 batch 43\nepoch 5 batch 44\nepoch 5 batch 45\nepoch 5 batch 46\nepoch 5 batch 47\nepoch 5 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 5, Training Loss: 1578.7330, Validation Loss: 0.0002, accuracy = 1.0000\nepoch 6 batch 0\nepoch 6 batch 1\nepoch 6 batch 2\nepoch 6 batch 3\nepoch 6 batch 4\nepoch 6 batch 5\nepoch 6 batch 6\nepoch 6 batch 7\nepoch 6 batch 8\nepoch 6 batch 9\nepoch 6 batch 10\nepoch 6 batch 11\nepoch 6 batch 12\nepoch 6 batch 13\nepoch 6 batch 14\nepoch 6 batch 15\nepoch 6 batch 16\nepoch 6 batch 17\nepoch 6 batch 18\nepoch 6 batch 19\nepoch 6 batch 20\nepoch 6 batch 21\nepoch 6 batch 22\nepoch 6 batch 23\nepoch 6 batch 24\nepoch 6 batch 25\nepoch 6 batch 26\nepoch 6 batch 27\nepoch 6 batch 28\nepoch 6 batch 29\nepoch 6 batch 30\nepoch 6 batch 31\nepoch 6 batch 32\nepoch 6 batch 33\nepoch 6 batch 34\nepoch 6 batch 35\nepoch 6 batch 36\nepoch 6 batch 37\nepoch 6 batch 38\nepoch 6 batch 39\nepoch 6 batch 40\nepoch 6 batch 41\nepoch 6 batch 42\nepoch 6 batch 43\nepoch 6 batch 44\nepoch 6 batch 45\nepoch 6 batch 46\nepoch 6 batch 47\nepoch 6 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 6, Training Loss: 1579.2269, Validation Loss: 0.0002, accuracy = 1.0000\nepoch 7 batch 0\nepoch 7 batch 1\nepoch 7 batch 2\nepoch 7 batch 3\nepoch 7 batch 4\nepoch 7 batch 5\nepoch 7 batch 6\nepoch 7 batch 7\nepoch 7 batch 8\nepoch 7 batch 9\nepoch 7 batch 10\nepoch 7 batch 11\nepoch 7 batch 12\nepoch 7 batch 13\nepoch 7 batch 14\nepoch 7 batch 15\nepoch 7 batch 16\nepoch 7 batch 17\nepoch 7 batch 18\nepoch 7 batch 19\nepoch 7 batch 20\nepoch 7 batch 21\nepoch 7 batch 22\nepoch 7 batch 23\nepoch 7 batch 24\nepoch 7 batch 25\nepoch 7 batch 26\nepoch 7 batch 27\nepoch 7 batch 28\nepoch 7 batch 29\nepoch 7 batch 30\nepoch 7 batch 31\nepoch 7 batch 32\nepoch 7 batch 33\nepoch 7 batch 34\nepoch 7 batch 35\nepoch 7 batch 36\nepoch 7 batch 37\nepoch 7 batch 38\nepoch 7 batch 39\nepoch 7 batch 40\nepoch 7 batch 41\nepoch 7 batch 42\nepoch 7 batch 43\nepoch 7 batch 44\nepoch 7 batch 45\nepoch 7 batch 46\nepoch 7 batch 47\nepoch 7 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 7, Training Loss: 1577.2113, Validation Loss: 0.0001, accuracy = 1.0000\nepoch 8 batch 0\nepoch 8 batch 1\nepoch 8 batch 2\nepoch 8 batch 3\nepoch 8 batch 4\nepoch 8 batch 5\nepoch 8 batch 6\nepoch 8 batch 7\nepoch 8 batch 8\nepoch 8 batch 9\nepoch 8 batch 10\nepoch 8 batch 11\nepoch 8 batch 12\nepoch 8 batch 13\nepoch 8 batch 14\nepoch 8 batch 15\nepoch 8 batch 16\nepoch 8 batch 17\nepoch 8 batch 18\nepoch 8 batch 19\nepoch 8 batch 20\nepoch 8 batch 21\nepoch 8 batch 22\nepoch 8 batch 23\nepoch 8 batch 24\nepoch 8 batch 25\nepoch 8 batch 26\nepoch 8 batch 27\nepoch 8 batch 28\nepoch 8 batch 29\nepoch 8 batch 30\nepoch 8 batch 31\nepoch 8 batch 32\nepoch 8 batch 33\nepoch 8 batch 34\nepoch 8 batch 35\nepoch 8 batch 36\nepoch 8 batch 37\nepoch 8 batch 38\nepoch 8 batch 39\nepoch 8 batch 40\nepoch 8 batch 41\nepoch 8 batch 42\nepoch 8 batch 43\nepoch 8 batch 44\nepoch 8 batch 45\nepoch 8 batch 46\nepoch 8 batch 47\nepoch 8 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 8, Training Loss: 1577.6578, Validation Loss: 0.0002, accuracy = 1.0000\nepoch 9 batch 0\nepoch 9 batch 1\nepoch 9 batch 2\nepoch 9 batch 3\nepoch 9 batch 4\nepoch 9 batch 5\nepoch 9 batch 6\nepoch 9 batch 7\nepoch 9 batch 8\nepoch 9 batch 9\nepoch 9 batch 10\nepoch 9 batch 11\nepoch 9 batch 12\nepoch 9 batch 13\nepoch 9 batch 14\nepoch 9 batch 15\nepoch 9 batch 16\nepoch 9 batch 17\nepoch 9 batch 18\nepoch 9 batch 19\nepoch 9 batch 20\nepoch 9 batch 21\nepoch 9 batch 22\nepoch 9 batch 23\nepoch 9 batch 24\nepoch 9 batch 25\nepoch 9 batch 26\nepoch 9 batch 27\nepoch 9 batch 28\nepoch 9 batch 29\nepoch 9 batch 30\nepoch 9 batch 31\nepoch 9 batch 32\nepoch 9 batch 33\nepoch 9 batch 34\nepoch 9 batch 35\nepoch 9 batch 36\nepoch 9 batch 37\nepoch 9 batch 38\nepoch 9 batch 39\nepoch 9 batch 40\nepoch 9 batch 41\nepoch 9 batch 42\nepoch 9 batch 43\nepoch 9 batch 44\nepoch 9 batch 45\nepoch 9 batch 46\nepoch 9 batch 47\nepoch 9 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 9, Training Loss: 1578.3398, Validation Loss: 0.0003, accuracy = 1.0000\nepoch 10 batch 0\nepoch 10 batch 1\nepoch 10 batch 2\nepoch 10 batch 3\nepoch 10 batch 4\nepoch 10 batch 5\nepoch 10 batch 6\nepoch 10 batch 7\nepoch 10 batch 8\nepoch 10 batch 9\nepoch 10 batch 10\nepoch 10 batch 11\nepoch 10 batch 12\nepoch 10 batch 13\nepoch 10 batch 14\nepoch 10 batch 15\nepoch 10 batch 16\nepoch 10 batch 17\nepoch 10 batch 18\nepoch 10 batch 19\nepoch 10 batch 20\nepoch 10 batch 21\nepoch 10 batch 22\nepoch 10 batch 23\nepoch 10 batch 24\nepoch 10 batch 25\nepoch 10 batch 26\nepoch 10 batch 27\nepoch 10 batch 28\nepoch 10 batch 29\nepoch 10 batch 30\nepoch 10 batch 31\nepoch 10 batch 32\nepoch 10 batch 33\nepoch 10 batch 34\nepoch 10 batch 35\nepoch 10 batch 36\nepoch 10 batch 37\nepoch 10 batch 38\nepoch 10 batch 39\nepoch 10 batch 40\nepoch 10 batch 41\nepoch 10 batch 42\nepoch 10 batch 43\nepoch 10 batch 44\nepoch 10 batch 45\nepoch 10 batch 46\nepoch 10 batch 47\nepoch 10 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 10, Training Loss: 1578.4588, Validation Loss: 0.0003, accuracy = 1.0000\nepoch 11 batch 0\nepoch 11 batch 1\nepoch 11 batch 2\nepoch 11 batch 3\nepoch 11 batch 4\nepoch 11 batch 5\nepoch 11 batch 6\nepoch 11 batch 7\nepoch 11 batch 8\nepoch 11 batch 9\nepoch 11 batch 10\nepoch 11 batch 11\nepoch 11 batch 12\nepoch 11 batch 13\nepoch 11 batch 14\nepoch 11 batch 15\nepoch 11 batch 16\nepoch 11 batch 17\nepoch 11 batch 18\nepoch 11 batch 19\nepoch 11 batch 20\nepoch 11 batch 21\nepoch 11 batch 22\nepoch 11 batch 23\nepoch 11 batch 24\nepoch 11 batch 25\nepoch 11 batch 26\nepoch 11 batch 27\nepoch 11 batch 28\nepoch 11 batch 29\nepoch 11 batch 30\nepoch 11 batch 31\nepoch 11 batch 32\nepoch 11 batch 33\nepoch 11 batch 34\nepoch 11 batch 35\nepoch 11 batch 36\nepoch 11 batch 37\nepoch 11 batch 38\nepoch 11 batch 39\nepoch 11 batch 40\nepoch 11 batch 41\nepoch 11 batch 42\nepoch 11 batch 43\nepoch 11 batch 44\nepoch 11 batch 45\nepoch 11 batch 46\nepoch 11 batch 47\nepoch 11 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 11, Training Loss: 1578.0108, Validation Loss: 0.0004, accuracy = 1.0000\nepoch 12 batch 0\nepoch 12 batch 1\nepoch 12 batch 2\nepoch 12 batch 3\nepoch 12 batch 4\nepoch 12 batch 5\nepoch 12 batch 6\nepoch 12 batch 7\nepoch 12 batch 8\nepoch 12 batch 9\nepoch 12 batch 10\nepoch 12 batch 11\nepoch 12 batch 12\nepoch 12 batch 13\nepoch 12 batch 14\nepoch 12 batch 15\nepoch 12 batch 16\nepoch 12 batch 17\nepoch 12 batch 18\nepoch 12 batch 19\nepoch 12 batch 20\nepoch 12 batch 21\nepoch 12 batch 22\nepoch 12 batch 23\nepoch 12 batch 24\nepoch 12 batch 25\nepoch 12 batch 26\nepoch 12 batch 27\nepoch 12 batch 28\nepoch 12 batch 29\nepoch 12 batch 30\nepoch 12 batch 31\nepoch 12 batch 32\nepoch 12 batch 33\nepoch 12 batch 34\nepoch 12 batch 35\nepoch 12 batch 36\nepoch 12 batch 37\nepoch 12 batch 38\nepoch 12 batch 39\nepoch 12 batch 40\nepoch 12 batch 41\nepoch 12 batch 42\nepoch 12 batch 43\nepoch 12 batch 44\nepoch 12 batch 45\nepoch 12 batch 46\nepoch 12 batch 47\nepoch 12 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 12, Training Loss: 1578.7905, Validation Loss: 0.0002, accuracy = 1.0000\nepoch 13 batch 0\nepoch 13 batch 1\nepoch 13 batch 2\nepoch 13 batch 3\nepoch 13 batch 4\nepoch 13 batch 5\nepoch 13 batch 6\nepoch 13 batch 7\nepoch 13 batch 8\nepoch 13 batch 9\nepoch 13 batch 10\nepoch 13 batch 11\nepoch 13 batch 12\nepoch 13 batch 13\nepoch 13 batch 14\nepoch 13 batch 15\nepoch 13 batch 16\nepoch 13 batch 17\nepoch 13 batch 18\nepoch 13 batch 19\nepoch 13 batch 20\nepoch 13 batch 21\nepoch 13 batch 22\nepoch 13 batch 24\nepoch 13 batch 25\nepoch 13 batch 26\nepoch 13 batch 27\nepoch 13 batch 28\nepoch 13 batch 29\nepoch 13 batch 30\nepoch 13 batch 31\nepoch 13 batch 32\nepoch 13 batch 33\nepoch 13 batch 34\nepoch 13 batch 35\nepoch 13 batch 36\nepoch 13 batch 37\nepoch 13 batch 38\nepoch 13 batch 39\nepoch 13 batch 40\nepoch 13 batch 41\nepoch 13 batch 42\nepoch 13 batch 43\nepoch 13 batch 44\nepoch 13 batch 45\nepoch 13 batch 46\nepoch 13 batch 47\nepoch 13 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 13, Training Loss: 1578.9764, Validation Loss: 0.0003, accuracy = 1.0000\nepoch 14 batch 0\nepoch 14 batch 1\nepoch 14 batch 2\nepoch 14 batch 3\nepoch 14 batch 4\nepoch 14 batch 5\nepoch 14 batch 6\nepoch 14 batch 7\nepoch 14 batch 8\nepoch 14 batch 9\nepoch 14 batch 10\nepoch 14 batch 11\nepoch 14 batch 12\nepoch 14 batch 13\nepoch 14 batch 14\nepoch 14 batch 15\nepoch 14 batch 16\nepoch 14 batch 17\nepoch 14 batch 18\nepoch 14 batch 19\nepoch 14 batch 20\nepoch 14 batch 21\nepoch 14 batch 22\nepoch 14 batch 23\nepoch 14 batch 24\nepoch 14 batch 25\nepoch 14 batch 26\nepoch 14 batch 27\nepoch 14 batch 28\nepoch 14 batch 29\nepoch 14 batch 30\nepoch 14 batch 31\nepoch 14 batch 32\nepoch 14 batch 33\nepoch 14 batch 34\nepoch 14 batch 35\nepoch 14 batch 36\nepoch 14 batch 37\nepoch 14 batch 38\nepoch 14 batch 39\nepoch 14 batch 40\nepoch 14 batch 41\nepoch 14 batch 42\nepoch 14 batch 43\nepoch 14 batch 44\nepoch 14 batch 45\nepoch 14 batch 46\nepoch 14 batch 47\nepoch 14 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 14, Training Loss: 1577.8145, Validation Loss: 0.0001, accuracy = 1.0000\nepoch 15 batch 0\nepoch 15 batch 1\nepoch 15 batch 2\nepoch 15 batch 3\nepoch 15 batch 4\nepoch 15 batch 5\nepoch 15 batch 6\nepoch 15 batch 7\nepoch 15 batch 8\nepoch 15 batch 9\nepoch 15 batch 10\nepoch 15 batch 11\nepoch 15 batch 12\nepoch 15 batch 13\nepoch 15 batch 14\nepoch 15 batch 15\nepoch 15 batch 16\nepoch 15 batch 17\nepoch 15 batch 18\nepoch 15 batch 19\nepoch 15 batch 20\nepoch 15 batch 21\nepoch 15 batch 22\nepoch 15 batch 23\nepoch 15 batch 24\nepoch 15 batch 25\nepoch 15 batch 26\nepoch 15 batch 27\nepoch 15 batch 28\nepoch 15 batch 29\nepoch 15 batch 30\nepoch 15 batch 31\nepoch 15 batch 32\nepoch 15 batch 33\nepoch 15 batch 34\nepoch 15 batch 35\nepoch 15 batch 36\nepoch 15 batch 37\nepoch 15 batch 38\nepoch 15 batch 39\nepoch 15 batch 40\nepoch 15 batch 41\nepoch 15 batch 42\nepoch 15 batch 43\nepoch 15 batch 44\nepoch 15 batch 45\nepoch 15 batch 46\nepoch 15 batch 47\nepoch 15 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 15, Training Loss: 1577.5861, Validation Loss: 0.0004, accuracy = 1.0000\nepoch 16 batch 0\nepoch 16 batch 1\nepoch 16 batch 2\nepoch 16 batch 3\nepoch 16 batch 4\nepoch 16 batch 5\nepoch 16 batch 6\nepoch 16 batch 7\nepoch 16 batch 8\nepoch 16 batch 9\nepoch 16 batch 10\nepoch 16 batch 11\nepoch 16 batch 12\nepoch 16 batch 13\nepoch 16 batch 14\nepoch 16 batch 15\nepoch 16 batch 16\nepoch 16 batch 17\nepoch 16 batch 18\nepoch 16 batch 19\nepoch 16 batch 20\nepoch 16 batch 21\nepoch 16 batch 22\nepoch 16 batch 23\nepoch 16 batch 24\nepoch 16 batch 25\nepoch 16 batch 26\nepoch 16 batch 27\nepoch 16 batch 28\nepoch 16 batch 29\nepoch 16 batch 30\nepoch 16 batch 31\nepoch 16 batch 32\nepoch 16 batch 33\nepoch 16 batch 34\nepoch 16 batch 35\nepoch 16 batch 36\nepoch 16 batch 37\nepoch 16 batch 38\nepoch 16 batch 39\nepoch 16 batch 40\nepoch 16 batch 41\nepoch 16 batch 42\nepoch 16 batch 43\nepoch 16 batch 44\nepoch 16 batch 45\nepoch 16 batch 46\nepoch 16 batch 47\nepoch 16 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 16, Training Loss: 1577.7434, Validation Loss: 0.0003, accuracy = 1.0000\nepoch 17 batch 0\nepoch 17 batch 1\nepoch 17 batch 2\nepoch 17 batch 3\nepoch 17 batch 4\nepoch 17 batch 5\nepoch 17 batch 6\nepoch 17 batch 7\nepoch 17 batch 8\nepoch 17 batch 9\nepoch 17 batch 10\nepoch 17 batch 11\nepoch 17 batch 12\nepoch 17 batch 13\nepoch 17 batch 14\nepoch 17 batch 15\nepoch 17 batch 16\nepoch 17 batch 17\nepoch 17 batch 18\nepoch 17 batch 19\nepoch 17 batch 20\nepoch 17 batch 21\nepoch 17 batch 22\nepoch 17 batch 23\nepoch 17 batch 24\nepoch 17 batch 25\nepoch 17 batch 26\nepoch 17 batch 27\nepoch 17 batch 28\nepoch 17 batch 29\nepoch 17 batch 30\nepoch 17 batch 31\nepoch 17 batch 32\nepoch 17 batch 33\nepoch 17 batch 34\nepoch 17 batch 35\nepoch 17 batch 36\nepoch 17 batch 37\nepoch 17 batch 38\nepoch 17 batch 39\nepoch 17 batch 40\nepoch 17 batch 41\nepoch 17 batch 42\nepoch 17 batch 43\nepoch 17 batch 44\nepoch 17 batch 45\nepoch 17 batch 46\nepoch 17 batch 47\nepoch 17 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 17, Training Loss: 1577.2903, Validation Loss: 0.0002, accuracy = 1.0000\nepoch 18 batch 0\nepoch 18 batch 1\nepoch 18 batch 2\nepoch 18 batch 3\nepoch 18 batch 4\nepoch 18 batch 5\nepoch 18 batch 6\nepoch 18 batch 7\nepoch 18 batch 8\nepoch 18 batch 9\nepoch 18 batch 10\nepoch 18 batch 11\nepoch 18 batch 12\nepoch 18 batch 13\nepoch 18 batch 14\nepoch 18 batch 15\nepoch 18 batch 16\nepoch 18 batch 17\nepoch 18 batch 18\nepoch 18 batch 19\nepoch 18 batch 20\nepoch 18 batch 21\nepoch 18 batch 22\nepoch 18 batch 23\nepoch 18 batch 24\nepoch 18 batch 25\nepoch 18 batch 26\nepoch 18 batch 27\nepoch 18 batch 28\nepoch 18 batch 29\nepoch 18 batch 30\nepoch 18 batch 31\nepoch 18 batch 32\nepoch 18 batch 33\nepoch 18 batch 34\nepoch 18 batch 35\nepoch 18 batch 36\nepoch 18 batch 37\nepoch 18 batch 38\nepoch 18 batch 39\nepoch 18 batch 40\nepoch 18 batch 41\nepoch 18 batch 42\nepoch 18 batch 43\nepoch 18 batch 44\nepoch 18 batch 45\nepoch 18 batch 46\nepoch 18 batch 47\nepoch 18 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 18, Training Loss: 1577.8504, Validation Loss: 0.0002, accuracy = 1.0000\nepoch 19 batch 0\nepoch 19 batch 1\nepoch 19 batch 2\nepoch 19 batch 3\nepoch 19 batch 4\nepoch 19 batch 5\nepoch 19 batch 6\nepoch 19 batch 7\nepoch 19 batch 8\nepoch 19 batch 9\nepoch 19 batch 10\nepoch 19 batch 11\nepoch 19 batch 12\nepoch 19 batch 13\nepoch 19 batch 14\nepoch 19 batch 15\nepoch 19 batch 16\nepoch 19 batch 17\nepoch 19 batch 18\nepoch 19 batch 19\nepoch 19 batch 20\nepoch 19 batch 21\nepoch 19 batch 22\nepoch 19 batch 23\nepoch 19 batch 24\nepoch 19 batch 25\nepoch 19 batch 26\nepoch 19 batch 27\nepoch 19 batch 28\nepoch 19 batch 29\nepoch 19 batch 30\nepoch 19 batch 31\nepoch 19 batch 32\nepoch 19 batch 33\nepoch 19 batch 34\nepoch 19 batch 35\nepoch 19 batch 36\nepoch 19 batch 37\nepoch 19 batch 38\nepoch 19 batch 39\nepoch 19 batch 40\nepoch 19 batch 41\nepoch 19 batch 42\nepoch 19 batch 43\nepoch 19 batch 44\nepoch 19 batch 45\nepoch 19 batch 46\nepoch 19 batch 47\nepoch 19 batch 48\nSaved model\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nEpoch: 19, Training Loss: 1577.8918, Validation Loss: 0.0002, accuracy = 1.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che è stata creata contenente il modello e i log\n\nif not test_only:\n    from IPython.display import FileLink\n    !zip -r file.zip {model_filepath}\n    FileLink(r'file.zip')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T18:17:13.986102Z","iopub.execute_input":"2024-05-21T18:17:13.986615Z","iopub.status.idle":"2024-05-21T18:17:13.999279Z","shell.execute_reply.started":"2024-05-21T18:17:13.986575Z","shell.execute_reply":"2024-05-21T18:17:13.998108Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"\nfrom torchmetrics.detection import MeanAveragePrecision\n\ndef test(model, test_loader, device=\"cpu\"):   \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in enumerate(test_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,labels)\n            if el[1]['boxes'].size()[0] != 0:\n                inputs.append(el[0][0])\n                targets.append(el[0][1])\n        \n        if len(inputs) == 0:\n            continue\n        \n        # inputs = inputs.to(device)\n        output = model(inputs)\n                \n        #     # Example output\n        #     {'boxes': tensor([[ 0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        # loss = sum(output['scores'])\n        # print(f\"{output = }\")\n        # test_loss += loss * len(inputs)\n        # print(f\"{test_loss = }\")\n\n        # print(f\"{output['scores'] = }\")\n        \n        # res_softmax = torch.softmax(torch.FloatTensor(output['scores']), dim=-1)\n        # print(f\"{res_softmax = }\")\n        # print(f\"{targets = }\")\n        \n        # correct = torch.eq(torch.max(torch.softmax(torch.FloatTensor(output['scores']), dim=0), dim=0)[1], targets).view(-1)\n        # num_correct += torch.sum(correct).item()\n        # num_examples += correct.shape[0]\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        mAP += metric(output,target).map_75\n        \n        del inputs\n        del targets\n        gc.collect()\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T18:56:44.811748Z","iopub.execute_input":"2024-05-25T18:56:44.812327Z","iopub.status.idle":"2024-05-25T18:56:44.823198Z","shell.execute_reply.started":"2024-05-25T18:56:44.812280Z","shell.execute_reply":"2024-05-25T18:56:44.821983Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n!pip install torchmetrics[detection]\n\ncheckpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\"))\nmodel.load_state_dict(checkpoint['model_state_dict'])\ntest(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T19:07:24.208751Z","iopub.execute_input":"2024-05-25T19:07:24.209187Z","iopub.status.idle":"2024-05-25T19:07:36.837834Z","shell.execute_reply.started":"2024-05-25T19:07:24.209156Z","shell.execute_reply":"2024-05-25T19:07:36.835861Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2+cpu)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nRequirement already satisfied: pycocotools>2.0.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.0.7)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2+cpu)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/model-trained\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[25], line 8\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43mMeanAveragePrecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43miou_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbbox\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_thresholds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m mAP \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchmetrics/detection/mean_ap.py:381\u001b[0m, in \u001b[0;36mMeanAveragePrecision.__init__\u001b[0;34m(self, box_format, iou_type, iou_thresholds, rec_thresholds, max_detection_thresholds, class_metrics, extended_summary, average, backend, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (_PYCOCOTOOLS_AVAILABLE \u001b[38;5;129;01mor\u001b[39;00m _FASTER_COCO_EVAL_AVAILABLE):\n\u001b[0;32m--> 381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`MAP` metric requires that `pycocotools` or `faster-coco-eval` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please install with `pip install pycocotools` or `pip install faster-coco-eval` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `pip install torchmetrics[detection]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _TORCHVISION_GREATER_EQUAL_0_8:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`MeanAveragePrecision` metric requires that `torchvision` version 0.8.0 or newer is installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please install with `pip install torchvision>=0.8` or `pip install torchmetrics[detection]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m     )\n","\u001b[0;31mModuleNotFoundError\u001b[0m: `MAP` metric requires that `pycocotools` or `faster-coco-eval` installed. Please install with `pip install pycocotools` or `pip install faster-coco-eval` or `pip install torchmetrics[detection]`."],"ename":"ModuleNotFoundError","evalue":"`MAP` metric requires that `pycocotools` or `faster-coco-eval` installed. Please install with `pip install pycocotools` or `pip install faster-coco-eval` or `pip install torchmetrics[detection]`.","output_type":"error"}]},{"cell_type":"code","source":"# TRAIN AGAIN\nimport pickle\n\ntrain_again = False\n\nif train_again:\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n    model = model_rcnn.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"))\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # Carica i DataLoader\n    with open(os.path.join(model_filepath, 'train_loader.pkl'), 'rb') as f:\n        train_loader = pickle.load(f)\n\n    with open(os.path.join(model_filepath, 'val_loader.pkl'), 'rb') as f:\n        val_loader = pickle.load(f)\n\n    with open(os.path.join(model_filepath, 'test_loader.pkl'), 'rb') as f:\n        test_loader = pickle.load(f)\n\n    train(model, optimizer, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T07:41:13.031345Z","iopub.execute_input":"2024-05-21T07:41:13.032041Z","iopub.status.idle":"2024-05-21T07:41:13.042753Z","shell.execute_reply.started":"2024-05-21T07:41:13.032003Z","shell.execute_reply":"2024-05-21T07:41:13.041401Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"raw","source":"# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for data in val_loader:\n#         images, labels = data[0].to(device), data[1].to(device)\n#         predictions = torch.argmax(model(images),dim=1)\n\n#         total += labels.size(0)\n#         correct += (predictions == labels).sum().item()\n\n# print('accuracy = {:f}'.format(correct / total))\n# print('correct: {:d}  total: {:d}'.format(correct, total))","metadata":{"_uuid":"ce6b85c5-91c9-488b-9ae0-74bd3514487f","_cell_guid":"93e16d46-9dc5-41c7-8837-52d4920e6149","id":"MLPxPQrile1o","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-21T07:41:13.044527Z","iopub.execute_input":"2024-05-21T07:41:13.045158Z","iopub.status.idle":"2024-05-21T07:41:13.054176Z","shell.execute_reply.started":"2024-05-21T07:41:13.045063Z","shell.execute_reply":"2024-05-21T07:41:13.052798Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}