{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8438193,"sourceType":"datasetVersion","datasetId":5026303},{"sourceId":8516397,"sourceType":"datasetVersion","datasetId":5084559,"isSourceIdPinned":true}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"-import os\n\n## MAIN CONFIGURATIONS\nmodel_id = '0'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 50  # Number of epochs the model will train for\nbatch_size = 32\ndataset_percentage = 0.03 # Which percentage of the dataset to use. 0.03 means 4.6k images in the training\ninit_lr = 1e-4 # Initial Learning Rate\ndata_augmentation_type = 'fourier'    # Which data augmentation tecnique are we using?\n                                    # 'noaug':     no data augmentation\n                                    # 'fourier':   fourier transforms\n\n## WHAT WILL THIS SESSION DO?\ntest_only = False # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\ndo_model_test = False # Tests the model after training\ncreate_log_file = True\n\n\nmodel_filepath = f\"model_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\n# !tree # Prints folder structure\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\n\n!pip install torchsummary\nfrom torchsummary import summary\n!pip install torchmetrics\n!pip install torchmetrics[detection]\n# !pip install pycocotools faster-coco-eval","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-04T09:30:23.918598Z","iopub.execute_input":"2024-06-04T09:30:23.919406Z","iopub.status.idle":"2024-06-04T09:31:08.183307Z","shell.execute_reply.started":"2024-06-04T09:30:23.919364Z","shell.execute_reply":"2024-06-04T09:31:08.182268Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nCollecting pycocotools>2.0.0 (from torchmetrics[detection])\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-04T09:31:08.185350Z","iopub.execute_input":"2024-06-04T09:31:08.186055Z","iopub.status.idle":"2024-06-04T09:31:08.220715Z","shell.execute_reply.started":"2024-06-04T09:31:08.186024Z","shell.execute_reply":"2024-06-04T09:31:08.219820Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"device = device(type='cuda')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\n\nif log_filepath:\n    print(log_filepath)\nelse:\n    print(\"No logging\")","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-06-04T09:31:08.221879Z","iopub.execute_input":"2024-06-04T09:31:08.222163Z","iopub.status.idle":"2024-06-04T09:31:08.233877Z","shell.execute_reply.started":"2024-06-04T09:31:08.222139Z","shell.execute_reply":"2024-06-04T09:31:08.232991Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_fourier_id0/log.txt'\nmodels/model_fourier_id0/log.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# FOURIER DATA AUGMENTATION\n\nimport torch.fft as fft\nimport torchvision\nimport random\n\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n        # Scale the image in range [0,1)\n        min_val = 0\n        max_val = 255\n        image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n        min_val = 0\n        max_val = 255\n        # Scale the image in range [0,1)\n        image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:31:08.262044Z","iopub.execute_input":"2024-06-04T09:31:08.262292Z","iopub.status.idle":"2024-06-04T09:31:08.283795Z","shell.execute_reply.started":"2024-06-04T09:31:08.262270Z","shell.execute_reply":"2024-06-04T09:31:08.282921Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n        \n        img_train_transforms = v2.Compose([\n             v2.RandomRotation(50),\n             v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n             v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n    \n    case 'fourier':\n        \n        img_train_transforms = v2.Compose([\n             FourierRandomNoise(),\n             PatchGaussian(),\n             FourierBasisAugmentation(),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n\nprint('ok')","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-06-04T09:31:08.285130Z","iopub.execute_input":"2024-06-04T09:31:08.285382Z","iopub.status.idle":"2024-06-04T09:31:08.332951Z","shell.execute_reply.started":"2024-06-04T09:31:08.285360Z","shell.execute_reply":"2024-06-04T09:31:08.332085Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        try:\n            label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n            label['boxes'] = torch.Tensor(label['boxes'])\n            label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(image.permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            image, label = self.transform(image, label)\n\n        return image, label\n\nprint('ok')","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","id":"V1Q6ogjksMqE","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-04T09:31:08.334104Z","iopub.execute_input":"2024-06-04T09:31:08.334447Z","iopub.status.idle":"2024-06-04T09:31:08.350243Z","shell.execute_reply.started":"2024-06-04T09:31:08.334416Z","shell.execute_reply":"2024-06-04T09:31:08.349328Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"if not test_only:\n\n    from sklearn.model_selection import train_test_split\n    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n    from torchvision import tv_tensors\n\n    # DATASET_DIR = os.path.join(\".\")\n    TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n    TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n    # print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\n    train_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n    train_list, test_list = train_test_split(train_list, test_size = float(1 - dataset_percentage)) # check first cell\n    train_list, val_list = train_test_split(train_list, test_size = 0.2)\n    test_list, _ = train_test_split(test_list, test_size = 0.99)\n    test_list, _ = train_test_split(test_list, test_size = 0.5)\n\n    train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n    test_data = ShipsDataset(test_list, transforms = img_test_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n    val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\n    def custom_collate_fn(batch):\n        # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n        # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n        # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n\n        return batch\n\n    train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n    val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n    test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n\n    # Why custom_collate_fn?\n    # pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n    # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n\n    print(len(train_data),len(train_loader))\n    print(len(val_data), len(val_loader))\n    print(len(test_loader))\n\n    # https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n    # La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n    # /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-06-04T09:31:08.351581Z","iopub.execute_input":"2024-06-04T09:31:08.351866Z","iopub.status.idle":"2024-06-04T09:33:08.077467Z","shell.execute_reply.started":"2024-06-04T09:31:08.351844Z","shell.execute_reply":"2024-06-04T09:33:08.076511Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"4620 145\n1156 37\n30\n","output_type":"stream"}]},{"cell_type":"code","source":"if not test_only:\n    # Save loaders\n    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\n    print('Dataset Loaders saved succesfully!')","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:33:08.080272Z","iopub.execute_input":"2024-06-04T09:33:08.080963Z","iopub.status.idle":"2024-06-04T09:34:20.565537Z","shell.execute_reply.started":"2024-06-04T09:33:08.080935Z","shell.execute_reply":"2024-06-04T09:34:20.564578Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dataset Loaders saved succesfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:34:20.566698Z","iopub.execute_input":"2024-06-04T09:34:20.566998Z","iopub.status.idle":"2024-06-04T09:34:20.574931Z","shell.execute_reply.started":"2024-06-04T09:34:20.566974Z","shell.execute_reply":"2024-06-04T09:34:20.574004Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0893, 0.0827, 0.0817]) original size\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0879, 0.0811, 0.0800]) 224x224\n\n# image_mean_train, image_std_train = get_mean_std(train_loader)\n# image_mean_val, image_std_val = get_mean_std(val_loader)\n# image_mean_test, image_std_test = get_mean_std(test_loader)\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n\n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n    \n    case 'fourier':\n        \n        image_mean_train = torch.Tensor([0.0954, 0.0930, 0.0948])\n        image_std_train = torch.Tensor([0.1039, 0.1016, 0.1036])\n\n        image_mean_val = torch.Tensor([0.1925, 0.2728, 0.3091])\n        image_std_val = torch.Tensor([0.0881, 0.0804, 0.0795])\n\n        image_mean_test = torch.tensor([0.1985, 0.2808, 0.3167])\n        image_std_test = torch.tensor([0.0846, 0.0770, 0.0768])\n\nprint(f\"{image_mean_train = }, {image_std_train =}\")\nprint(f\"{image_mean_val = }, {image_std_val =}\")\nprint(f\"{image_mean_test = }, {image_std_test =}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:34:20.576066Z","iopub.execute_input":"2024-06-04T09:34:20.576327Z","iopub.status.idle":"2024-06-04T09:34:20.651972Z","shell.execute_reply.started":"2024-06-04T09:34:20.576305Z","shell.execute_reply":"2024-06-04T09:34:20.651136Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"image_mean_train = tensor([0.0954, 0.0930, 0.0948]), image_std_train =tensor([0.1039, 0.1016, 0.1036])\nimage_mean_val = tensor([0.1925, 0.2728, 0.3091]), image_std_val =tensor([0.0881, 0.0804, 0.0795])\nimage_mean_test = tensor([0.1985, 0.2808, 0.3167]), image_std_test =tensor([0.0846, 0.0770, 0.0768])\n","output_type":"stream"}]},{"cell_type":"code","source":"def new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\nmodel_rcnn = new_model()\n\nprint('ok')","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","id":"5J9M_bnAxnDk","execution":{"iopub.status.busy":"2024-06-04T09:34:20.653139Z","iopub.execute_input":"2024-06-04T09:34:20.653484Z","iopub.status.idle":"2024-06-04T09:34:25.018551Z","shell.execute_reply.started":"2024-06-04T09:34:20.653454Z","shell.execute_reply":"2024-06-04T09:34:25.017644Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:03<00:00, 48.7MB/s] \n","output_type":"stream"},{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")\n\nprint(\"ok\")","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","id":"Du5q6_RRCmD4","execution":{"iopub.status.busy":"2024-06-04T09:34:25.019819Z","iopub.execute_input":"2024-06-04T09:34:25.020253Z","iopub.status.idle":"2024-06-04T09:34:25.027291Z","shell.execute_reply.started":"2024-06-04T09:34:25.020220Z","shell.execute_reply":"2024-06-04T09:34:25.026321Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\nif not test_only:\n    \n    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n\n    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=\"cpu\", start_from_epoch=0):\n\n        model.transform.image_mean = image_mean_train\n        model.transform.image_std = image_std_train\n        model._skip_resize = True\n\n        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n\n            training_loss = 0.0\n            batch_cumsum = 0\n            model.train()\n\n            for i, batch in enumerate(train_loader):\n                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n                print(f\"epoch {epoch} batch {i}\")\n                batch_cumsum += len(batch) # needed to compute the training loss later\n                optimizer.zero_grad()\n\n                # filtering out empty images (model does not accept empty targets)\n                inputs = []\n                targets = []\n                for el in batch:       # el = ((image,dict),dict) when transforms are active\n                    if el[1]['boxes'].size()[0] != 0:\n                        inputs.append(el[0].to(device))\n                        targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n\n                        # print(f\"{el = }\")\n                        # Example el\n                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                        #          ...,\n                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                        #          ...,\n                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                        #          ...,\n                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n\n                if len(inputs) == 0:\n                    continue\n\n                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n                \"\"\" EXAMPLE :\n                {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n                 'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n                 'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n\n                 How losses are computed:\n\n                 -loss_classifier-\n                 classification_loss = F.cross_entropy(class_logits, labels)\n\n                 -loss_box_reg-\n                 box_loss = F.smooth_l1_loss(\n                    box_regression[sampled_pos_inds_subset, labels_pos],\n                    regression_targets[sampled_pos_inds_subset],\n                    beta=1 / 9,\n                    reduction=\"sum\",\n                )\n                box_loss = box_loss / labels.numel()\n\n                -loss_rpn_box_reg-\n                box_loss = F.smooth_l1_loss(\n                pred_bbox_deltas[sampled_pos_inds],\n                regression_targets[sampled_pos_inds],\n                beta=1 / 9,\n                reduction=\"sum\",\n                ) / (sampled_inds.numel())\n\n                -loss_objectness-\n                objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n                 \"\"\"\n\n                loss = sum(loss for loss in output.values())\n                loss.backward()\n                optimizer.step()\n                training_loss += loss.data.item() * len(batch)\n\n    #             del inputs\n    #             del targets\n    #             gc.collect()    \n\n            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n            scheduler.step() # changes LR\n            training_loss /= batch_cumsum\n            training_losses.append(training_loss)\n            # save_checkpoint(epoch, model, optimizer, scheduler, training_loss, lrs)\n\n            # VALIDATION\n            model.transform.image_mean = image_mean_val\n            model.transform.image_std = image_std_val\n\n            model.train()\n            num_correct = 0\n            num_examples = 0\n            valid_loss = 0\n\n            with torch.no_grad():\n                for i,batch in enumerate(val_loader):\n                    print(\"batch\", i)\n                    inputs = []\n                    targets = []\n\n                    for el in batch:       # el = (image,labels)\n                        if el[1]['boxes'].size()[0] != 0:\n                            inputs.append(el[0].to(device))\n                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n\n                    if len(inputs) == 0:\n                        continue\n\n                    output = model(inputs, targets)\n\n                    loss = sum(loss for loss in output.values())\n                    valid_loss += loss.data.item() *len(batch)\n\n    #                 del inputs\n    #                 del targets\n    #                 gc.collect()\n\n            valid_loss /= len(val_loader.dataset)\n            validation_losses.append(valid_loss)\n\n            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n\n\n            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs)\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5\nprint('ok')","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-06-04T09:39:10.713717Z","iopub.execute_input":"2024-06-04T09:39:10.714091Z","iopub.status.idle":"2024-06-04T09:39:10.736963Z","shell.execute_reply.started":"2024-06-04T09:39:10.714064Z","shell.execute_reply":"2024-06-04T09:39:10.735998Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }\")\n    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n    # plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T09:39:37.939571Z","iopub.execute_input":"2024-06-04T09:39:37.940679Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Beginning training, num_epochs = 50\ndevice = device(type='cuda')\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"epoch 0 batch 0\nepoch 0 batch 1\nepoch 0 batch 2\nepoch 0 batch 3\nepoch 0 batch 4\nepoch 0 batch 5\nepoch 0 batch 6\nepoch 0 batch 7\nepoch 0 batch 8\nepoch 0 batch 9\nepoch 0 batch 10\nepoch 0 batch 11\nepoch 0 batch 12\nepoch 0 batch 13\nepoch 0 batch 14\nepoch 0 batch 15\nepoch 0 batch 16\nepoch 0 batch 17\nepoch 0 batch 18\nepoch 0 batch 19\nepoch 0 batch 20\nepoch 0 batch 21\nepoch 0 batch 22\nepoch 0 batch 23\nepoch 0 batch 24\nepoch 0 batch 25\nepoch 0 batch 26\nepoch 0 batch 27\nepoch 0 batch 28\nepoch 0 batch 29\nepoch 0 batch 30\nepoch 0 batch 31\nepoch 0 batch 32\nepoch 0 batch 33\nepoch 0 batch 34\nepoch 0 batch 35\nepoch 0 batch 36\nepoch 0 batch 37\nepoch 0 batch 38\nepoch 0 batch 39\nepoch 0 batch 40\nepoch 0 batch 41\nepoch 0 batch 42\nepoch 0 batch 43\nepoch 0 batch 44\nepoch 0 batch 45\nepoch 0 batch 46\nepoch 0 batch 47\nepoch 0 batch 48\nepoch 0 batch 49\nepoch 0 batch 50\nepoch 0 batch 51\nepoch 0 batch 52\nepoch 0 batch 53\nepoch 0 batch 54\nepoch 0 batch 55\nepoch 0 batch 56\nepoch 0 batch 57\nepoch 0 batch 58\nepoch 0 batch 59\nepoch 0 batch 60\nepoch 0 batch 61\nepoch 0 batch 62\nepoch 0 batch 63\nepoch 0 batch 64\nepoch 0 batch 65\nepoch 0 batch 66\nepoch 0 batch 67\nepoch 0 batch 68\nepoch 0 batch 69\nepoch 0 batch 70\nepoch 0 batch 71\nepoch 0 batch 72\nepoch 0 batch 73\nepoch 0 batch 74\nepoch 0 batch 75\nepoch 0 batch 76\nepoch 0 batch 77\nepoch 0 batch 78\nepoch 0 batch 79\nepoch 0 batch 80\nepoch 0 batch 81\nepoch 0 batch 82\nepoch 0 batch 83\nepoch 0 batch 84\nepoch 0 batch 85\nepoch 0 batch 86\nepoch 0 batch 87\nepoch 0 batch 88\nepoch 0 batch 89\nepoch 0 batch 90\nepoch 0 batch 91\nepoch 0 batch 92\nepoch 0 batch 93\nepoch 0 batch 94\nepoch 0 batch 95\nepoch 0 batch 96\nepoch 0 batch 97\nepoch 0 batch 98\nepoch 0 batch 99\nepoch 0 batch 100\nepoch 0 batch 101\nepoch 0 batch 102\nepoch 0 batch 103\nepoch 0 batch 104\nepoch 0 batch 105\nepoch 0 batch 106\nepoch 0 batch 107\nepoch 0 batch 108\nepoch 0 batch 109\nepoch 0 batch 110\nepoch 0 batch 111\nepoch 0 batch 112\nepoch 0 batch 113\nepoch 0 batch 114\nepoch 0 batch 115\nepoch 0 batch 116\nepoch 0 batch 117\nepoch 0 batch 118\nepoch 0 batch 119\nepoch 0 batch 120\nepoch 0 batch 121\nepoch 0 batch 122\nepoch 0 batch 123\nepoch 0 batch 124\nepoch 0 batch 125\nepoch 0 batch 126\nepoch 0 batch 127\nepoch 0 batch 128\nepoch 0 batch 129\nepoch 0 batch 130\nepoch 0 batch 131\nepoch 0 batch 132\nepoch 0 batch 133\nepoch 0 batch 134\nepoch 0 batch 135\nepoch 0 batch 136\nepoch 0 batch 137\nepoch 0 batch 138\nepoch 0 batch 139\nepoch 0 batch 140\nepoch 0 batch 141\nepoch 0 batch 142\nepoch 0 batch 143\nepoch 0 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 0, Training Loss: 4.5330, Validation Loss: 4.5895, lr: 0.00010000\nSaved model\nepoch 1 batch 0\nepoch 1 batch 1\nepoch 1 batch 2\nepoch 1 batch 3\nepoch 1 batch 4\nepoch 1 batch 5\nepoch 1 batch 6\nepoch 1 batch 7\nepoch 1 batch 8\nepoch 1 batch 9\nepoch 1 batch 10\nepoch 1 batch 11\nepoch 1 batch 12\nepoch 1 batch 13\nepoch 1 batch 14\nepoch 1 batch 15\nepoch 1 batch 16\nepoch 1 batch 17\nepoch 1 batch 18\nepoch 1 batch 19\nepoch 1 batch 20\nepoch 1 batch 21\nepoch 1 batch 22\nepoch 1 batch 23\nepoch 1 batch 24\nepoch 1 batch 25\nepoch 1 batch 26\nepoch 1 batch 27\nepoch 1 batch 28\nepoch 1 batch 29\nepoch 1 batch 30\nepoch 1 batch 31\nepoch 1 batch 32\nepoch 1 batch 33\nepoch 1 batch 34\nepoch 1 batch 35\nepoch 1 batch 36\nepoch 1 batch 37\nepoch 1 batch 38\nepoch 1 batch 39\nepoch 1 batch 40\nepoch 1 batch 41\nepoch 1 batch 42\nepoch 1 batch 43\nepoch 1 batch 44\nepoch 1 batch 45\nepoch 1 batch 46\nepoch 1 batch 47\nepoch 1 batch 48\nepoch 1 batch 49\nepoch 1 batch 50\nepoch 1 batch 51\nepoch 1 batch 52\nepoch 1 batch 53\nepoch 1 batch 54\nepoch 1 batch 55\nepoch 1 batch 56\nepoch 1 batch 57\nepoch 1 batch 58\nepoch 1 batch 59\nepoch 1 batch 60\nepoch 1 batch 61\nepoch 1 batch 62\nepoch 1 batch 63\nepoch 1 batch 64\nepoch 1 batch 65\nepoch 1 batch 66\nepoch 1 batch 67\nepoch 1 batch 68\nepoch 1 batch 69\nepoch 1 batch 70\nepoch 1 batch 71\nepoch 1 batch 72\nepoch 1 batch 73\nepoch 1 batch 74\nepoch 1 batch 75\nepoch 1 batch 76\nepoch 1 batch 77\nepoch 1 batch 78\nepoch 1 batch 79\nepoch 1 batch 80\nepoch 1 batch 81\nepoch 1 batch 82\nepoch 1 batch 83\nepoch 1 batch 84\nepoch 1 batch 85\nepoch 1 batch 86\nepoch 1 batch 87\nepoch 1 batch 88\nepoch 1 batch 89\nepoch 1 batch 90\nepoch 1 batch 91\nepoch 1 batch 92\nepoch 1 batch 93\nepoch 1 batch 94\nepoch 1 batch 95\nepoch 1 batch 96\nepoch 1 batch 97\nepoch 1 batch 98\nepoch 1 batch 99\nepoch 1 batch 100\nepoch 1 batch 101\nepoch 1 batch 102\nepoch 1 batch 103\nepoch 1 batch 104\nepoch 1 batch 105\nepoch 1 batch 106\nepoch 1 batch 107\nepoch 1 batch 108\nepoch 1 batch 109\nepoch 1 batch 110\nepoch 1 batch 111\nepoch 1 batch 112\nepoch 1 batch 113\nepoch 1 batch 114\nepoch 1 batch 115\nepoch 1 batch 116\nepoch 1 batch 117\nepoch 1 batch 118\nepoch 1 batch 119\nepoch 1 batch 120\nepoch 1 batch 121\nepoch 1 batch 122\nepoch 1 batch 123\nepoch 1 batch 124\nepoch 1 batch 125\nepoch 1 batch 126\nepoch 1 batch 127\nepoch 1 batch 128\nepoch 1 batch 129\nepoch 1 batch 130\nepoch 1 batch 131\nepoch 1 batch 132\nepoch 1 batch 133\nepoch 1 batch 134\nepoch 1 batch 135\nepoch 1 batch 136\nepoch 1 batch 137\nepoch 1 batch 138\nepoch 1 batch 139\nepoch 1 batch 140\nepoch 1 batch 141\nepoch 1 batch 142\nepoch 1 batch 143\nepoch 1 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 1, Training Loss: 4.1685, Validation Loss: 4.6212, lr: 0.00010000\nSaved model\nepoch 2 batch 0\nepoch 2 batch 1\nepoch 2 batch 2\nepoch 2 batch 3\nepoch 2 batch 4\nepoch 2 batch 5\nepoch 2 batch 6\nepoch 2 batch 7\nepoch 2 batch 8\nepoch 2 batch 9\nepoch 2 batch 10\nepoch 2 batch 11\nepoch 2 batch 12\nepoch 2 batch 13\nepoch 2 batch 14\nepoch 2 batch 15\nepoch 2 batch 16\nepoch 2 batch 17\nepoch 2 batch 18\nepoch 2 batch 19\nepoch 2 batch 20\nepoch 2 batch 21\nepoch 2 batch 22\nepoch 2 batch 23\nepoch 2 batch 24\nepoch 2 batch 25\nepoch 2 batch 26\nepoch 2 batch 27\nepoch 2 batch 28\nepoch 2 batch 29\nepoch 2 batch 30\nepoch 2 batch 31\nepoch 2 batch 32\nepoch 2 batch 33\nepoch 2 batch 34\nepoch 2 batch 35\nepoch 2 batch 36\nepoch 2 batch 37\nepoch 2 batch 38\nepoch 2 batch 39\nepoch 2 batch 40\nepoch 2 batch 41\nepoch 2 batch 42\nepoch 2 batch 43\nepoch 2 batch 44\nepoch 2 batch 45\nepoch 2 batch 46\nepoch 2 batch 47\nepoch 2 batch 48\nepoch 2 batch 49\nepoch 2 batch 50\nepoch 2 batch 51\nepoch 2 batch 52\nepoch 2 batch 53\nepoch 2 batch 54\nepoch 2 batch 55\nepoch 2 batch 56\nepoch 2 batch 57\nepoch 2 batch 58\nepoch 2 batch 59\nepoch 2 batch 60\nepoch 2 batch 61\nepoch 2 batch 62\nepoch 2 batch 63\nepoch 2 batch 64\nepoch 2 batch 65\nepoch 2 batch 66\nepoch 2 batch 67\nepoch 2 batch 68\nepoch 2 batch 69\nepoch 2 batch 70\nepoch 2 batch 71\nepoch 2 batch 72\nepoch 2 batch 73\nepoch 2 batch 74\nepoch 2 batch 75\nepoch 2 batch 76\nepoch 2 batch 77\nepoch 2 batch 78\nepoch 2 batch 79\nepoch 2 batch 80\nepoch 2 batch 81\nepoch 2 batch 82\nepoch 2 batch 83\nepoch 2 batch 84\nepoch 2 batch 85\nepoch 2 batch 86\nepoch 2 batch 87\nepoch 2 batch 88\nepoch 2 batch 89\nepoch 2 batch 90\nepoch 2 batch 91\nepoch 2 batch 92\nepoch 2 batch 93\nepoch 2 batch 94\nepoch 2 batch 95\nepoch 2 batch 96\nepoch 2 batch 97\nepoch 2 batch 98\nepoch 2 batch 99\nepoch 2 batch 100\nepoch 2 batch 101\nepoch 2 batch 102\nepoch 2 batch 103\nepoch 2 batch 104\nepoch 2 batch 105\nepoch 2 batch 106\nepoch 2 batch 107\nepoch 2 batch 108\nepoch 2 batch 109\nepoch 2 batch 110\nepoch 2 batch 111\nepoch 2 batch 112\nepoch 2 batch 113\nepoch 2 batch 114\nepoch 2 batch 115\nepoch 2 batch 116\nepoch 2 batch 117\nepoch 2 batch 118\nepoch 2 batch 119\nepoch 2 batch 120\nepoch 2 batch 121\nepoch 2 batch 122\nepoch 2 batch 123\nepoch 2 batch 124\nepoch 2 batch 125\nepoch 2 batch 126\nepoch 2 batch 127\nepoch 2 batch 128\nepoch 2 batch 129\nepoch 2 batch 130\nepoch 2 batch 131\nepoch 2 batch 132\nepoch 2 batch 133\nepoch 2 batch 134\nepoch 2 batch 135\nepoch 2 batch 136\nepoch 2 batch 137\nepoch 2 batch 138\nepoch 2 batch 139\nepoch 2 batch 140\nepoch 2 batch 141\nepoch 2 batch 142\nepoch 2 batch 143\nepoch 2 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 2, Training Loss: 4.1417, Validation Loss: 4.5658, lr: 0.00010000\nSaved model\nepoch 3 batch 0\nepoch 3 batch 1\nepoch 3 batch 2\nepoch 3 batch 3\nepoch 3 batch 4\nepoch 3 batch 5\nepoch 3 batch 6\nepoch 3 batch 7\nepoch 3 batch 8\nepoch 3 batch 9\nepoch 3 batch 10\nepoch 3 batch 11\nepoch 3 batch 12\nepoch 3 batch 13\nepoch 3 batch 14\nepoch 3 batch 15\nepoch 3 batch 16\nepoch 3 batch 17\nepoch 3 batch 18\nepoch 3 batch 19\nepoch 3 batch 20\nepoch 3 batch 21\nepoch 3 batch 22\nepoch 3 batch 23\nepoch 3 batch 24\nepoch 3 batch 25\nepoch 3 batch 26\nepoch 3 batch 27\nepoch 3 batch 28\nepoch 3 batch 29\nepoch 3 batch 30\nepoch 3 batch 31\nepoch 3 batch 32\nepoch 3 batch 33\nepoch 3 batch 34\nepoch 3 batch 35\nepoch 3 batch 36\nepoch 3 batch 37\nepoch 3 batch 38\nepoch 3 batch 39\nepoch 3 batch 40\nepoch 3 batch 41\nepoch 3 batch 42\nepoch 3 batch 43\nepoch 3 batch 44\nepoch 3 batch 45\nepoch 3 batch 46\nepoch 3 batch 47\nepoch 3 batch 48\nepoch 3 batch 49\nepoch 3 batch 50\nepoch 3 batch 51\nepoch 3 batch 52\nepoch 3 batch 53\nepoch 3 batch 54\nepoch 3 batch 55\nepoch 3 batch 56\nepoch 3 batch 57\nepoch 3 batch 58\nepoch 3 batch 59\nepoch 3 batch 60\nepoch 3 batch 61\nepoch 3 batch 62\nepoch 3 batch 63\nepoch 3 batch 64\nepoch 3 batch 65\nepoch 3 batch 66\nepoch 3 batch 67\nepoch 3 batch 68\nepoch 3 batch 69\nepoch 3 batch 70\nepoch 3 batch 71\nepoch 3 batch 72\nepoch 3 batch 73\nepoch 3 batch 74\nepoch 3 batch 75\nepoch 3 batch 76\nepoch 3 batch 77\nepoch 3 batch 78\nepoch 3 batch 79\nepoch 3 batch 80\nepoch 3 batch 81\nepoch 3 batch 82\nepoch 3 batch 83\nepoch 3 batch 84\nepoch 3 batch 85\nepoch 3 batch 86\nepoch 3 batch 87\nepoch 3 batch 88\nepoch 3 batch 89\nepoch 3 batch 90\nepoch 3 batch 91\nepoch 3 batch 92\nepoch 3 batch 93\nepoch 3 batch 94\nepoch 3 batch 95\nepoch 3 batch 96\nepoch 3 batch 97\nepoch 3 batch 98\nepoch 3 batch 99\nepoch 3 batch 100\nepoch 3 batch 101\nepoch 3 batch 102\nepoch 3 batch 103\nepoch 3 batch 104\nepoch 3 batch 105\nepoch 3 batch 106\nepoch 3 batch 107\nepoch 3 batch 108\nepoch 3 batch 109\nepoch 3 batch 110\nepoch 3 batch 111\nepoch 3 batch 112\nepoch 3 batch 113\nepoch 3 batch 114\nepoch 3 batch 115\nepoch 3 batch 116\nepoch 3 batch 117\nepoch 3 batch 118\nepoch 3 batch 119\nepoch 3 batch 120\nepoch 3 batch 121\nepoch 3 batch 122\nepoch 3 batch 123\nepoch 3 batch 124\nepoch 3 batch 125\nepoch 3 batch 126\nepoch 3 batch 127\nepoch 3 batch 128\nepoch 3 batch 129\nepoch 3 batch 130\nepoch 3 batch 131\nepoch 3 batch 132\nepoch 3 batch 133\nepoch 3 batch 134\nepoch 3 batch 135\nepoch 3 batch 136\nepoch 3 batch 137\nepoch 3 batch 138\nepoch 3 batch 139\nepoch 3 batch 140\nepoch 3 batch 141\nepoch 3 batch 142\nepoch 3 batch 143\nepoch 3 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 3, Training Loss: 4.1533, Validation Loss: 4.5753, lr: 0.00010000\nSaved model\nepoch 4 batch 0\nepoch 4 batch 1\nepoch 4 batch 2\nepoch 4 batch 3\nepoch 4 batch 4\nepoch 4 batch 5\nepoch 4 batch 6\nepoch 4 batch 7\nepoch 4 batch 8\nepoch 4 batch 9\nepoch 4 batch 10\nepoch 4 batch 11\nepoch 4 batch 12\nepoch 4 batch 13\nepoch 4 batch 14\nepoch 4 batch 15\nepoch 4 batch 16\nepoch 4 batch 17\nepoch 4 batch 18\nepoch 4 batch 19\nepoch 4 batch 20\nepoch 4 batch 21\nepoch 4 batch 22\nepoch 4 batch 23\nepoch 4 batch 24\nepoch 4 batch 25\nepoch 4 batch 26\nepoch 4 batch 27\nepoch 4 batch 28\nepoch 4 batch 29\nepoch 4 batch 30\nepoch 4 batch 31\nepoch 4 batch 32\nepoch 4 batch 33\nepoch 4 batch 34\nepoch 4 batch 35\nepoch 4 batch 36\nepoch 4 batch 37\nepoch 4 batch 38\nepoch 4 batch 39\nepoch 4 batch 40\nepoch 4 batch 41\nepoch 4 batch 42\nepoch 4 batch 43\nepoch 4 batch 44\nepoch 4 batch 45\nepoch 4 batch 46\nepoch 4 batch 47\nepoch 4 batch 48\nepoch 4 batch 49\nepoch 4 batch 50\nepoch 4 batch 51\nepoch 4 batch 52\nepoch 4 batch 53\nepoch 4 batch 54\nepoch 4 batch 55\nepoch 4 batch 56\nepoch 4 batch 57\nepoch 4 batch 58\nepoch 4 batch 59\nepoch 4 batch 60\nepoch 4 batch 61\nepoch 4 batch 62\nepoch 4 batch 63\nepoch 4 batch 64\nepoch 4 batch 65\nepoch 4 batch 66\nepoch 4 batch 67\nepoch 4 batch 68\nepoch 4 batch 69\nepoch 4 batch 70\nepoch 4 batch 71\nepoch 4 batch 72\nepoch 4 batch 73\nepoch 4 batch 74\nepoch 4 batch 75\nepoch 4 batch 76\nepoch 4 batch 77\nepoch 4 batch 78\nepoch 4 batch 79\nepoch 4 batch 80\nepoch 4 batch 81\nepoch 4 batch 82\nepoch 4 batch 83\nepoch 4 batch 84\nepoch 4 batch 85\nepoch 4 batch 86\nepoch 4 batch 87\nepoch 4 batch 88\nepoch 4 batch 89\nepoch 4 batch 90\nepoch 4 batch 91\nepoch 4 batch 92\nepoch 4 batch 93\nepoch 4 batch 94\nepoch 4 batch 95\nepoch 4 batch 96\nepoch 4 batch 97\nepoch 4 batch 98\nepoch 4 batch 99\nepoch 4 batch 100\nepoch 4 batch 101\nepoch 4 batch 102\nepoch 4 batch 103\nepoch 4 batch 104\nepoch 4 batch 105\nepoch 4 batch 106\nepoch 4 batch 107\nepoch 4 batch 108\nepoch 4 batch 109\nepoch 4 batch 110\nepoch 4 batch 111\nepoch 4 batch 112\nepoch 4 batch 113\nepoch 4 batch 114\nepoch 4 batch 115\nepoch 4 batch 116\nepoch 4 batch 117\nepoch 4 batch 118\nepoch 4 batch 119\nepoch 4 batch 120\nepoch 4 batch 121\nepoch 4 batch 122\nepoch 4 batch 123\nepoch 4 batch 124\nepoch 4 batch 125\nepoch 4 batch 126\nepoch 4 batch 127\nepoch 4 batch 128\nepoch 4 batch 129\nepoch 4 batch 130\nepoch 4 batch 131\nepoch 4 batch 132\nepoch 4 batch 133\nepoch 4 batch 134\nepoch 4 batch 135\nepoch 4 batch 136\nepoch 4 batch 137\nepoch 4 batch 138\nepoch 4 batch 139\nepoch 4 batch 140\nepoch 4 batch 141\nepoch 4 batch 142\nepoch 4 batch 143\nepoch 4 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 4, Training Loss: 4.1639, Validation Loss: 4.6026, lr: 0.00010000\nSaved model\nepoch 5 batch 0\nepoch 5 batch 1\nepoch 5 batch 2\nepoch 5 batch 3\nepoch 5 batch 4\nepoch 5 batch 5\nepoch 5 batch 6\nepoch 5 batch 7\nepoch 5 batch 8\nepoch 5 batch 9\nepoch 5 batch 10\nepoch 5 batch 11\nepoch 5 batch 12\nepoch 5 batch 13\nepoch 5 batch 14\nepoch 5 batch 15\nepoch 5 batch 16\nepoch 5 batch 17\nepoch 5 batch 18\nepoch 5 batch 19\nepoch 5 batch 20\nepoch 5 batch 21\nepoch 5 batch 22\nepoch 5 batch 23\nepoch 5 batch 24\nepoch 5 batch 25\nepoch 5 batch 26\nepoch 5 batch 27\nepoch 5 batch 28\nepoch 5 batch 29\nepoch 5 batch 30\nepoch 5 batch 31\nepoch 5 batch 32\nepoch 5 batch 33\nepoch 5 batch 34\nepoch 5 batch 35\nepoch 5 batch 36\nepoch 5 batch 37\nepoch 5 batch 38\nepoch 5 batch 39\nepoch 5 batch 40\nepoch 5 batch 41\nepoch 5 batch 42\nepoch 5 batch 43\nepoch 5 batch 44\nepoch 5 batch 45\nepoch 5 batch 46\nepoch 5 batch 47\nepoch 5 batch 48\nepoch 5 batch 49\nepoch 5 batch 50\nepoch 5 batch 51\nepoch 5 batch 52\nepoch 5 batch 53\nepoch 5 batch 54\nepoch 5 batch 55\nepoch 5 batch 56\nepoch 5 batch 57\nepoch 5 batch 58\nepoch 5 batch 59\nepoch 5 batch 60\nepoch 5 batch 61\nepoch 5 batch 62\nepoch 5 batch 63\nepoch 5 batch 64\nepoch 5 batch 65\nepoch 5 batch 66\nepoch 5 batch 67\nepoch 5 batch 68\nepoch 5 batch 69\nepoch 5 batch 70\nepoch 5 batch 71\nepoch 5 batch 72\nepoch 5 batch 73\nepoch 5 batch 74\nepoch 5 batch 75\nepoch 5 batch 76\nepoch 5 batch 77\nepoch 5 batch 78\nepoch 5 batch 79\nepoch 5 batch 80\nepoch 5 batch 81\nepoch 5 batch 82\nepoch 5 batch 83\nepoch 5 batch 84\nepoch 5 batch 85\nepoch 5 batch 86\nepoch 5 batch 87\nepoch 5 batch 88\nepoch 5 batch 89\nepoch 5 batch 90\nepoch 5 batch 91\nepoch 5 batch 92\nepoch 5 batch 93\nepoch 5 batch 94\nepoch 5 batch 95\nepoch 5 batch 96\nepoch 5 batch 97\nepoch 5 batch 98\nepoch 5 batch 99\nepoch 5 batch 100\nepoch 5 batch 101\nepoch 5 batch 102\nepoch 5 batch 103\nepoch 5 batch 104\nepoch 5 batch 105\nepoch 5 batch 106\nepoch 5 batch 107\nepoch 5 batch 108\nepoch 5 batch 109\nepoch 5 batch 110\nepoch 5 batch 111\nepoch 5 batch 112\nepoch 5 batch 113\nepoch 5 batch 114\nepoch 5 batch 115\nepoch 5 batch 116\nepoch 5 batch 117\nepoch 5 batch 118\nepoch 5 batch 119\nepoch 5 batch 120\nepoch 5 batch 121\nepoch 5 batch 122\nepoch 5 batch 123\nepoch 5 batch 124\nepoch 5 batch 125\nepoch 5 batch 126\nepoch 5 batch 127\nepoch 5 batch 128\nepoch 5 batch 129\nepoch 5 batch 130\nepoch 5 batch 131\nepoch 5 batch 132\nepoch 5 batch 133\nepoch 5 batch 134\nepoch 5 batch 135\nepoch 5 batch 136\nepoch 5 batch 137\nepoch 5 batch 138\nepoch 5 batch 139\nepoch 5 batch 140\nepoch 5 batch 141\nepoch 5 batch 142\nepoch 5 batch 143\nepoch 5 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 5, Training Loss: 4.1459, Validation Loss: 4.5608, lr: 0.00009000\nSaved model\nepoch 6 batch 0\nepoch 6 batch 1\nepoch 6 batch 2\nepoch 6 batch 3\nepoch 6 batch 4\nepoch 6 batch 5\nepoch 6 batch 6\nepoch 6 batch 7\nepoch 6 batch 8\nepoch 6 batch 9\nepoch 6 batch 10\nepoch 6 batch 11\nepoch 6 batch 12\nepoch 6 batch 13\nepoch 6 batch 14\nepoch 6 batch 15\nepoch 6 batch 16\nepoch 6 batch 17\nepoch 6 batch 18\nepoch 6 batch 19\nepoch 6 batch 20\nepoch 6 batch 21\nepoch 6 batch 22\nepoch 6 batch 23\nepoch 6 batch 24\nepoch 6 batch 25\nepoch 6 batch 26\nepoch 6 batch 27\nepoch 6 batch 28\nepoch 6 batch 29\nepoch 6 batch 30\nepoch 6 batch 31\nepoch 6 batch 32\nepoch 6 batch 33\nepoch 6 batch 34\nepoch 6 batch 35\nepoch 6 batch 36\nepoch 6 batch 37\nepoch 6 batch 38\nepoch 6 batch 39\nepoch 6 batch 40\nepoch 6 batch 41\nepoch 6 batch 42\nepoch 6 batch 43\nepoch 6 batch 44\nepoch 6 batch 45\nepoch 6 batch 46\nepoch 6 batch 47\nepoch 6 batch 48\nepoch 6 batch 49\nepoch 6 batch 50\nepoch 6 batch 51\nepoch 6 batch 52\nepoch 6 batch 53\nepoch 6 batch 54\nepoch 6 batch 55\nepoch 6 batch 56\nepoch 6 batch 57\nepoch 6 batch 58\nepoch 6 batch 59\nepoch 6 batch 60\nepoch 6 batch 61\nepoch 6 batch 62\nepoch 6 batch 63\nepoch 6 batch 64\nepoch 6 batch 65\nepoch 6 batch 66\nepoch 6 batch 67\nepoch 6 batch 68\nepoch 6 batch 69\nepoch 6 batch 70\nepoch 6 batch 71\nepoch 6 batch 72\nepoch 6 batch 73\nepoch 6 batch 74\nepoch 6 batch 75\nepoch 6 batch 76\nepoch 6 batch 77\nepoch 6 batch 78\nepoch 6 batch 79\nepoch 6 batch 80\nepoch 6 batch 81\nepoch 6 batch 82\nepoch 6 batch 83\nepoch 6 batch 84\nepoch 6 batch 85\nepoch 6 batch 86\nepoch 6 batch 87\nepoch 6 batch 88\nepoch 6 batch 89\nepoch 6 batch 90\nepoch 6 batch 91\nepoch 6 batch 92\nepoch 6 batch 93\nepoch 6 batch 94\nepoch 6 batch 95\nepoch 6 batch 96\nepoch 6 batch 97\nepoch 6 batch 98\nepoch 6 batch 99\nepoch 6 batch 100\nepoch 6 batch 101\nepoch 6 batch 102\nepoch 6 batch 103\nepoch 6 batch 104\nepoch 6 batch 105\nepoch 6 batch 106\nepoch 6 batch 107\nepoch 6 batch 108\nepoch 6 batch 109\nepoch 6 batch 110\nepoch 6 batch 111\nepoch 6 batch 112\nepoch 6 batch 113\nepoch 6 batch 114\nepoch 6 batch 115\nepoch 6 batch 116\nepoch 6 batch 117\nepoch 6 batch 118\nepoch 6 batch 119\nepoch 6 batch 120\nepoch 6 batch 121\nepoch 6 batch 122\nepoch 6 batch 123\nepoch 6 batch 124\nepoch 6 batch 125\nepoch 6 batch 126\nepoch 6 batch 127\nepoch 6 batch 128\nepoch 6 batch 129\nepoch 6 batch 130\nepoch 6 batch 131\nepoch 6 batch 132\nepoch 6 batch 133\nepoch 6 batch 134\nepoch 6 batch 135\nepoch 6 batch 136\nepoch 6 batch 137\nepoch 6 batch 138\nepoch 6 batch 139\nepoch 6 batch 140\nepoch 6 batch 141\nepoch 6 batch 142\nepoch 6 batch 143\nepoch 6 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 6, Training Loss: 4.1630, Validation Loss: 4.5867, lr: 0.00009000\nSaved model\nepoch 7 batch 0\nepoch 7 batch 1\nepoch 7 batch 2\nepoch 7 batch 3\nepoch 7 batch 4\nepoch 7 batch 5\nepoch 7 batch 6\nepoch 7 batch 7\nepoch 7 batch 8\nepoch 7 batch 9\nepoch 7 batch 10\nepoch 7 batch 11\nepoch 7 batch 12\nepoch 7 batch 13\nepoch 7 batch 14\nepoch 7 batch 15\nepoch 7 batch 16\nepoch 7 batch 17\nepoch 7 batch 18\nepoch 7 batch 19\nepoch 7 batch 20\nepoch 7 batch 21\nepoch 7 batch 22\nepoch 7 batch 23\nepoch 7 batch 24\nepoch 7 batch 25\nepoch 7 batch 26\nepoch 7 batch 27\nepoch 7 batch 28\nepoch 7 batch 29\nepoch 7 batch 30\nepoch 7 batch 31\nepoch 7 batch 32\nepoch 7 batch 33\nepoch 7 batch 34\nepoch 7 batch 35\nepoch 7 batch 36\nepoch 7 batch 37\nepoch 7 batch 38\nepoch 7 batch 39\nepoch 7 batch 40\nepoch 7 batch 41\nepoch 7 batch 42\nepoch 7 batch 43\nepoch 7 batch 44\nepoch 7 batch 45\nepoch 7 batch 46\nepoch 7 batch 47\nepoch 7 batch 48\nepoch 7 batch 49\nepoch 7 batch 50\nepoch 7 batch 51\nepoch 7 batch 52\nepoch 7 batch 53\nepoch 7 batch 54\nepoch 7 batch 55\nepoch 7 batch 56\nepoch 7 batch 57\nepoch 7 batch 58\nepoch 7 batch 59\nepoch 7 batch 60\nepoch 7 batch 61\nepoch 7 batch 62\nepoch 7 batch 63\nepoch 7 batch 64\nepoch 7 batch 65\nepoch 7 batch 66\nepoch 7 batch 67\nepoch 7 batch 68\nepoch 7 batch 69\nepoch 7 batch 70\nepoch 7 batch 71\nepoch 7 batch 72\nepoch 7 batch 73\nepoch 7 batch 74\nepoch 7 batch 75\nepoch 7 batch 76\nepoch 7 batch 77\nepoch 7 batch 78\nepoch 7 batch 79\nepoch 7 batch 80\nepoch 7 batch 81\nepoch 7 batch 82\nepoch 7 batch 83\nepoch 7 batch 84\nepoch 7 batch 85\nepoch 7 batch 86\nepoch 7 batch 87\nepoch 7 batch 88\nepoch 7 batch 89\nepoch 7 batch 90\nepoch 7 batch 91\nepoch 7 batch 92\nepoch 7 batch 93\nepoch 7 batch 94\nepoch 7 batch 95\nepoch 7 batch 96\nepoch 7 batch 97\nepoch 7 batch 98\nepoch 7 batch 99\nepoch 7 batch 100\nepoch 7 batch 101\nepoch 7 batch 102\nepoch 7 batch 103\nepoch 7 batch 104\nepoch 7 batch 105\nepoch 7 batch 106\nepoch 7 batch 107\nepoch 7 batch 108\nepoch 7 batch 109\nepoch 7 batch 110\nepoch 7 batch 111\nepoch 7 batch 112\nepoch 7 batch 113\nepoch 7 batch 114\nepoch 7 batch 115\nepoch 7 batch 116\nepoch 7 batch 117\nepoch 7 batch 118\nepoch 7 batch 119\nepoch 7 batch 120\nepoch 7 batch 121\nepoch 7 batch 122\nepoch 7 batch 123\nepoch 7 batch 124\nepoch 7 batch 125\nepoch 7 batch 126\nepoch 7 batch 127\nepoch 7 batch 128\nepoch 7 batch 129\nepoch 7 batch 130\nepoch 7 batch 131\nepoch 7 batch 132\nepoch 7 batch 133\nepoch 7 batch 134\nepoch 7 batch 135\nepoch 7 batch 136\nepoch 7 batch 137\nepoch 7 batch 138\nepoch 7 batch 139\nepoch 7 batch 140\nepoch 7 batch 141\nepoch 7 batch 142\nepoch 7 batch 143\nepoch 7 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 7, Training Loss: 4.1401, Validation Loss: 4.5762, lr: 0.00009000\nSaved model\nepoch 8 batch 0\nepoch 8 batch 1\nepoch 8 batch 2\nepoch 8 batch 3\nepoch 8 batch 4\nepoch 8 batch 5\nepoch 8 batch 6\nepoch 8 batch 7\nepoch 8 batch 8\nepoch 8 batch 9\nepoch 8 batch 10\nepoch 8 batch 11\nepoch 8 batch 12\nepoch 8 batch 13\nepoch 8 batch 14\nepoch 8 batch 15\nepoch 8 batch 16\nepoch 8 batch 17\nepoch 8 batch 18\nepoch 8 batch 19\nepoch 8 batch 20\nepoch 8 batch 21\nepoch 8 batch 22\nepoch 8 batch 23\nepoch 8 batch 24\nepoch 8 batch 25\nepoch 8 batch 26\nepoch 8 batch 27\nepoch 8 batch 28\nepoch 8 batch 29\nepoch 8 batch 30\nepoch 8 batch 31\nepoch 8 batch 32\nepoch 8 batch 33\nepoch 8 batch 34\nepoch 8 batch 35\nepoch 8 batch 36\nepoch 8 batch 37\nepoch 8 batch 38\nepoch 8 batch 39\nepoch 8 batch 40\nepoch 8 batch 41\nepoch 8 batch 42\nepoch 8 batch 43\nepoch 8 batch 44\nepoch 8 batch 45\nepoch 8 batch 46\nepoch 8 batch 47\nepoch 8 batch 48\nepoch 8 batch 49\nepoch 8 batch 50\nepoch 8 batch 51\nepoch 8 batch 52\nepoch 8 batch 53\nepoch 8 batch 54\nepoch 8 batch 55\nepoch 8 batch 56\nepoch 8 batch 57\nepoch 8 batch 58\nepoch 8 batch 59\nepoch 8 batch 60\nepoch 8 batch 61\nepoch 8 batch 62\nepoch 8 batch 63\nepoch 8 batch 64\nepoch 8 batch 65\nepoch 8 batch 66\nepoch 8 batch 67\nepoch 8 batch 68\nepoch 8 batch 69\nepoch 8 batch 70\nepoch 8 batch 71\nepoch 8 batch 72\nepoch 8 batch 73\nepoch 8 batch 74\nepoch 8 batch 75\nepoch 8 batch 76\nepoch 8 batch 77\nepoch 8 batch 78\nepoch 8 batch 79\nepoch 8 batch 80\nepoch 8 batch 81\nepoch 8 batch 82\nepoch 8 batch 83\nepoch 8 batch 84\nepoch 8 batch 85\nepoch 8 batch 86\nepoch 8 batch 87\nepoch 8 batch 88\nepoch 8 batch 89\nepoch 8 batch 90\nepoch 8 batch 91\nepoch 8 batch 92\nepoch 8 batch 93\nepoch 8 batch 94\nepoch 8 batch 95\nepoch 8 batch 96\nepoch 8 batch 97\nepoch 8 batch 98\nepoch 8 batch 99\nepoch 8 batch 100\nepoch 8 batch 101\nepoch 8 batch 102\nepoch 8 batch 103\nepoch 8 batch 104\nepoch 8 batch 105\nepoch 8 batch 106\nepoch 8 batch 107\nepoch 8 batch 108\nepoch 8 batch 109\nepoch 8 batch 110\nepoch 8 batch 111\nepoch 8 batch 112\nepoch 8 batch 113\nepoch 8 batch 114\nepoch 8 batch 115\nepoch 8 batch 116\nepoch 8 batch 117\nepoch 8 batch 118\nepoch 8 batch 119\nepoch 8 batch 120\nepoch 8 batch 121\nepoch 8 batch 122\nepoch 8 batch 123\nepoch 8 batch 124\nepoch 8 batch 125\nepoch 8 batch 126\nepoch 8 batch 127\nepoch 8 batch 128\nepoch 8 batch 129\nepoch 8 batch 130\nepoch 8 batch 131\nepoch 8 batch 132\nepoch 8 batch 133\nepoch 8 batch 134\nepoch 8 batch 135\nepoch 8 batch 136\nepoch 8 batch 137\nepoch 8 batch 138\nepoch 8 batch 139\nepoch 8 batch 140\nepoch 8 batch 141\nepoch 8 batch 142\nepoch 8 batch 143\nepoch 8 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 8, Training Loss: 4.1776, Validation Loss: 4.5589, lr: 0.00009000\nSaved model\nepoch 9 batch 0\nepoch 9 batch 1\nepoch 9 batch 2\nepoch 9 batch 3\nepoch 9 batch 4\nepoch 9 batch 5\nepoch 9 batch 6\nepoch 9 batch 7\nepoch 9 batch 8\nepoch 9 batch 9\nepoch 9 batch 10\nepoch 9 batch 11\nepoch 9 batch 12\nepoch 9 batch 13\nepoch 9 batch 14\nepoch 9 batch 15\nepoch 9 batch 16\nepoch 9 batch 17\nepoch 9 batch 18\nepoch 9 batch 19\nepoch 9 batch 20\nepoch 9 batch 21\nepoch 9 batch 22\nepoch 9 batch 23\nepoch 9 batch 24\nepoch 9 batch 25\nepoch 9 batch 26\nepoch 9 batch 27\nepoch 9 batch 28\nepoch 9 batch 29\nepoch 9 batch 30\nepoch 9 batch 31\nepoch 9 batch 32\nepoch 9 batch 33\nepoch 9 batch 34\nepoch 9 batch 35\nepoch 9 batch 36\nepoch 9 batch 37\nepoch 9 batch 38\nepoch 9 batch 39\nepoch 9 batch 40\nepoch 9 batch 41\nepoch 9 batch 42\nepoch 9 batch 43\nepoch 9 batch 44\nepoch 9 batch 45\nepoch 9 batch 46\nepoch 9 batch 47\nepoch 9 batch 48\nepoch 9 batch 49\nepoch 9 batch 50\nepoch 9 batch 51\nepoch 9 batch 52\nepoch 9 batch 53\nepoch 9 batch 54\nepoch 9 batch 55\nepoch 9 batch 56\nepoch 9 batch 57\nepoch 9 batch 58\nepoch 9 batch 59\nepoch 9 batch 60\nepoch 9 batch 61\nepoch 9 batch 62\nepoch 9 batch 63\nepoch 9 batch 64\nepoch 9 batch 65\nepoch 9 batch 66\nepoch 9 batch 67\nepoch 9 batch 68\nepoch 9 batch 69\nepoch 9 batch 70\nepoch 9 batch 71\nepoch 9 batch 72\nepoch 9 batch 73\nepoch 9 batch 74\nepoch 9 batch 75\nepoch 9 batch 76\nepoch 9 batch 77\nepoch 9 batch 78\nepoch 9 batch 79\nepoch 9 batch 80\nepoch 9 batch 81\nepoch 9 batch 82\nepoch 9 batch 83\nepoch 9 batch 84\nepoch 9 batch 85\nepoch 9 batch 86\nepoch 9 batch 87\nepoch 9 batch 88\nepoch 9 batch 89\nepoch 9 batch 90\nepoch 9 batch 91\nepoch 9 batch 92\nepoch 9 batch 93\nepoch 9 batch 94\nepoch 9 batch 95\nepoch 9 batch 96\nepoch 9 batch 97\nepoch 9 batch 98\nepoch 9 batch 99\nepoch 9 batch 100\nepoch 9 batch 101\nepoch 9 batch 102\nepoch 9 batch 103\nepoch 9 batch 104\nepoch 9 batch 105\nepoch 9 batch 106\nepoch 9 batch 107\nepoch 9 batch 108\nepoch 9 batch 109\nepoch 9 batch 110\nepoch 9 batch 111\nepoch 9 batch 112\nepoch 9 batch 113\nepoch 9 batch 114\nepoch 9 batch 115\nepoch 9 batch 116\nepoch 9 batch 117\nepoch 9 batch 118\nepoch 9 batch 119\nepoch 9 batch 120\nepoch 9 batch 121\nepoch 9 batch 122\nepoch 9 batch 123\nepoch 9 batch 124\nepoch 9 batch 125\nepoch 9 batch 126\nepoch 9 batch 127\nepoch 9 batch 128\nepoch 9 batch 129\nepoch 9 batch 130\nepoch 9 batch 131\nepoch 9 batch 132\nepoch 9 batch 133\nepoch 9 batch 134\nepoch 9 batch 135\nepoch 9 batch 136\nepoch 9 batch 137\nepoch 9 batch 138\nepoch 9 batch 139\nepoch 9 batch 140\nepoch 9 batch 141\nepoch 9 batch 142\nepoch 9 batch 143\nepoch 9 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 9, Training Loss: 4.1581, Validation Loss: 4.5394, lr: 0.00009000\nSaved model\nepoch 10 batch 0\nepoch 10 batch 1\nepoch 10 batch 2\nepoch 10 batch 3\nepoch 10 batch 4\nepoch 10 batch 5\nepoch 10 batch 6\nepoch 10 batch 7\nepoch 10 batch 8\nepoch 10 batch 9\nepoch 10 batch 10\nepoch 10 batch 11\nepoch 10 batch 12\nepoch 10 batch 13\nepoch 10 batch 14\nepoch 10 batch 15\nepoch 10 batch 16\nepoch 10 batch 17\nepoch 10 batch 18\nepoch 10 batch 19\nepoch 10 batch 20\nepoch 10 batch 21\nepoch 10 batch 22\nepoch 10 batch 23\nepoch 10 batch 24\nepoch 10 batch 25\nepoch 10 batch 26\nepoch 10 batch 27\nepoch 10 batch 28\nepoch 10 batch 29\nepoch 10 batch 30\nepoch 10 batch 31\nepoch 10 batch 32\nepoch 10 batch 33\nepoch 10 batch 34\nepoch 10 batch 35\nepoch 10 batch 36\nepoch 10 batch 37\nepoch 10 batch 38\nepoch 10 batch 39\nepoch 10 batch 40\nepoch 10 batch 41\nepoch 10 batch 42\nepoch 10 batch 43\nepoch 10 batch 44\nepoch 10 batch 45\nepoch 10 batch 46\nepoch 10 batch 47\nepoch 10 batch 48\nepoch 10 batch 49\nepoch 10 batch 50\nepoch 10 batch 51\nepoch 10 batch 52\nepoch 10 batch 53\nepoch 10 batch 54\nepoch 10 batch 55\nepoch 10 batch 56\nepoch 10 batch 57\nepoch 10 batch 58\nepoch 10 batch 59\nepoch 10 batch 60\nepoch 10 batch 61\nepoch 10 batch 62\nepoch 10 batch 63\nepoch 10 batch 64\nepoch 10 batch 65\nepoch 10 batch 66\nepoch 10 batch 67\nepoch 10 batch 68\nepoch 10 batch 69\nepoch 10 batch 70\nepoch 10 batch 71\nepoch 10 batch 72\nepoch 10 batch 73\nepoch 10 batch 74\nepoch 10 batch 75\nepoch 10 batch 76\nepoch 10 batch 77\nepoch 10 batch 78\nepoch 10 batch 79\nepoch 10 batch 80\nepoch 10 batch 81\nepoch 10 batch 82\nepoch 10 batch 83\nepoch 10 batch 84\nepoch 10 batch 85\nepoch 10 batch 86\nepoch 10 batch 87\nepoch 10 batch 88\nepoch 10 batch 89\nepoch 10 batch 90\nepoch 10 batch 91\nepoch 10 batch 92\nepoch 10 batch 93\nepoch 10 batch 94\nepoch 10 batch 95\nepoch 10 batch 96\nepoch 10 batch 97\nepoch 10 batch 98\nepoch 10 batch 99\nepoch 10 batch 100\nepoch 10 batch 101\nepoch 10 batch 102\nepoch 10 batch 103\nepoch 10 batch 104\nepoch 10 batch 105\nepoch 10 batch 106\nepoch 10 batch 107\nepoch 10 batch 108\nepoch 10 batch 109\nepoch 10 batch 110\nepoch 10 batch 111\nepoch 10 batch 112\nepoch 10 batch 113\nepoch 10 batch 114\nepoch 10 batch 115\nepoch 10 batch 116\nepoch 10 batch 117\nepoch 10 batch 118\nepoch 10 batch 119\nepoch 10 batch 120\nepoch 10 batch 121\nepoch 10 batch 122\nepoch 10 batch 123\nepoch 10 batch 124\nepoch 10 batch 125\nepoch 10 batch 126\nepoch 10 batch 127\nepoch 10 batch 128\nepoch 10 batch 129\nepoch 10 batch 130\nepoch 10 batch 131\nepoch 10 batch 132\nepoch 10 batch 133\nepoch 10 batch 134\nepoch 10 batch 135\nepoch 10 batch 136\nepoch 10 batch 137\nepoch 10 batch 138\nepoch 10 batch 139\nepoch 10 batch 140\nepoch 10 batch 141\nepoch 10 batch 142\nepoch 10 batch 143\nepoch 10 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 10, Training Loss: 4.1313, Validation Loss: 4.5980, lr: 0.00008100\nSaved model\nepoch 11 batch 0\nepoch 11 batch 1\nepoch 11 batch 2\nepoch 11 batch 3\nepoch 11 batch 4\nepoch 11 batch 5\nepoch 11 batch 6\nepoch 11 batch 7\nepoch 11 batch 8\nepoch 11 batch 9\nepoch 11 batch 10\nepoch 11 batch 11\nepoch 11 batch 12\nepoch 11 batch 13\nepoch 11 batch 14\nepoch 11 batch 15\nepoch 11 batch 16\nepoch 11 batch 17\nepoch 11 batch 18\nepoch 11 batch 19\nepoch 11 batch 20\nepoch 11 batch 21\nepoch 11 batch 22\nepoch 11 batch 23\nepoch 11 batch 24\nepoch 11 batch 25\nepoch 11 batch 26\nepoch 11 batch 27\nepoch 11 batch 28\nepoch 11 batch 29\nepoch 11 batch 30\nepoch 11 batch 31\nepoch 11 batch 32\nepoch 11 batch 33\nepoch 11 batch 34\nepoch 11 batch 35\nepoch 11 batch 36\nepoch 11 batch 37\nepoch 11 batch 38\nepoch 11 batch 39\nepoch 11 batch 40\nepoch 11 batch 41\nepoch 11 batch 42\nepoch 11 batch 43\nepoch 11 batch 44\nepoch 11 batch 45\nepoch 11 batch 46\nepoch 11 batch 47\nepoch 11 batch 48\nepoch 11 batch 49\nepoch 11 batch 50\nepoch 11 batch 51\nepoch 11 batch 52\nepoch 11 batch 53\nepoch 11 batch 54\nepoch 11 batch 55\nepoch 11 batch 56\nepoch 11 batch 57\nepoch 11 batch 58\nepoch 11 batch 59\nepoch 11 batch 60\nepoch 11 batch 61\nepoch 11 batch 62\nepoch 11 batch 63\nepoch 11 batch 64\nepoch 11 batch 65\nepoch 11 batch 66\nepoch 11 batch 67\nepoch 11 batch 68\nepoch 11 batch 69\nepoch 11 batch 70\nepoch 11 batch 71\nepoch 11 batch 72\nepoch 11 batch 73\nepoch 11 batch 74\nepoch 11 batch 75\nepoch 11 batch 76\nepoch 11 batch 77\nepoch 11 batch 78\nepoch 11 batch 79\nepoch 11 batch 80\nepoch 11 batch 81\nepoch 11 batch 82\nepoch 11 batch 83\nepoch 11 batch 84\nepoch 11 batch 85\nepoch 11 batch 86\nepoch 11 batch 87\nepoch 11 batch 88\nepoch 11 batch 89\nepoch 11 batch 90\nepoch 11 batch 91\nepoch 11 batch 92\nepoch 11 batch 93\nepoch 11 batch 94\nepoch 11 batch 95\nepoch 11 batch 96\nepoch 11 batch 97\nepoch 11 batch 98\nepoch 11 batch 99\nepoch 11 batch 100\nepoch 11 batch 101\nepoch 11 batch 102\nepoch 11 batch 103\nepoch 11 batch 104\nepoch 11 batch 105\nepoch 11 batch 106\nepoch 11 batch 107\nepoch 11 batch 108\nepoch 11 batch 109\nepoch 11 batch 110\nepoch 11 batch 111\nepoch 11 batch 112\nepoch 11 batch 113\nepoch 11 batch 114\nepoch 11 batch 115\nepoch 11 batch 116\nepoch 11 batch 117\nepoch 11 batch 118\nepoch 11 batch 119\nepoch 11 batch 120\nepoch 11 batch 121\nepoch 11 batch 122\nepoch 11 batch 123\nepoch 11 batch 124\nepoch 11 batch 125\nepoch 11 batch 126\nepoch 11 batch 127\nepoch 11 batch 128\nepoch 11 batch 129\nepoch 11 batch 130\nepoch 11 batch 131\nepoch 11 batch 132\nepoch 11 batch 133\nepoch 11 batch 134\nepoch 11 batch 135\nepoch 11 batch 136\nepoch 11 batch 137\nepoch 11 batch 138\nepoch 11 batch 139\nepoch 11 batch 140\nepoch 11 batch 141\nepoch 11 batch 142\nepoch 11 batch 143\nepoch 11 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 11, Training Loss: 4.1628, Validation Loss: 4.5524, lr: 0.00008100\nSaved model\nepoch 12 batch 0\nepoch 12 batch 1\nepoch 12 batch 2\nepoch 12 batch 3\nepoch 12 batch 4\nepoch 12 batch 5\nepoch 12 batch 6\nepoch 12 batch 7\nepoch 12 batch 8\nepoch 12 batch 9\nepoch 12 batch 10\nepoch 12 batch 11\nepoch 12 batch 12\nepoch 12 batch 13\nepoch 12 batch 14\nepoch 12 batch 15\nepoch 12 batch 16\nepoch 12 batch 17\nepoch 12 batch 18\nepoch 12 batch 19\nepoch 12 batch 20\nepoch 12 batch 21\nepoch 12 batch 22\nepoch 12 batch 23\nepoch 12 batch 24\nepoch 12 batch 25\nepoch 12 batch 26\nepoch 12 batch 27\nepoch 12 batch 28\nepoch 12 batch 29\nepoch 12 batch 30\nepoch 12 batch 31\nepoch 12 batch 32\nepoch 12 batch 33\nepoch 12 batch 34\nepoch 12 batch 35\nepoch 12 batch 36\nepoch 12 batch 37\nepoch 12 batch 38\nepoch 12 batch 39\nepoch 12 batch 40\nepoch 12 batch 41\nepoch 12 batch 42\nepoch 12 batch 43\nepoch 12 batch 44\nepoch 12 batch 45\nepoch 12 batch 46\nepoch 12 batch 47\nepoch 12 batch 48\nepoch 12 batch 49\nepoch 12 batch 50\nepoch 12 batch 51\nepoch 12 batch 52\nepoch 12 batch 53\nepoch 12 batch 54\nepoch 12 batch 55\nepoch 12 batch 56\nepoch 12 batch 57\nepoch 12 batch 58\nepoch 12 batch 59\nepoch 12 batch 60\nepoch 12 batch 61\nepoch 12 batch 62\nepoch 12 batch 63\nepoch 12 batch 64\nepoch 12 batch 65\nepoch 12 batch 66\nepoch 12 batch 67\nepoch 12 batch 68\nepoch 12 batch 69\nepoch 12 batch 70\nepoch 12 batch 71\nepoch 12 batch 72\nepoch 12 batch 73\nepoch 12 batch 74\nepoch 12 batch 75\nepoch 12 batch 76\nepoch 12 batch 77\nepoch 12 batch 78\nepoch 12 batch 79\nepoch 12 batch 80\nepoch 12 batch 81\nepoch 12 batch 82\nepoch 12 batch 83\nepoch 12 batch 84\nepoch 12 batch 85\nepoch 12 batch 86\nepoch 12 batch 87\nepoch 12 batch 88\nepoch 12 batch 89\nepoch 12 batch 90\nepoch 12 batch 91\nepoch 12 batch 92\nepoch 12 batch 93\nepoch 12 batch 94\nepoch 12 batch 95\nepoch 12 batch 96\nepoch 12 batch 97\nepoch 12 batch 98\nepoch 12 batch 99\nepoch 12 batch 100\nepoch 12 batch 101\nepoch 12 batch 102\nepoch 12 batch 103\nepoch 12 batch 104\nepoch 12 batch 105\nepoch 12 batch 106\nepoch 12 batch 107\nepoch 12 batch 108\nepoch 12 batch 109\nepoch 12 batch 110\nepoch 12 batch 111\nepoch 12 batch 112\nepoch 12 batch 113\nepoch 12 batch 114\nepoch 12 batch 115\nepoch 12 batch 116\nepoch 12 batch 117\nepoch 12 batch 118\nepoch 12 batch 119\nepoch 12 batch 120\nepoch 12 batch 121\nepoch 12 batch 122\nepoch 12 batch 123\nepoch 12 batch 124\nepoch 12 batch 125\nepoch 12 batch 126\nepoch 12 batch 127\nepoch 12 batch 128\nepoch 12 batch 129\nepoch 12 batch 130\nepoch 12 batch 131\nepoch 12 batch 132\nepoch 12 batch 133\nepoch 12 batch 134\nepoch 12 batch 135\nepoch 12 batch 136\nepoch 12 batch 137\nepoch 12 batch 138\nepoch 12 batch 139\nepoch 12 batch 140\nepoch 12 batch 141\nepoch 12 batch 142\nepoch 12 batch 143\nepoch 12 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 12, Training Loss: 4.1298, Validation Loss: 4.5950, lr: 0.00008100\nSaved model\nepoch 13 batch 0\nepoch 13 batch 1\nepoch 13 batch 2\nepoch 13 batch 3\nepoch 13 batch 4\nepoch 13 batch 5\nepoch 13 batch 6\nepoch 13 batch 7\nepoch 13 batch 8\nepoch 13 batch 9\nepoch 13 batch 10\nepoch 13 batch 11\nepoch 13 batch 12\nepoch 13 batch 13\nepoch 13 batch 14\nepoch 13 batch 15\nepoch 13 batch 16\nepoch 13 batch 17\nepoch 13 batch 18\nepoch 13 batch 19\nepoch 13 batch 20\nepoch 13 batch 21\nepoch 13 batch 22\nepoch 13 batch 23\nepoch 13 batch 24\nepoch 13 batch 25\nepoch 13 batch 26\nepoch 13 batch 27\nepoch 13 batch 28\nepoch 13 batch 29\nepoch 13 batch 30\nepoch 13 batch 31\nepoch 13 batch 32\nepoch 13 batch 33\nepoch 13 batch 34\nepoch 13 batch 35\nepoch 13 batch 36\nepoch 13 batch 37\nepoch 13 batch 38\nepoch 13 batch 39\nepoch 13 batch 40\nepoch 13 batch 41\nepoch 13 batch 42\nepoch 13 batch 43\nepoch 13 batch 44\nepoch 13 batch 45\nepoch 13 batch 46\nepoch 13 batch 47\nepoch 13 batch 48\nepoch 13 batch 49\nepoch 13 batch 50\nepoch 13 batch 51\nepoch 13 batch 52\nepoch 13 batch 53\nepoch 13 batch 54\nepoch 13 batch 55\nepoch 13 batch 56\nepoch 13 batch 57\nepoch 13 batch 58\nepoch 13 batch 59\nepoch 13 batch 60\nepoch 13 batch 61\nepoch 13 batch 62\nepoch 13 batch 63\nepoch 13 batch 64\nepoch 13 batch 65\nepoch 13 batch 66\nepoch 13 batch 67\nepoch 13 batch 68\nepoch 13 batch 69\nepoch 13 batch 70\nepoch 13 batch 71\nepoch 13 batch 72\nepoch 13 batch 73\nepoch 13 batch 74\nepoch 13 batch 75\nepoch 13 batch 76\nepoch 13 batch 77\nepoch 13 batch 78\nepoch 13 batch 79\nepoch 13 batch 80\nepoch 13 batch 81\nepoch 13 batch 82\nepoch 13 batch 83\nepoch 13 batch 84\nepoch 13 batch 85\nepoch 13 batch 86\nepoch 13 batch 87\nepoch 13 batch 88\nepoch 13 batch 89\nepoch 13 batch 90\nepoch 13 batch 91\nepoch 13 batch 92\nepoch 13 batch 93\nepoch 13 batch 94\nepoch 13 batch 95\nepoch 13 batch 96\nepoch 13 batch 97\nepoch 13 batch 98\nepoch 13 batch 99\nepoch 13 batch 100\nepoch 13 batch 101\nepoch 13 batch 102\nepoch 13 batch 103\nepoch 13 batch 104\nepoch 13 batch 105\nepoch 13 batch 106\nepoch 13 batch 107\nepoch 13 batch 108\nepoch 13 batch 109\nepoch 13 batch 110\nepoch 13 batch 111\nepoch 13 batch 112\nepoch 13 batch 113\nepoch 13 batch 114\nepoch 13 batch 115\nepoch 13 batch 116\nepoch 13 batch 117\nepoch 13 batch 118\nepoch 13 batch 119\nepoch 13 batch 120\nepoch 13 batch 121\nepoch 13 batch 122\nepoch 13 batch 123\nepoch 13 batch 124\nepoch 13 batch 125\nepoch 13 batch 126\nepoch 13 batch 127\nepoch 13 batch 128\nepoch 13 batch 129\nepoch 13 batch 130\nepoch 13 batch 131\nepoch 13 batch 132\nepoch 13 batch 133\nepoch 13 batch 134\nepoch 13 batch 135\nepoch 13 batch 136\nepoch 13 batch 137\nepoch 13 batch 138\nepoch 13 batch 139\nepoch 13 batch 140\nepoch 13 batch 141\nepoch 13 batch 142\nepoch 13 batch 143\nepoch 13 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 13, Training Loss: 4.1620, Validation Loss: 4.5713, lr: 0.00008100\nSaved model\nepoch 14 batch 0\nepoch 14 batch 1\nepoch 14 batch 2\nepoch 14 batch 3\nepoch 14 batch 4\nepoch 14 batch 5\nepoch 14 batch 6\nepoch 14 batch 7\nepoch 14 batch 8\nepoch 14 batch 9\nepoch 14 batch 10\nepoch 14 batch 11\nepoch 14 batch 12\nepoch 14 batch 13\nepoch 14 batch 14\nepoch 14 batch 15\nepoch 14 batch 16\nepoch 14 batch 17\nepoch 14 batch 18\nepoch 14 batch 19\nepoch 14 batch 20\nepoch 14 batch 21\nepoch 14 batch 22\nepoch 14 batch 23\nepoch 14 batch 24\nepoch 14 batch 25\nepoch 14 batch 26\nepoch 14 batch 27\nepoch 14 batch 28\nepoch 14 batch 29\nepoch 14 batch 30\nepoch 14 batch 31\nepoch 14 batch 32\nepoch 14 batch 33\nepoch 14 batch 34\nepoch 14 batch 35\nepoch 14 batch 36\nepoch 14 batch 37\nepoch 14 batch 38\nepoch 14 batch 39\nepoch 14 batch 40\nepoch 14 batch 41\nepoch 14 batch 42\nepoch 14 batch 43\nepoch 14 batch 44\nepoch 14 batch 45\nepoch 14 batch 46\nepoch 14 batch 47\nepoch 14 batch 48\nepoch 14 batch 49\nepoch 14 batch 50\nepoch 14 batch 51\nepoch 14 batch 52\nepoch 14 batch 53\nepoch 14 batch 54\nepoch 14 batch 55\nepoch 14 batch 56\nepoch 14 batch 57\nepoch 14 batch 58\nepoch 14 batch 59\nepoch 14 batch 60\nepoch 14 batch 61\nepoch 14 batch 62\nepoch 14 batch 63\nepoch 14 batch 64\nepoch 14 batch 65\nepoch 14 batch 66\nepoch 14 batch 67\nepoch 14 batch 68\nepoch 14 batch 69\nepoch 14 batch 70\nepoch 14 batch 71\nepoch 14 batch 72\nepoch 14 batch 73\nepoch 14 batch 74\nepoch 14 batch 75\nepoch 14 batch 76\nepoch 14 batch 77\nepoch 14 batch 78\nepoch 14 batch 79\nepoch 14 batch 80\nepoch 14 batch 81\nepoch 14 batch 82\nepoch 14 batch 83\nepoch 14 batch 84\nepoch 14 batch 85\nepoch 14 batch 86\nepoch 14 batch 87\nepoch 14 batch 88\nepoch 14 batch 89\nepoch 14 batch 90\nepoch 14 batch 91\nepoch 14 batch 92\nepoch 14 batch 93\nepoch 14 batch 94\nepoch 14 batch 95\nepoch 14 batch 96\nepoch 14 batch 97\nepoch 14 batch 98\nepoch 14 batch 99\nepoch 14 batch 100\nepoch 14 batch 101\nepoch 14 batch 102\nepoch 14 batch 103\nepoch 14 batch 104\nepoch 14 batch 105\nepoch 14 batch 106\nepoch 14 batch 107\nepoch 14 batch 108\nepoch 14 batch 109\nepoch 14 batch 110\nepoch 14 batch 111\nepoch 14 batch 112\nepoch 14 batch 113\nepoch 14 batch 114\nepoch 14 batch 115\nepoch 14 batch 116\nepoch 14 batch 117\nepoch 14 batch 118\nepoch 14 batch 119\nepoch 14 batch 120\nepoch 14 batch 121\nepoch 14 batch 122\nepoch 14 batch 123\nepoch 14 batch 124\nepoch 14 batch 125\nepoch 14 batch 126\nepoch 14 batch 127\nepoch 14 batch 128\nepoch 14 batch 129\nepoch 14 batch 130\nepoch 14 batch 131\nepoch 14 batch 132\nepoch 14 batch 133\nepoch 14 batch 134\nepoch 14 batch 135\nepoch 14 batch 136\nepoch 14 batch 137\nepoch 14 batch 138\nepoch 14 batch 139\nepoch 14 batch 140\nepoch 14 batch 141\nepoch 14 batch 142\nepoch 14 batch 143\nepoch 14 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 14, Training Loss: 4.1561, Validation Loss: 4.5928, lr: 0.00008100\nSaved model\nepoch 15 batch 0\nepoch 15 batch 1\nepoch 15 batch 2\nepoch 15 batch 3\nepoch 15 batch 4\nepoch 15 batch 5\nepoch 15 batch 6\nepoch 15 batch 7\nepoch 15 batch 8\nepoch 15 batch 9\nepoch 15 batch 10\nepoch 15 batch 11\nepoch 15 batch 12\nepoch 15 batch 13\nepoch 15 batch 14\nepoch 15 batch 15\nepoch 15 batch 16\nepoch 15 batch 17\nepoch 15 batch 18\nepoch 15 batch 19\nepoch 15 batch 20\nepoch 15 batch 21\nepoch 15 batch 22\nepoch 15 batch 23\nepoch 15 batch 24\nepoch 15 batch 25\nepoch 15 batch 26\nepoch 15 batch 27\nepoch 15 batch 28\nepoch 15 batch 29\nepoch 15 batch 30\nepoch 15 batch 31\nepoch 15 batch 32\nepoch 15 batch 33\nepoch 15 batch 34\nepoch 15 batch 35\nepoch 15 batch 36\nepoch 15 batch 37\nepoch 15 batch 38\nepoch 15 batch 39\nepoch 15 batch 40\nepoch 15 batch 41\nepoch 15 batch 42\nepoch 15 batch 43\nepoch 15 batch 44\nepoch 15 batch 45\nepoch 15 batch 46\nepoch 15 batch 47\nepoch 15 batch 48\nepoch 15 batch 49\nepoch 15 batch 50\nepoch 15 batch 51\nepoch 15 batch 52\nepoch 15 batch 53\nepoch 15 batch 54\nepoch 15 batch 55\nepoch 15 batch 56\nepoch 15 batch 57\nepoch 15 batch 58\nepoch 15 batch 59\nepoch 15 batch 60\nepoch 15 batch 61\nepoch 15 batch 62\nepoch 15 batch 63\nepoch 15 batch 64\nepoch 15 batch 65\nepoch 15 batch 66\nepoch 15 batch 67\nepoch 15 batch 68\nepoch 15 batch 69\nepoch 15 batch 70\nepoch 15 batch 71\nepoch 15 batch 72\nepoch 15 batch 73\nepoch 15 batch 74\nepoch 15 batch 75\nepoch 15 batch 76\nepoch 15 batch 77\nepoch 15 batch 78\nepoch 15 batch 79\nepoch 15 batch 80\nepoch 15 batch 81\nepoch 15 batch 82\nepoch 15 batch 83\nepoch 15 batch 84\nepoch 15 batch 85\nepoch 15 batch 86\nepoch 15 batch 87\nepoch 15 batch 88\nepoch 15 batch 89\nepoch 15 batch 90\nepoch 15 batch 91\nepoch 15 batch 92\nepoch 15 batch 93\nepoch 15 batch 94\nepoch 15 batch 95\nepoch 15 batch 96\nepoch 15 batch 97\nepoch 15 batch 98\nepoch 15 batch 99\nepoch 15 batch 100\nepoch 15 batch 101\nepoch 15 batch 102\nepoch 15 batch 103\nepoch 15 batch 104\nepoch 15 batch 105\nepoch 15 batch 106\nepoch 15 batch 107\nepoch 15 batch 108\nepoch 15 batch 109\nepoch 15 batch 110\nepoch 15 batch 111\nepoch 15 batch 112\nepoch 15 batch 113\nepoch 15 batch 114\nepoch 15 batch 115\nepoch 15 batch 116\nepoch 15 batch 117\nepoch 15 batch 118\nepoch 15 batch 119\nepoch 15 batch 120\nepoch 15 batch 121\nepoch 15 batch 122\nepoch 15 batch 123\nepoch 15 batch 124\nepoch 15 batch 125\nepoch 15 batch 126\nepoch 15 batch 127\nepoch 15 batch 128\nepoch 15 batch 129\nepoch 15 batch 130\nepoch 15 batch 131\nepoch 15 batch 132\nepoch 15 batch 133\nepoch 15 batch 134\nepoch 15 batch 135\nepoch 15 batch 136\nepoch 15 batch 137\nepoch 15 batch 138\nepoch 15 batch 139\nepoch 15 batch 140\nepoch 15 batch 141\nepoch 15 batch 142\nepoch 15 batch 143\nepoch 15 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 15, Training Loss: 4.1473, Validation Loss: 4.5542, lr: 0.00007290\nSaved model\nepoch 16 batch 0\nepoch 16 batch 1\nepoch 16 batch 2\nepoch 16 batch 3\nepoch 16 batch 4\nepoch 16 batch 5\nepoch 16 batch 6\nepoch 16 batch 7\nepoch 16 batch 8\nepoch 16 batch 9\nepoch 16 batch 10\nepoch 16 batch 11\nepoch 16 batch 12\nepoch 16 batch 13\nepoch 16 batch 14\nepoch 16 batch 15\nepoch 16 batch 16\nepoch 16 batch 17\nepoch 16 batch 18\nepoch 16 batch 19\nepoch 16 batch 20\nepoch 16 batch 21\nepoch 16 batch 22\nepoch 16 batch 23\nepoch 16 batch 24\nepoch 16 batch 25\nepoch 16 batch 26\nepoch 16 batch 27\nepoch 16 batch 28\nepoch 16 batch 29\nepoch 16 batch 30\nepoch 16 batch 31\nepoch 16 batch 32\nepoch 16 batch 33\nepoch 16 batch 34\nepoch 16 batch 35\nepoch 16 batch 36\nepoch 16 batch 37\nepoch 16 batch 38\nepoch 16 batch 39\nepoch 16 batch 40\nepoch 16 batch 41\nepoch 16 batch 42\nepoch 16 batch 43\nepoch 16 batch 44\nepoch 16 batch 45\nepoch 16 batch 46\nepoch 16 batch 47\nepoch 16 batch 48\nepoch 16 batch 49\nepoch 16 batch 50\nepoch 16 batch 51\nepoch 16 batch 52\nepoch 16 batch 53\nepoch 16 batch 54\nepoch 16 batch 55\nepoch 16 batch 56\nepoch 16 batch 57\nepoch 16 batch 58\nepoch 16 batch 59\nepoch 16 batch 60\nepoch 16 batch 61\nepoch 16 batch 62\nepoch 16 batch 63\nepoch 16 batch 64\nepoch 16 batch 65\nepoch 16 batch 66\nepoch 16 batch 67\nepoch 16 batch 68\nepoch 16 batch 69\nepoch 16 batch 70\nepoch 16 batch 71\nepoch 16 batch 72\nepoch 16 batch 73\nepoch 16 batch 74\nepoch 16 batch 75\nepoch 16 batch 76\nepoch 16 batch 77\nepoch 16 batch 78\nepoch 16 batch 79\nepoch 16 batch 80\nepoch 16 batch 81\nepoch 16 batch 82\nepoch 16 batch 83\nepoch 16 batch 84\nepoch 16 batch 85\nepoch 16 batch 86\nepoch 16 batch 87\nepoch 16 batch 88\nepoch 16 batch 89\nepoch 16 batch 90\nepoch 16 batch 91\nepoch 16 batch 92\nepoch 16 batch 93\nepoch 16 batch 94\nepoch 16 batch 95\nepoch 16 batch 96\nepoch 16 batch 97\nepoch 16 batch 98\nepoch 16 batch 99\nepoch 16 batch 100\nepoch 16 batch 101\nepoch 16 batch 102\nepoch 16 batch 103\nepoch 16 batch 104\nepoch 16 batch 105\nepoch 16 batch 106\nepoch 16 batch 107\nepoch 16 batch 108\nepoch 16 batch 109\nepoch 16 batch 110\nepoch 16 batch 111\nepoch 16 batch 112\nepoch 16 batch 113\nepoch 16 batch 114\nepoch 16 batch 115\nepoch 16 batch 116\nepoch 16 batch 117\nepoch 16 batch 118\nepoch 16 batch 119\nepoch 16 batch 120\nepoch 16 batch 121\nepoch 16 batch 122\nepoch 16 batch 123\nepoch 16 batch 124\nepoch 16 batch 125\nepoch 16 batch 126\nepoch 16 batch 127\nepoch 16 batch 128\nepoch 16 batch 129\nepoch 16 batch 130\nepoch 16 batch 131\nepoch 16 batch 132\nepoch 16 batch 133\nepoch 16 batch 134\nepoch 16 batch 135\nepoch 16 batch 136\nepoch 16 batch 137\nepoch 16 batch 138\nepoch 16 batch 139\nepoch 16 batch 140\nepoch 16 batch 141\nepoch 16 batch 142\nepoch 16 batch 143\nepoch 16 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 16, Training Loss: 4.1461, Validation Loss: 4.5514, lr: 0.00007290\nSaved model\nepoch 17 batch 0\nepoch 17 batch 1\nepoch 17 batch 2\nepoch 17 batch 3\nepoch 17 batch 4\nepoch 17 batch 5\nepoch 17 batch 6\nepoch 17 batch 7\nepoch 17 batch 8\nepoch 17 batch 9\nepoch 17 batch 10\nepoch 17 batch 11\nepoch 17 batch 12\nepoch 17 batch 13\nepoch 17 batch 14\nepoch 17 batch 15\nepoch 17 batch 16\nepoch 17 batch 17\nepoch 17 batch 18\nepoch 17 batch 19\nepoch 17 batch 20\nepoch 17 batch 21\nepoch 17 batch 22\nepoch 17 batch 23\nepoch 17 batch 24\nepoch 17 batch 25\nepoch 17 batch 26\nepoch 17 batch 27\nepoch 17 batch 28\nepoch 17 batch 29\nepoch 17 batch 30\nepoch 17 batch 31\nepoch 17 batch 32\nepoch 17 batch 33\nepoch 17 batch 34\nepoch 17 batch 35\nepoch 17 batch 36\nepoch 17 batch 37\nepoch 17 batch 38\nepoch 17 batch 39\nepoch 17 batch 40\nepoch 17 batch 41\nepoch 17 batch 42\nepoch 17 batch 43\nepoch 17 batch 44\nepoch 17 batch 45\nepoch 17 batch 46\nepoch 17 batch 47\nepoch 17 batch 48\nepoch 17 batch 49\nepoch 17 batch 50\nepoch 17 batch 51\nepoch 17 batch 52\nepoch 17 batch 53\nepoch 17 batch 54\nepoch 17 batch 55\nepoch 17 batch 56\nepoch 17 batch 57\nepoch 17 batch 58\nepoch 17 batch 59\nepoch 17 batch 60\nepoch 17 batch 61\nepoch 17 batch 62\nepoch 17 batch 63\nepoch 17 batch 64\nepoch 17 batch 65\nepoch 17 batch 66\nepoch 17 batch 67\nepoch 17 batch 68\nepoch 17 batch 69\nepoch 17 batch 70\nepoch 17 batch 71\nepoch 17 batch 72\nepoch 17 batch 73\nepoch 17 batch 74\nepoch 17 batch 75\nepoch 17 batch 76\nepoch 17 batch 77\nepoch 17 batch 78\nepoch 17 batch 79\nepoch 17 batch 80\nepoch 17 batch 81\nepoch 17 batch 82\nepoch 17 batch 83\nepoch 17 batch 84\nepoch 17 batch 85\nepoch 17 batch 86\nepoch 17 batch 87\nepoch 17 batch 88\nepoch 17 batch 89\nepoch 17 batch 90\nepoch 17 batch 91\nepoch 17 batch 92\nepoch 17 batch 93\nepoch 17 batch 94\nepoch 17 batch 95\nepoch 17 batch 96\nepoch 17 batch 97\nepoch 17 batch 98\nepoch 17 batch 99\nepoch 17 batch 100\nepoch 17 batch 101\nepoch 17 batch 102\nepoch 17 batch 103\nepoch 17 batch 104\nepoch 17 batch 105\nepoch 17 batch 106\nepoch 17 batch 107\nepoch 17 batch 108\nepoch 17 batch 109\nepoch 17 batch 110\nepoch 17 batch 111\nepoch 17 batch 112\nepoch 17 batch 113\nepoch 17 batch 114\nepoch 17 batch 115\nepoch 17 batch 116\nepoch 17 batch 117\nepoch 17 batch 118\nepoch 17 batch 119\nepoch 17 batch 120\nepoch 17 batch 121\nepoch 17 batch 122\nepoch 17 batch 123\nepoch 17 batch 124\nepoch 17 batch 125\nepoch 17 batch 126\nepoch 17 batch 127\nepoch 17 batch 128\nepoch 17 batch 129\nepoch 17 batch 130\nepoch 17 batch 131\nepoch 17 batch 132\nepoch 17 batch 133\nepoch 17 batch 134\nepoch 17 batch 135\nepoch 17 batch 136\nepoch 17 batch 137\nepoch 17 batch 138\nepoch 17 batch 139\nepoch 17 batch 140\nepoch 17 batch 141\nepoch 17 batch 142\nepoch 17 batch 143\nepoch 17 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 17, Training Loss: 4.1459, Validation Loss: 4.5490, lr: 0.00007290\nSaved model\nepoch 18 batch 0\nepoch 18 batch 1\nepoch 18 batch 2\nepoch 18 batch 3\nepoch 18 batch 4\nepoch 18 batch 5\nepoch 18 batch 6\nepoch 18 batch 7\nepoch 18 batch 8\nepoch 18 batch 9\nepoch 18 batch 10\nepoch 18 batch 11\nepoch 18 batch 12\nepoch 18 batch 13\nepoch 18 batch 14\nepoch 18 batch 15\nepoch 18 batch 16\nepoch 18 batch 17\nepoch 18 batch 18\nepoch 18 batch 19\nepoch 18 batch 20\nepoch 18 batch 21\nepoch 18 batch 22\nepoch 18 batch 23\nepoch 18 batch 24\nepoch 18 batch 25\nepoch 18 batch 26\nepoch 18 batch 27\nepoch 18 batch 28\nepoch 18 batch 29\nepoch 18 batch 30\nepoch 18 batch 31\nepoch 18 batch 32\nepoch 18 batch 33\nepoch 18 batch 34\nepoch 18 batch 35\nepoch 18 batch 36\nepoch 18 batch 37\nepoch 18 batch 38\nepoch 18 batch 39\nepoch 18 batch 40\nepoch 18 batch 41\nepoch 18 batch 42\nepoch 18 batch 43\nepoch 18 batch 44\nepoch 18 batch 45\nepoch 18 batch 46\nepoch 18 batch 47\nepoch 18 batch 48\nepoch 18 batch 49\nepoch 18 batch 50\nepoch 18 batch 51\nepoch 18 batch 52\nepoch 18 batch 53\nepoch 18 batch 54\nepoch 18 batch 55\nepoch 18 batch 56\nepoch 18 batch 57\nepoch 18 batch 58\nepoch 18 batch 59\nepoch 18 batch 60\nepoch 18 batch 61\nepoch 18 batch 62\nepoch 18 batch 63\nepoch 18 batch 64\nepoch 18 batch 65\nepoch 18 batch 66\nepoch 18 batch 67\nepoch 18 batch 68\nepoch 18 batch 69\nepoch 18 batch 70\nepoch 18 batch 71\nepoch 18 batch 72\nepoch 18 batch 73\nepoch 18 batch 74\nepoch 18 batch 75\nepoch 18 batch 76\nepoch 18 batch 77\nepoch 18 batch 78\nepoch 18 batch 79\nepoch 18 batch 80\nepoch 18 batch 81\nepoch 18 batch 82\nepoch 18 batch 83\nepoch 18 batch 84\nepoch 18 batch 85\nepoch 18 batch 86\nepoch 18 batch 87\nepoch 18 batch 88\nepoch 18 batch 89\nepoch 18 batch 90\nepoch 18 batch 91\nepoch 18 batch 92\nepoch 18 batch 93\nepoch 18 batch 94\nepoch 18 batch 95\nepoch 18 batch 96\nepoch 18 batch 97\nepoch 18 batch 98\nepoch 18 batch 99\nepoch 18 batch 100\nepoch 18 batch 101\nepoch 18 batch 102\nepoch 18 batch 103\nepoch 18 batch 104\nepoch 18 batch 105\nepoch 18 batch 106\nepoch 18 batch 107\nepoch 18 batch 108\nepoch 18 batch 109\nepoch 18 batch 110\nepoch 18 batch 111\nepoch 18 batch 112\nepoch 18 batch 113\nepoch 18 batch 114\nepoch 18 batch 115\nepoch 18 batch 116\nepoch 18 batch 117\nepoch 18 batch 118\nepoch 18 batch 119\nepoch 18 batch 120\nepoch 18 batch 121\nepoch 18 batch 122\nepoch 18 batch 123\nepoch 18 batch 124\nepoch 18 batch 125\nepoch 18 batch 126\nepoch 18 batch 127\nepoch 18 batch 128\nepoch 18 batch 129\nepoch 18 batch 130\nepoch 18 batch 131\nepoch 18 batch 132\nepoch 18 batch 133\nepoch 18 batch 134\nepoch 18 batch 135\nepoch 18 batch 136\nepoch 18 batch 137\nepoch 18 batch 138\nepoch 18 batch 139\nepoch 18 batch 140\nepoch 18 batch 141\nepoch 18 batch 142\nepoch 18 batch 143\nepoch 18 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 18, Training Loss: 4.1538, Validation Loss: 4.5400, lr: 0.00007290\nSaved model\nepoch 19 batch 0\nepoch 19 batch 1\nepoch 19 batch 2\nepoch 19 batch 3\nepoch 19 batch 4\nepoch 19 batch 5\nepoch 19 batch 6\nepoch 19 batch 7\nepoch 19 batch 8\nepoch 19 batch 9\nepoch 19 batch 10\nepoch 19 batch 11\nepoch 19 batch 12\nepoch 19 batch 13\nepoch 19 batch 14\nepoch 19 batch 15\nepoch 19 batch 16\nepoch 19 batch 17\nepoch 19 batch 18\nepoch 19 batch 19\nepoch 19 batch 20\nepoch 19 batch 21\nepoch 19 batch 22\nepoch 19 batch 23\nepoch 19 batch 24\nepoch 19 batch 25\nepoch 19 batch 26\nepoch 19 batch 27\nepoch 19 batch 28\nepoch 19 batch 29\nepoch 19 batch 30\nepoch 19 batch 31\nepoch 19 batch 32\nepoch 19 batch 33\nepoch 19 batch 34\nepoch 19 batch 35\nepoch 19 batch 36\nepoch 19 batch 37\nepoch 19 batch 38\nepoch 19 batch 39\nepoch 19 batch 40\nepoch 19 batch 41\nepoch 19 batch 42\nepoch 19 batch 43\nepoch 19 batch 44\nepoch 19 batch 45\nepoch 19 batch 46\nepoch 19 batch 47\nepoch 19 batch 48\nepoch 19 batch 49\nepoch 19 batch 50\nepoch 19 batch 51\nepoch 19 batch 52\nepoch 19 batch 53\nepoch 19 batch 54\nepoch 19 batch 55\nepoch 19 batch 56\nepoch 19 batch 57\nepoch 19 batch 58\nepoch 19 batch 59\nepoch 19 batch 60\nepoch 19 batch 61\nepoch 19 batch 62\nepoch 19 batch 63\nepoch 19 batch 64\nepoch 19 batch 65\nepoch 19 batch 66\nepoch 19 batch 67\nepoch 19 batch 68\nepoch 19 batch 69\nepoch 19 batch 70\nepoch 19 batch 71\nepoch 19 batch 72\nepoch 19 batch 73\nepoch 19 batch 74\nepoch 19 batch 75\nepoch 19 batch 76\nepoch 19 batch 77\nepoch 19 batch 78\nepoch 19 batch 79\nepoch 19 batch 80\nepoch 19 batch 81\nepoch 19 batch 82\nepoch 19 batch 83\nepoch 19 batch 84\nepoch 19 batch 85\nepoch 19 batch 86\nepoch 19 batch 87\nepoch 19 batch 88\nepoch 19 batch 89\nepoch 19 batch 90\nepoch 19 batch 91\nepoch 19 batch 92\nepoch 19 batch 93\nepoch 19 batch 94\nepoch 19 batch 95\nepoch 19 batch 96\nepoch 19 batch 97\nepoch 19 batch 98\nepoch 19 batch 99\nepoch 19 batch 100\nepoch 19 batch 101\nepoch 19 batch 102\nepoch 19 batch 103\nepoch 19 batch 104\nepoch 19 batch 105\nepoch 19 batch 106\nepoch 19 batch 107\nepoch 19 batch 108\nepoch 19 batch 109\nepoch 19 batch 110\nepoch 19 batch 111\nepoch 19 batch 112\nepoch 19 batch 113\nepoch 19 batch 114\nepoch 19 batch 115\nepoch 19 batch 116\nepoch 19 batch 117\nepoch 19 batch 118\nepoch 19 batch 119\nepoch 19 batch 120\nepoch 19 batch 121\nepoch 19 batch 122\nepoch 19 batch 123\nepoch 19 batch 124\nepoch 19 batch 125\nepoch 19 batch 126\nepoch 19 batch 127\nepoch 19 batch 128\nepoch 19 batch 129\nepoch 19 batch 130\nepoch 19 batch 131\nepoch 19 batch 132\nepoch 19 batch 133\nepoch 19 batch 134\nepoch 19 batch 135\nepoch 19 batch 136\nepoch 19 batch 137\nepoch 19 batch 138\nepoch 19 batch 139\nepoch 19 batch 140\nepoch 19 batch 141\nepoch 19 batch 142\nepoch 19 batch 143\nepoch 19 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 19, Training Loss: 4.1487, Validation Loss: 4.5370, lr: 0.00007290\nSaved model\nepoch 20 batch 0\nepoch 20 batch 1\nepoch 20 batch 2\nepoch 20 batch 3\nepoch 20 batch 4\nepoch 20 batch 5\nepoch 20 batch 6\nepoch 20 batch 7\nepoch 20 batch 8\nepoch 20 batch 9\nepoch 20 batch 10\nepoch 20 batch 11\nepoch 20 batch 12\nepoch 20 batch 13\nepoch 20 batch 14\nepoch 20 batch 15\nepoch 20 batch 16\nepoch 20 batch 17\nepoch 20 batch 18\nepoch 20 batch 19\nepoch 20 batch 20\nepoch 20 batch 21\nepoch 20 batch 22\nepoch 20 batch 23\nepoch 20 batch 24\nepoch 20 batch 25\nepoch 20 batch 26\nepoch 20 batch 27\nepoch 20 batch 28\nepoch 20 batch 29\nepoch 20 batch 30\nepoch 20 batch 31\nepoch 20 batch 32\nepoch 20 batch 33\nepoch 20 batch 34\nepoch 20 batch 35\nepoch 20 batch 36\nepoch 20 batch 37\nepoch 20 batch 38\nepoch 20 batch 39\nepoch 20 batch 40\nepoch 20 batch 41\nepoch 20 batch 42\nepoch 20 batch 43\nepoch 20 batch 44\nepoch 20 batch 45\nepoch 20 batch 46\nepoch 20 batch 47\nepoch 20 batch 48\nepoch 20 batch 49\nepoch 20 batch 50\nepoch 20 batch 51\nepoch 20 batch 52\nepoch 20 batch 53\nepoch 20 batch 54\nepoch 20 batch 55\nepoch 20 batch 56\nepoch 20 batch 57\nepoch 20 batch 58\nepoch 20 batch 59\nepoch 20 batch 60\nepoch 20 batch 61\nepoch 20 batch 62\nepoch 20 batch 63\nepoch 20 batch 64\nepoch 20 batch 65\nepoch 20 batch 66\nepoch 20 batch 67\nepoch 20 batch 68\nepoch 20 batch 69\nepoch 20 batch 70\nepoch 20 batch 71\nepoch 20 batch 72\nepoch 20 batch 73\nepoch 20 batch 74\nepoch 20 batch 75\nepoch 20 batch 76\nepoch 20 batch 77\nepoch 20 batch 78\nepoch 20 batch 79\nepoch 20 batch 80\nepoch 20 batch 81\nepoch 20 batch 82\nepoch 20 batch 83\nepoch 20 batch 84\nepoch 20 batch 85\nepoch 20 batch 86\nepoch 20 batch 87\nepoch 20 batch 88\nepoch 20 batch 89\nepoch 20 batch 90\nepoch 20 batch 91\nepoch 20 batch 92\nepoch 20 batch 93\nepoch 20 batch 94\nepoch 20 batch 95\nepoch 20 batch 96\nepoch 20 batch 97\nepoch 20 batch 98\nepoch 20 batch 99\nepoch 20 batch 100\nepoch 20 batch 101\nepoch 20 batch 102\nepoch 20 batch 103\nepoch 20 batch 104\nepoch 20 batch 105\nepoch 20 batch 106\nepoch 20 batch 107\nepoch 20 batch 108\nepoch 20 batch 109\nepoch 20 batch 110\nepoch 20 batch 111\nepoch 20 batch 112\nepoch 20 batch 113\nepoch 20 batch 114\nepoch 20 batch 115\nepoch 20 batch 116\nepoch 20 batch 117\nepoch 20 batch 118\nepoch 20 batch 119\nepoch 20 batch 120\nepoch 20 batch 121\nepoch 20 batch 122\nepoch 20 batch 123\nepoch 20 batch 124\nepoch 20 batch 125\nepoch 20 batch 126\nepoch 20 batch 127\nepoch 20 batch 128\nepoch 20 batch 129\nepoch 20 batch 130\nepoch 20 batch 131\nepoch 20 batch 132\nepoch 20 batch 133\nepoch 20 batch 134\nepoch 20 batch 135\nepoch 20 batch 136\nepoch 20 batch 137\nepoch 20 batch 138\nepoch 20 batch 139\nepoch 20 batch 140\nepoch 20 batch 141\nepoch 20 batch 142\nepoch 20 batch 143\nepoch 20 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 20, Training Loss: 4.1562, Validation Loss: 4.5640, lr: 0.00006561\nSaved model\nepoch 21 batch 0\nepoch 21 batch 1\nepoch 21 batch 2\nepoch 21 batch 3\nepoch 21 batch 4\nepoch 21 batch 5\nepoch 21 batch 6\nepoch 21 batch 7\nepoch 21 batch 8\nepoch 21 batch 9\nepoch 21 batch 10\nepoch 21 batch 11\nepoch 21 batch 12\nepoch 21 batch 13\nepoch 21 batch 14\nepoch 21 batch 15\nepoch 21 batch 16\nepoch 21 batch 17\nepoch 21 batch 18\nepoch 21 batch 19\nepoch 21 batch 20\nepoch 21 batch 21\nepoch 21 batch 22\nepoch 21 batch 23\nepoch 21 batch 24\nepoch 21 batch 25\nepoch 21 batch 26\nepoch 21 batch 27\nepoch 21 batch 28\nepoch 21 batch 29\nepoch 21 batch 30\nepoch 21 batch 31\nepoch 21 batch 32\nepoch 21 batch 33\nepoch 21 batch 34\nepoch 21 batch 35\nepoch 21 batch 36\nepoch 21 batch 37\nepoch 21 batch 38\nepoch 21 batch 39\nepoch 21 batch 40\nepoch 21 batch 41\nepoch 21 batch 42\nepoch 21 batch 43\nepoch 21 batch 44\nepoch 21 batch 45\nepoch 21 batch 46\nepoch 21 batch 47\nepoch 21 batch 48\nepoch 21 batch 49\nepoch 21 batch 50\nepoch 21 batch 51\nepoch 21 batch 52\nepoch 21 batch 53\nepoch 21 batch 54\nepoch 21 batch 55\nepoch 21 batch 56\nepoch 21 batch 57\nepoch 21 batch 58\nepoch 21 batch 59\nepoch 21 batch 60\nepoch 21 batch 61\nepoch 21 batch 62\nepoch 21 batch 63\nepoch 21 batch 64\nepoch 21 batch 65\nepoch 21 batch 66\nepoch 21 batch 67\nepoch 21 batch 68\nepoch 21 batch 69\nepoch 21 batch 70\nepoch 21 batch 71\nepoch 21 batch 72\nepoch 21 batch 73\nepoch 21 batch 74\nepoch 21 batch 75\nepoch 21 batch 76\nepoch 21 batch 77\nepoch 21 batch 78\nepoch 21 batch 79\nepoch 21 batch 80\nepoch 21 batch 81\nepoch 21 batch 82\nepoch 21 batch 83\nepoch 21 batch 84\nepoch 21 batch 85\nepoch 21 batch 86\nepoch 21 batch 87\nepoch 21 batch 88\nepoch 21 batch 89\nepoch 21 batch 90\nepoch 21 batch 91\nepoch 21 batch 92\nepoch 21 batch 93\nepoch 21 batch 94\nepoch 21 batch 95\nepoch 21 batch 96\nepoch 21 batch 97\nepoch 21 batch 98\nepoch 21 batch 99\nepoch 21 batch 100\nepoch 21 batch 101\nepoch 21 batch 102\nepoch 21 batch 103\nepoch 21 batch 104\nepoch 21 batch 105\nepoch 21 batch 106\nepoch 21 batch 107\nepoch 21 batch 108\nepoch 21 batch 109\nepoch 21 batch 110\nepoch 21 batch 111\nepoch 21 batch 112\nepoch 21 batch 113\nepoch 21 batch 114\nepoch 21 batch 115\nepoch 21 batch 116\nepoch 21 batch 117\nepoch 21 batch 118\nepoch 21 batch 119\nepoch 21 batch 120\nepoch 21 batch 121\nepoch 21 batch 122\nepoch 21 batch 123\nepoch 21 batch 124\nepoch 21 batch 125\nepoch 21 batch 126\nepoch 21 batch 127\nepoch 21 batch 128\nepoch 21 batch 129\nepoch 21 batch 130\nepoch 21 batch 131\nepoch 21 batch 132\nepoch 21 batch 133\nepoch 21 batch 134\nepoch 21 batch 135\nepoch 21 batch 136\nepoch 21 batch 137\nepoch 21 batch 138\nepoch 21 batch 139\nepoch 21 batch 140\nepoch 21 batch 141\nepoch 21 batch 142\nepoch 21 batch 143\nepoch 21 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 21, Training Loss: 4.1369, Validation Loss: 4.5767, lr: 0.00006561\nSaved model\nepoch 22 batch 0\nepoch 22 batch 1\nepoch 22 batch 2\nepoch 22 batch 3\nepoch 22 batch 4\nepoch 22 batch 5\nepoch 22 batch 6\nepoch 22 batch 7\nepoch 22 batch 8\nepoch 22 batch 9\nepoch 22 batch 10\nepoch 22 batch 11\nepoch 22 batch 12\nepoch 22 batch 13\nepoch 22 batch 14\nepoch 22 batch 15\nepoch 22 batch 16\nepoch 22 batch 17\nepoch 22 batch 18\nepoch 22 batch 19\nepoch 22 batch 20\nepoch 22 batch 21\nepoch 22 batch 22\nepoch 22 batch 23\nepoch 22 batch 24\nepoch 22 batch 25\nepoch 22 batch 26\nepoch 22 batch 27\nepoch 22 batch 28\nepoch 22 batch 29\nepoch 22 batch 30\nepoch 22 batch 31\nepoch 22 batch 32\nepoch 22 batch 33\nepoch 22 batch 34\nepoch 22 batch 35\nepoch 22 batch 36\nepoch 22 batch 37\nepoch 22 batch 38\nepoch 22 batch 39\nepoch 22 batch 40\nepoch 22 batch 41\nepoch 22 batch 42\nepoch 22 batch 43\nepoch 22 batch 44\nepoch 22 batch 45\nepoch 22 batch 46\nepoch 22 batch 47\nepoch 22 batch 48\nepoch 22 batch 49\nepoch 22 batch 50\nepoch 22 batch 51\nepoch 22 batch 52\nepoch 22 batch 53\nepoch 22 batch 54\nepoch 22 batch 55\nepoch 22 batch 56\nepoch 22 batch 57\nepoch 22 batch 58\nepoch 22 batch 59\nepoch 22 batch 60\nepoch 22 batch 61\nepoch 22 batch 62\nepoch 22 batch 63\nepoch 22 batch 64\nepoch 22 batch 65\nepoch 22 batch 66\nepoch 22 batch 67\nepoch 22 batch 68\nepoch 22 batch 69\nepoch 22 batch 70\nepoch 22 batch 71\nepoch 22 batch 72\nepoch 22 batch 73\nepoch 22 batch 74\nepoch 22 batch 75\nepoch 22 batch 76\nepoch 22 batch 77\nepoch 22 batch 78\nepoch 22 batch 79\nepoch 22 batch 80\nepoch 22 batch 81\nepoch 22 batch 82\nepoch 22 batch 83\nepoch 22 batch 84\nepoch 22 batch 85\nepoch 22 batch 86\nepoch 22 batch 87\nepoch 22 batch 88\nepoch 22 batch 89\nepoch 22 batch 90\nepoch 22 batch 91\nepoch 22 batch 92\nepoch 22 batch 93\nepoch 22 batch 94\nepoch 22 batch 95\nepoch 22 batch 96\nepoch 22 batch 97\nepoch 22 batch 98\nepoch 22 batch 99\nepoch 22 batch 100\nepoch 22 batch 101\nepoch 22 batch 102\nepoch 22 batch 103\nepoch 22 batch 104\nepoch 22 batch 105\nepoch 22 batch 106\nepoch 22 batch 107\nepoch 22 batch 108\nepoch 22 batch 109\nepoch 22 batch 110\nepoch 22 batch 111\nepoch 22 batch 112\nepoch 22 batch 113\nepoch 22 batch 114\nepoch 22 batch 115\nepoch 22 batch 116\nepoch 22 batch 117\nepoch 22 batch 118\nepoch 22 batch 119\nepoch 22 batch 120\nepoch 22 batch 121\nepoch 22 batch 122\nepoch 22 batch 123\nepoch 22 batch 124\nepoch 22 batch 125\nepoch 22 batch 126\nepoch 22 batch 127\nepoch 22 batch 128\nepoch 22 batch 129\nepoch 22 batch 130\nepoch 22 batch 131\nepoch 22 batch 132\nepoch 22 batch 133\nepoch 22 batch 134\nepoch 22 batch 135\nepoch 22 batch 136\nepoch 22 batch 137\nepoch 22 batch 138\nepoch 22 batch 139\nepoch 22 batch 140\nepoch 22 batch 141\nepoch 22 batch 142\nepoch 22 batch 143\nepoch 22 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 22, Training Loss: 4.1522, Validation Loss: 4.5826, lr: 0.00006561\nSaved model\nepoch 23 batch 0\nepoch 23 batch 1\nepoch 23 batch 2\nepoch 23 batch 3\nepoch 23 batch 4\nepoch 23 batch 5\nepoch 23 batch 6\nepoch 23 batch 7\nepoch 23 batch 8\nepoch 23 batch 9\nepoch 23 batch 10\nepoch 23 batch 11\nepoch 23 batch 12\nepoch 23 batch 13\nepoch 23 batch 14\nepoch 23 batch 15\nepoch 23 batch 16\nepoch 23 batch 17\nepoch 23 batch 18\nepoch 23 batch 19\nepoch 23 batch 20\nepoch 23 batch 21\nepoch 23 batch 22\nepoch 23 batch 23\nepoch 23 batch 24\nepoch 23 batch 25\nepoch 23 batch 26\nepoch 23 batch 27\nepoch 23 batch 28\nepoch 23 batch 29\nepoch 23 batch 30\nepoch 23 batch 31\nepoch 23 batch 32\nepoch 23 batch 33\nepoch 23 batch 34\nepoch 23 batch 35\nepoch 23 batch 36\nepoch 23 batch 37\nepoch 23 batch 38\nepoch 23 batch 39\nepoch 23 batch 40\nepoch 23 batch 41\nepoch 23 batch 42\nepoch 23 batch 43\nepoch 23 batch 44\nepoch 23 batch 45\nepoch 23 batch 46\nepoch 23 batch 47\nepoch 23 batch 48\nepoch 23 batch 49\nepoch 23 batch 50\nepoch 23 batch 51\nepoch 23 batch 52\nepoch 23 batch 53\nepoch 23 batch 54\nepoch 23 batch 55\nepoch 23 batch 56\nepoch 23 batch 57\nepoch 23 batch 58\nepoch 23 batch 59\nepoch 23 batch 60\nepoch 23 batch 61\nepoch 23 batch 62\nepoch 23 batch 63\nepoch 23 batch 64\nepoch 23 batch 65\nepoch 23 batch 66\nepoch 23 batch 67\nepoch 23 batch 68\nepoch 23 batch 69\nepoch 23 batch 70\nepoch 23 batch 71\nepoch 23 batch 72\nepoch 23 batch 73\nepoch 23 batch 74\nepoch 23 batch 75\nepoch 23 batch 76\nepoch 23 batch 77\nepoch 23 batch 78\nepoch 23 batch 79\nepoch 23 batch 80\nepoch 23 batch 81\nepoch 23 batch 82\nepoch 23 batch 83\nepoch 23 batch 84\nepoch 23 batch 85\nepoch 23 batch 86\nepoch 23 batch 87\nepoch 23 batch 88\nepoch 23 batch 89\nepoch 23 batch 90\nepoch 23 batch 91\nepoch 23 batch 92\nepoch 23 batch 93\nepoch 23 batch 94\nepoch 23 batch 95\nepoch 23 batch 96\nepoch 23 batch 97\nepoch 23 batch 98\nepoch 23 batch 99\nepoch 23 batch 100\nepoch 23 batch 101\nepoch 23 batch 102\nepoch 23 batch 103\nepoch 23 batch 104\nepoch 23 batch 105\nepoch 23 batch 106\nepoch 23 batch 107\nepoch 23 batch 108\nepoch 23 batch 109\nepoch 23 batch 110\nepoch 23 batch 111\nepoch 23 batch 112\nepoch 23 batch 113\nepoch 23 batch 114\nepoch 23 batch 115\nepoch 23 batch 116\nepoch 23 batch 117\nepoch 23 batch 118\nepoch 23 batch 119\nepoch 23 batch 120\nepoch 23 batch 121\nepoch 23 batch 122\nepoch 23 batch 123\nepoch 23 batch 124\nepoch 23 batch 125\nepoch 23 batch 126\nepoch 23 batch 127\nepoch 23 batch 128\nepoch 23 batch 129\nepoch 23 batch 130\nepoch 23 batch 131\nepoch 23 batch 132\nepoch 23 batch 133\nepoch 23 batch 134\nepoch 23 batch 135\nepoch 23 batch 136\nepoch 23 batch 137\nepoch 23 batch 138\nepoch 23 batch 139\nepoch 23 batch 140\nepoch 23 batch 141\nepoch 23 batch 142\nepoch 23 batch 143\nepoch 23 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 23, Training Loss: 4.1641, Validation Loss: 4.6033, lr: 0.00006561\nSaved model\nepoch 24 batch 0\nepoch 24 batch 1\nepoch 24 batch 2\nepoch 24 batch 3\nepoch 24 batch 4\nepoch 24 batch 5\nepoch 24 batch 6\nepoch 24 batch 7\nepoch 24 batch 8\nepoch 24 batch 9\nepoch 24 batch 10\nepoch 24 batch 11\nepoch 24 batch 12\nepoch 24 batch 13\nepoch 24 batch 14\nepoch 24 batch 15\nepoch 24 batch 16\nepoch 24 batch 17\nepoch 24 batch 18\nepoch 24 batch 19\nepoch 24 batch 20\nepoch 24 batch 21\nepoch 24 batch 22\nepoch 24 batch 23\nepoch 24 batch 24\nepoch 24 batch 25\nepoch 24 batch 26\nepoch 24 batch 27\nepoch 24 batch 28\nepoch 24 batch 29\nepoch 24 batch 30\nepoch 24 batch 31\nepoch 24 batch 32\nepoch 24 batch 33\nepoch 24 batch 34\nepoch 24 batch 35\nepoch 24 batch 36\nepoch 24 batch 37\nepoch 24 batch 38\nepoch 24 batch 39\nepoch 24 batch 40\nepoch 24 batch 41\nepoch 24 batch 42\nepoch 24 batch 43\nepoch 24 batch 44\nepoch 24 batch 45\nepoch 24 batch 46\nepoch 24 batch 47\nepoch 24 batch 48\nepoch 24 batch 49\nepoch 24 batch 50\nepoch 24 batch 51\nepoch 24 batch 52\nepoch 24 batch 53\nepoch 24 batch 54\nepoch 24 batch 55\nepoch 24 batch 56\nepoch 24 batch 57\nepoch 24 batch 58\nepoch 24 batch 59\nepoch 24 batch 60\nepoch 24 batch 61\nepoch 24 batch 62\nepoch 24 batch 63\nepoch 24 batch 64\nepoch 24 batch 65\nepoch 24 batch 66\nepoch 24 batch 67\nepoch 24 batch 68\nepoch 24 batch 69\nepoch 24 batch 70\nepoch 24 batch 71\nepoch 24 batch 72\nepoch 24 batch 73\nepoch 24 batch 74\nepoch 24 batch 75\nepoch 24 batch 76\nepoch 24 batch 77\nepoch 24 batch 78\nepoch 24 batch 79\nepoch 24 batch 80\nepoch 24 batch 81\nepoch 24 batch 82\nepoch 24 batch 83\nepoch 24 batch 84\nepoch 24 batch 85\nepoch 24 batch 86\nepoch 24 batch 87\nepoch 24 batch 88\nepoch 24 batch 89\nepoch 24 batch 90\nepoch 24 batch 91\nepoch 24 batch 92\nepoch 24 batch 93\nepoch 24 batch 94\nepoch 24 batch 95\nepoch 24 batch 96\nepoch 24 batch 97\nepoch 24 batch 98\nepoch 24 batch 99\nepoch 24 batch 100\nepoch 24 batch 101\nepoch 24 batch 102\nepoch 24 batch 103\nepoch 24 batch 104\nepoch 24 batch 105\nepoch 24 batch 106\nepoch 24 batch 107\nepoch 24 batch 108\nepoch 24 batch 109\nepoch 24 batch 110\nepoch 24 batch 111\nepoch 24 batch 112\nepoch 24 batch 113\nepoch 24 batch 114\nepoch 24 batch 115\nepoch 24 batch 116\nepoch 24 batch 117\nepoch 24 batch 118\nepoch 24 batch 119\nepoch 24 batch 120\nepoch 24 batch 121\nepoch 24 batch 122\nepoch 24 batch 123\nepoch 24 batch 124\nepoch 24 batch 125\nepoch 24 batch 126\nepoch 24 batch 127\nepoch 24 batch 128\nepoch 24 batch 129\nepoch 24 batch 130\nepoch 24 batch 131\nepoch 24 batch 132\nepoch 24 batch 133\nepoch 24 batch 134\nepoch 24 batch 135\nepoch 24 batch 136\nepoch 24 batch 137\nepoch 24 batch 138\nepoch 24 batch 139\nepoch 24 batch 140\nepoch 24 batch 141\nepoch 24 batch 142\nepoch 24 batch 143\nepoch 24 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 24, Training Loss: 4.1674, Validation Loss: 4.5864, lr: 0.00006561\nSaved model\nepoch 25 batch 0\nepoch 25 batch 1\nepoch 25 batch 2\nepoch 25 batch 3\nepoch 25 batch 4\nepoch 25 batch 5\nepoch 25 batch 6\nepoch 25 batch 7\nepoch 25 batch 8\nepoch 25 batch 9\nepoch 25 batch 10\nepoch 25 batch 11\nepoch 25 batch 12\nepoch 25 batch 13\nepoch 25 batch 14\nepoch 25 batch 15\nepoch 25 batch 16\nepoch 25 batch 17\nepoch 25 batch 18\nepoch 25 batch 19\nepoch 25 batch 20\nepoch 25 batch 21\nepoch 25 batch 22\nepoch 25 batch 23\nepoch 25 batch 24\nepoch 25 batch 25\nepoch 25 batch 26\nepoch 25 batch 27\nepoch 25 batch 28\nepoch 25 batch 29\nepoch 25 batch 30\nepoch 25 batch 31\nepoch 25 batch 32\nepoch 25 batch 33\nepoch 25 batch 34\nepoch 25 batch 35\nepoch 25 batch 36\nepoch 25 batch 37\nepoch 25 batch 38\nepoch 25 batch 39\nepoch 25 batch 40\nepoch 25 batch 41\nepoch 25 batch 42\nepoch 25 batch 43\nepoch 25 batch 44\nepoch 25 batch 45\nepoch 25 batch 46\nepoch 25 batch 47\nepoch 25 batch 48\nepoch 25 batch 49\nepoch 25 batch 50\nepoch 25 batch 51\nepoch 25 batch 52\nepoch 25 batch 53\nepoch 25 batch 54\nepoch 25 batch 55\nepoch 25 batch 56\nepoch 25 batch 57\nepoch 25 batch 58\nepoch 25 batch 59\nepoch 25 batch 60\nepoch 25 batch 61\nepoch 25 batch 62\nepoch 25 batch 63\nepoch 25 batch 64\nepoch 25 batch 65\nepoch 25 batch 66\nepoch 25 batch 67\nepoch 25 batch 68\nepoch 25 batch 69\nepoch 25 batch 70\nepoch 25 batch 71\nepoch 25 batch 72\nepoch 25 batch 73\nepoch 25 batch 74\nepoch 25 batch 75\nepoch 25 batch 76\nepoch 25 batch 77\nepoch 25 batch 78\nepoch 25 batch 79\nepoch 25 batch 80\nepoch 25 batch 81\nepoch 25 batch 82\nepoch 25 batch 83\nepoch 25 batch 84\nepoch 25 batch 85\nepoch 25 batch 86\nepoch 25 batch 87\nepoch 25 batch 88\nepoch 25 batch 89\nepoch 25 batch 90\nepoch 25 batch 91\nepoch 25 batch 92\nepoch 25 batch 93\nepoch 25 batch 94\nepoch 25 batch 95\nepoch 25 batch 96\nepoch 25 batch 97\nepoch 25 batch 98\nepoch 25 batch 99\nepoch 25 batch 100\nepoch 25 batch 101\nepoch 25 batch 102\nepoch 25 batch 103\nepoch 25 batch 104\nepoch 25 batch 105\nepoch 25 batch 106\nepoch 25 batch 107\nepoch 25 batch 108\nepoch 25 batch 109\nepoch 25 batch 110\nepoch 25 batch 111\nepoch 25 batch 112\nepoch 25 batch 113\nepoch 25 batch 114\nepoch 25 batch 115\nepoch 25 batch 116\nepoch 25 batch 117\nepoch 25 batch 118\nepoch 25 batch 119\nepoch 25 batch 120\nepoch 25 batch 121\nepoch 25 batch 122\nepoch 25 batch 123\nepoch 25 batch 124\nepoch 25 batch 125\nepoch 25 batch 126\nepoch 25 batch 127\nepoch 25 batch 128\nepoch 25 batch 129\nepoch 25 batch 130\nepoch 25 batch 131\nepoch 25 batch 132\nepoch 25 batch 133\nepoch 25 batch 134\nepoch 25 batch 135\nepoch 25 batch 136\nepoch 25 batch 137\nepoch 25 batch 138\nepoch 25 batch 139\nepoch 25 batch 140\nepoch 25 batch 141\nepoch 25 batch 142\nepoch 25 batch 143\nepoch 25 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 25, Training Loss: 4.1521, Validation Loss: 4.5633, lr: 0.00005905\nSaved model\nepoch 26 batch 0\nepoch 26 batch 1\nepoch 26 batch 2\nepoch 26 batch 3\nepoch 26 batch 4\nepoch 26 batch 5\nepoch 26 batch 6\nepoch 26 batch 7\nepoch 26 batch 8\nepoch 26 batch 9\nepoch 26 batch 10\nepoch 26 batch 11\nepoch 26 batch 12\nepoch 26 batch 13\nepoch 26 batch 14\nepoch 26 batch 15\nepoch 26 batch 16\nepoch 26 batch 17\nepoch 26 batch 18\nepoch 26 batch 19\nepoch 26 batch 20\nepoch 26 batch 21\nepoch 26 batch 22\nepoch 26 batch 23\nepoch 26 batch 24\nepoch 26 batch 25\nepoch 26 batch 26\nepoch 26 batch 27\nepoch 26 batch 28\nepoch 26 batch 29\nepoch 26 batch 30\nepoch 26 batch 31\nepoch 26 batch 32\nepoch 26 batch 33\nepoch 26 batch 34\nepoch 26 batch 35\nepoch 26 batch 36\nepoch 26 batch 37\nepoch 26 batch 38\nepoch 26 batch 39\nepoch 26 batch 40\nepoch 26 batch 41\nepoch 26 batch 42\nepoch 26 batch 43\nepoch 26 batch 44\nepoch 26 batch 45\nepoch 26 batch 46\nepoch 26 batch 47\nepoch 26 batch 48\nepoch 26 batch 49\nepoch 26 batch 50\nepoch 26 batch 51\nepoch 26 batch 52\nepoch 26 batch 53\nepoch 26 batch 54\nepoch 26 batch 55\nepoch 26 batch 56\nepoch 26 batch 57\nepoch 26 batch 58\nepoch 26 batch 59\nepoch 26 batch 60\nepoch 26 batch 61\nepoch 26 batch 62\nepoch 26 batch 63\nepoch 26 batch 64\nepoch 26 batch 65\nepoch 26 batch 66\nepoch 26 batch 67\nepoch 26 batch 68\nepoch 26 batch 69\nepoch 26 batch 70\nepoch 26 batch 71\nepoch 26 batch 72\nepoch 26 batch 73\nepoch 26 batch 74\nepoch 26 batch 75\nepoch 26 batch 76\nepoch 26 batch 77\nepoch 26 batch 78\nepoch 26 batch 79\nepoch 26 batch 80\nepoch 26 batch 81\nepoch 26 batch 82\nepoch 26 batch 83\nepoch 26 batch 84\nepoch 26 batch 85\nepoch 26 batch 86\nepoch 26 batch 87\nepoch 26 batch 88\nepoch 26 batch 89\nepoch 26 batch 90\nepoch 26 batch 91\nepoch 26 batch 92\nepoch 26 batch 93\nepoch 26 batch 94\nepoch 26 batch 95\nepoch 26 batch 96\nepoch 26 batch 97\nepoch 26 batch 98\nepoch 26 batch 99\nepoch 26 batch 100\nepoch 26 batch 101\nepoch 26 batch 102\nepoch 26 batch 103\nepoch 26 batch 104\nepoch 26 batch 105\nepoch 26 batch 106\nepoch 26 batch 107\nepoch 26 batch 108\nepoch 26 batch 109\nepoch 26 batch 110\nepoch 26 batch 111\nepoch 26 batch 112\nepoch 26 batch 113\nepoch 26 batch 114\nepoch 26 batch 115\nepoch 26 batch 116\nepoch 26 batch 117\nepoch 26 batch 118\nepoch 26 batch 119\nepoch 26 batch 120\nepoch 26 batch 121\nepoch 26 batch 122\nepoch 26 batch 123\nepoch 26 batch 124\nepoch 26 batch 125\nepoch 26 batch 126\nepoch 26 batch 127\nepoch 26 batch 128\nepoch 26 batch 129\nepoch 26 batch 130\nepoch 26 batch 131\nepoch 26 batch 132\nepoch 26 batch 133\nepoch 26 batch 134\nepoch 26 batch 135\nepoch 26 batch 136\nepoch 26 batch 137\nepoch 26 batch 138\nepoch 26 batch 139\nepoch 26 batch 140\nepoch 26 batch 141\nepoch 26 batch 142\nepoch 26 batch 143\nepoch 26 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 26, Training Loss: 4.1181, Validation Loss: 4.5858, lr: 0.00005905\nSaved model\nepoch 27 batch 0\nepoch 27 batch 1\nepoch 27 batch 2\nepoch 27 batch 3\nepoch 27 batch 4\nepoch 27 batch 5\nepoch 27 batch 6\nepoch 27 batch 7\nepoch 27 batch 8\nepoch 27 batch 9\nepoch 27 batch 10\nepoch 27 batch 11\nepoch 27 batch 12\nepoch 27 batch 13\nepoch 27 batch 14\nepoch 27 batch 15\nepoch 27 batch 16\nepoch 27 batch 17\nepoch 27 batch 18\nepoch 27 batch 19\nepoch 27 batch 20\nepoch 27 batch 21\nepoch 27 batch 22\nepoch 27 batch 23\nepoch 27 batch 24\nepoch 27 batch 25\nepoch 27 batch 26\nepoch 27 batch 27\nepoch 27 batch 28\nepoch 27 batch 29\nepoch 27 batch 30\nepoch 27 batch 31\nepoch 27 batch 32\nepoch 27 batch 33\nepoch 27 batch 34\nepoch 27 batch 35\nepoch 27 batch 36\nepoch 27 batch 37\nepoch 27 batch 38\nepoch 27 batch 39\nepoch 27 batch 40\nepoch 27 batch 41\nepoch 27 batch 42\nepoch 27 batch 43\nepoch 27 batch 44\nepoch 27 batch 45\nepoch 27 batch 46\nepoch 27 batch 47\nepoch 27 batch 48\nepoch 27 batch 49\nepoch 27 batch 50\nepoch 27 batch 51\nepoch 27 batch 52\nepoch 27 batch 53\nepoch 27 batch 54\nepoch 27 batch 55\nepoch 27 batch 56\nepoch 27 batch 57\nepoch 27 batch 58\nepoch 27 batch 59\nepoch 27 batch 60\nepoch 27 batch 61\nepoch 27 batch 62\nepoch 27 batch 63\nepoch 27 batch 64\nepoch 27 batch 65\nepoch 27 batch 66\nepoch 27 batch 67\nepoch 27 batch 68\nepoch 27 batch 69\nepoch 27 batch 70\nepoch 27 batch 71\nepoch 27 batch 72\nepoch 27 batch 73\nepoch 27 batch 74\nepoch 27 batch 75\nepoch 27 batch 76\nepoch 27 batch 77\nepoch 27 batch 78\nepoch 27 batch 79\nepoch 27 batch 80\nepoch 27 batch 81\nepoch 27 batch 82\nepoch 27 batch 83\nepoch 27 batch 84\nepoch 27 batch 85\nepoch 27 batch 86\nepoch 27 batch 87\nepoch 27 batch 88\nepoch 27 batch 89\nepoch 27 batch 90\nepoch 27 batch 91\nepoch 27 batch 92\nepoch 27 batch 93\nepoch 27 batch 94\nepoch 27 batch 95\nepoch 27 batch 96\nepoch 27 batch 97\nepoch 27 batch 98\nepoch 27 batch 99\nepoch 27 batch 100\nepoch 27 batch 101\nepoch 27 batch 102\nepoch 27 batch 103\nepoch 27 batch 104\nepoch 27 batch 105\nepoch 27 batch 106\nepoch 27 batch 107\nepoch 27 batch 108\nepoch 27 batch 109\nepoch 27 batch 110\nepoch 27 batch 111\nepoch 27 batch 112\nepoch 27 batch 113\nepoch 27 batch 114\nepoch 27 batch 115\nepoch 27 batch 116\nepoch 27 batch 117\nepoch 27 batch 118\nepoch 27 batch 119\nepoch 27 batch 120\nepoch 27 batch 121\nepoch 27 batch 122\nepoch 27 batch 123\nepoch 27 batch 124\nepoch 27 batch 125\nepoch 27 batch 126\nepoch 27 batch 127\nepoch 27 batch 128\nepoch 27 batch 129\nepoch 27 batch 130\nepoch 27 batch 131\nepoch 27 batch 132\nepoch 27 batch 133\nepoch 27 batch 134\nepoch 27 batch 135\nepoch 27 batch 136\nepoch 27 batch 137\nepoch 27 batch 138\nepoch 27 batch 139\nepoch 27 batch 140\nepoch 27 batch 141\nepoch 27 batch 142\nepoch 27 batch 143\nepoch 27 batch 144\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nEpoch: 27, Training Loss: 4.1399, Validation Loss: 4.5707, lr: 0.00005905\nSaved model\nepoch 28 batch 0\nepoch 28 batch 1\nepoch 28 batch 2\nepoch 28 batch 3\nepoch 28 batch 4\nepoch 28 batch 5\nepoch 28 batch 6\nepoch 28 batch 7\nepoch 28 batch 8\nepoch 28 batch 9\nepoch 28 batch 10\nepoch 28 batch 11\nepoch 28 batch 12\nepoch 28 batch 13\nepoch 28 batch 14\nepoch 28 batch 15\nepoch 28 batch 16\nepoch 28 batch 17\nepoch 28 batch 18\nepoch 28 batch 19\nepoch 28 batch 20\nepoch 28 batch 21\nepoch 28 batch 22\nepoch 28 batch 23\nepoch 28 batch 24\nepoch 28 batch 25\nepoch 28 batch 26\nepoch 28 batch 27\nepoch 28 batch 28\nepoch 28 batch 29\nepoch 28 batch 30\nepoch 28 batch 31\nepoch 28 batch 32\nepoch 28 batch 33\nepoch 28 batch 34\nepoch 28 batch 35\nepoch 28 batch 36\nepoch 28 batch 37\nepoch 28 batch 38\nepoch 28 batch 39\nepoch 28 batch 40\nepoch 28 batch 41\nepoch 28 batch 42\nepoch 28 batch 43\nepoch 28 batch 44\nepoch 28 batch 45\nepoch 28 batch 46\nepoch 28 batch 47\nepoch 28 batch 48\nepoch 28 batch 49\nepoch 28 batch 50\nepoch 28 batch 51\nepoch 28 batch 52\nepoch 28 batch 53\nepoch 28 batch 54\nepoch 28 batch 55\nepoch 28 batch 56\nepoch 28 batch 57\nepoch 28 batch 58\nepoch 28 batch 59\nepoch 28 batch 60\nepoch 28 batch 61\nepoch 28 batch 62\nepoch 28 batch 63\nepoch 28 batch 64\nepoch 28 batch 65\nepoch 28 batch 66\nepoch 28 batch 67\nepoch 28 batch 68\nepoch 28 batch 69\nepoch 28 batch 70\nepoch 28 batch 71\nepoch 28 batch 72\nepoch 28 batch 73\nepoch 28 batch 74\nepoch 28 batch 75\nepoch 28 batch 76\nepoch 28 batch 77\nepoch 28 batch 78\nepoch 28 batch 79\nepoch 28 batch 80\nepoch 28 batch 81\nepoch 28 batch 82\nepoch 28 batch 83\nepoch 28 batch 84\nepoch 28 batch 85\nepoch 28 batch 86\nepoch 28 batch 87\nepoch 28 batch 88\nepoch 28 batch 89\nepoch 28 batch 90\n","output_type":"stream"}]},{"cell_type":"code","source":"# Plot data\n\nplot_data = False\n\nif plot_data:\n    \n    import matplotlib.pyplot as plt\n    \n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Per scaricare il contenuto di kaggle/working (e quindi recuperare i modelli)\n# Crea lo zip della cartella che è stata creata contenente il modello e i log\n\n# from IPython.display import FileLink\n# !zip -r file.zip {model_filepath}\n# FileLink(r'file.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchmetrics.detection import MeanAveragePrecision\nimport torchvision.transforms.functional as F\n\ndef test(model, test_loader, device=\"cpu\"): \n    \n    model.transform.image_mean  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in enumerate(test_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = ((image,dict),dict)\n            if el[1]['boxes'].size()[0] != 0:\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n        # print(type(model(torch.cuda.FloatTensor(inputs))))\n#         print(\"out :\\n\", output)\n#         print(\"target :\\n\",targets)\n        #     # Example output\n        #     {'boxes': tensor([[\n        #       0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(torch.device(\"cpu\"))\n            dic[\"labels\"] = dic[\"labels\"].to(torch.device(\"cpu\"))\n            dic[\"scores\"] = dic[\"scores\"].to(torch.device(\"cpu\"))\n            \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n\nprint(\"ok\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n\nif do_model_test or test_only:\n#     checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n    test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n    \n    model = new_model()\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)\n\n# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n# #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\n# model.load_state_dict(checkpoint['model_state_dict'])\n# test(model.to(device), test_loader, device=device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN AGAIN (Continue training)\n\nimport pickle\n\ntrain_again = True\n\nif train_again:    \n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n    print(\"Loadeders and model loaded succesfully\") \n    \n#     print(f\"{device = }\")\n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.LambdaLR(\n        optimizer,\n        lr_lambda = lambda epoch: 0.8 ** epoch,\n    )\n    \n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch']\n    \n    torch.compile(model)\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, lrs, validation_losses, training_losses, epochs=num_epochs, start_from_epoch=epoch)\n    \n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\")) # decimal scale -> the output is hard to read, instead try with this: \n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")\n\n    # Resume training from a specific epoch\n    # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    \n    # Il file salvato model.tar contiene optiimzer, scheduler, loss e tanto altro\n\n    # train(model, model.optimizer, model.scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\nsimo e' intelligente\n\n\n\nsimo e' buono, \nenf e' attraente, \nsimo e' intelligente\nludo e' buona\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}