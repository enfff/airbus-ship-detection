{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8713159,"sourceType":"datasetVersion","datasetId":5181471,"isSourceIdPinned":true},{"sourceId":8741262,"sourceType":"datasetVersion","datasetId":5248227}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n## MAIN CONFIGURATIONS\nnum_epochs = 40  # Number of epochs the model will train for\nbatch_size = 32\ninit_lr = 1e-6 # Initial Learning Rate\ndata_augmentation_type = 'patch_gaussian'\n# Which data augmentation tecnique are we using?\n# 'nothing': no transformations, only image resize\n# 'geometric': basic geometric transforms\n# 'patch_gaussian': adds a gaussian patch to each\n# 'fourier_random_noise': currently doesn't work\n# 'fourier_basis_augmentation'\n                                        \ntrain_percentage = 0.20 #  0.03 means 192_556 * 0.03 ~ 4.6k images in the training set\nval_percentage = train_percentage/3\ntest_percentage = val_percentage\n\nassert (train_percentage + val_percentage + test_percentage) <= 1.0, \"Bad dataset split!\"\n\n# WHAT WILL THIS SESSION DO?\ntest_only = True # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\ndo_model_test = True # Tests the model after training\ncreate_log_file = True # self-explicatory\ncalculate_mean_std = False # Calculates the mean, and std of the datasets. When set to false, it uses the pre-calculated means, std.\nprint_images_during_training = False # self-explicatory\nsave_dataset = False # Saves the dataset to a file\n\nmodel_filepath = f\"model_{data_augmentation_type}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\n# !tree # Prints folder structure\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:52:19.787006Z","iopub.execute_input":"2024-06-20T21:52:19.787370Z","iopub.status.idle":"2024-06-20T21:52:19.804426Z","shell.execute_reply.started":"2024-06-20T21:52:19.787339Z","shell.execute_reply":"2024-06-20T21:52:19.803370Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_patch_gaussian'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np\nimport tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:52:19.807002Z","iopub.execute_input":"2024-06-20T21:52:19.807416Z","iopub.status.idle":"2024-06-20T21:52:23.368154Z","shell.execute_reply.started":"2024-06-20T21:52:19.807361Z","shell.execute_reply":"2024-06-20T21:52:23.367136Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# if do_model_test or test_only:\n#     try:\n#         from torchmetrics.detection import MeanAveragePrecision\n# #         metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n#     except:\n#         !pip install torchmetrics\n#         !pip install torchmetrics[detection]\n#         !pip install pycocotools\n#         !pip install faster-coco-eval\n        \n#         from torchmetrics.detection import MeanAveragePrecision\n# #         metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n\n!pip install torchmetrics\n!pip install torchmetrics[detection]\n!pip install pycocotools\n#!pip install faster-coco-eval","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:52:23.369256Z","iopub.execute_input":"2024-06-20T21:52:23.369670Z","iopub.status.idle":"2024-06-20T21:53:00.054765Z","shell.execute_reply.started":"2024-06-20T21:52:23.369643Z","shell.execute_reply":"2024-06-20T21:53:00.053731Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nRequirement already satisfied: pycocotools>2.0.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.0.8)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\nRequirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (2.0.8)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-20T21:53:00.056501Z","iopub.execute_input":"2024-06-20T21:53:00.056916Z","iopub.status.idle":"2024-06-20T21:53:00.086162Z","shell.execute_reply.started":"2024-06-20T21:53:00.056876Z","shell.execute_reply":"2024-06-20T21:53:00.085284Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"device = device(type='cuda')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)","metadata":{"_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-06-20T21:53:00.088775Z","iopub.execute_input":"2024-06-20T21:53:00.089080Z","iopub.status.idle":"2024-06-20T21:53:00.103307Z","shell.execute_reply.started":"2024-06-20T21:53:00.089054Z","shell.execute_reply":"2024-06-20T21:53:00.102413Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_patch_gaussian/log.txt'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.fft as fft\nimport torchvision\nimport random\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n#         # Scale the image in range [0,1)\n#         min_val = 0\n#         max_val = 255\n#         image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n#         min_val = 0\n#         max_val = 255\n#         # Scale the image in range [0,1)\n#         image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:00.104703Z","iopub.execute_input":"2024-06-20T21:53:00.105209Z","iopub.status.idle":"2024-06-20T21:53:00.126560Z","shell.execute_reply.started":"2024-06-20T21:53:00.105176Z","shell.execute_reply":"2024-06-20T21:53:00.125700Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\n# Please note: the normalization of the dataset happens later on the code\n\nmatch data_augmentation_type:\n    \n    case 'nothing':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n        ])\n    \n    case 'geometric':\n        \n        img_train_transforms = v2.Compose([\n            v2.RandomRotation(50),\n            v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n            v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])    \n  \n    case 'patch_gaussian':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            PatchGaussian(),\n        ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n        ])\n        \n    case 'fourier_random_noise':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            FourierRandomNoise(),\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n        \n    case 'fourier_basis_augmentation':\n        \n        img_train_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n            v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            FourierBasisAugmentation(),\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\nprint('ok')","metadata":{"_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-06-20T21:53:00.127991Z","iopub.execute_input":"2024-06-20T21:53:00.128597Z","iopub.status.idle":"2024-06-20T21:53:00.167211Z","shell.execute_reply.started":"2024-06-20T21:53:00.128560Z","shell.execute_reply":"2024-06-20T21:53:00.166349Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\nfrom torchvision.tv_tensors import BoundingBoxes\n\n# def show(imgs):\n\n#     if not isinstance(imgs, list):\n#         imgs = [imgs]\n#     fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n#     for i, img in enumerate(imgs):\n#         img = img.detach()\n#         img = F.to_pil_image(img)\n#         axs[0, i].imshow(np.asarray(img))\n#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = sorted(file_list, key = lambda f: f.split('/')[-1])\n        self.targets = sorted(targets, key=lambda d: d['image_id'])\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        \n        try:\n            label = self.targets[idx]\n        except:\n            print(f\"Tried to access {idx = } which DOESN'T EXIST on rcnn_targets!\")\n        \n        assert self.file_list[idx].split('/')[-1] == label['image_id'], f\"Bounding Box mismatch for {idx = }, file: {self.file_list[idx].split('/')[-1]} and label: {label['image_id']}\"\n        \n        try:\n            image = read_image(self.file_list[idx])    # numpy tensor\n            image = F.convert_image_dtype(image, torch.float) # Images must be FloatTensor with values in [0, 1]\n        except RuntimeError as e:\n            Warning(f'Errore con {self.file_list[idx]}')\n#             self.targets[idx]['labels'] = torch.tensor([])\n            return None, self.targets[idx]\n\n      #  print(self.file_list[idx])\n      #  print(self.targets[idx])\n        \n        try:\n            label['boxes'] = BoundingBoxes(data=label['boxes'], format='XYXY', canvas_size=tuple(image.size()[-2:]))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(F.convert_image_dtype(image).permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            if label['boxes'].numel():\n                image, label = self.transform(image, label)\n                # print(\"type of label:\", type(label))\n            else:\n                image = self.transform(image)\n            \n        return image, label\n\nprint('ok')","metadata":{"_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","_kg_hide-input":true,"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","id":"V1Q6ogjksMqE","execution":{"iopub.status.busy":"2024-06-20T21:53:00.168687Z","iopub.execute_input":"2024-06-20T21:53:00.169255Z","iopub.status.idle":"2024-06-20T21:53:00.182278Z","shell.execute_reply.started":"2024-06-20T21:53:00.169222Z","shell.execute_reply":"2024-06-20T21:53:00.181319Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n\ndef custom_collate_fn(batch):\n    # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n    # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n    # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n    return batch\n\n#if not test_only:\n\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import tv_tensors\nfrom torch.utils.data import SubsetRandomSampler\n\n# DATASET_DIR = os.path.join(\".\")\nTRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\nTEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\ndata_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\nship_dataset = ShipsDataset(data_list, transforms = img_train_transforms, targets=torch.load('/kaggle/input/targets-rcnn/rcnn_targets.pt'))\n\n# Fix the generator for reproducibility, remove once we understand that it works\ngenerator = torch.Generator().manual_seed(42)\n(\n    train_dataset,\n    val_dataset,\n    test_dataset,\n    unused_dataset\n) = torch.utils.data.random_split(ship_dataset, [train_percentage, val_percentage, test_percentage, 1 - train_percentage - val_percentage - test_percentage], generator)\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\nval_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\ntest_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n\nprint(f\"Whole dataset size: {len(ship_dataset)}\")\nprint(f\"train loader size: {len(train_loader)} batches ({train_percentage:.2f}%)\")\nprint(f\"validation loader size: {len(val_loader)} batches ({val_percentage:.2f}%)\")\nprint(f\"test loader size: {len(test_loader)} batches ({test_percentage:.2f}%)\")\n\nlogger.info(f\"Whole dataset size: {len(ship_dataset)}\")\nlogger.info(f\"train loader size: {len(train_loader)} batches ({train_percentage:.2f}%)\")\nlogger.info(f\"validation loader size: {len(val_loader)} batches ({val_percentage:.2f}%)\")\nlogger.info(f\"test loader size: {len(test_loader)} batches ({test_percentage:.2f}%)\")\n\n# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"\n\nprint('ok')","metadata":{"_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-06-20T21:53:00.183455Z","iopub.execute_input":"2024-06-20T21:53:00.183722Z","iopub.status.idle":"2024-06-20T21:53:19.943904Z","shell.execute_reply.started":"2024-06-20T21:53:00.183698Z","shell.execute_reply":"2024-06-20T21:53:19.943120Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Whole dataset size: 192556\ntrain loader size: 1204 batches (0.20%)\nvalidation loader size: 402 batches (0.07%)\ntest loader size: 402 batches (0.07%)\nok\n","output_type":"stream"}]},{"cell_type":"code","source":"if save_dataset:\n    # Save loaders\n    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\n    print('Dataset Loaders saved succesfully!')\n    logger.info(\"Finished saving Dataset Loaders\")\nelse:\n    print('Skipping saving dataset')","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:19.945169Z","iopub.execute_input":"2024-06-20T21:53:19.945619Z","iopub.status.idle":"2024-06-20T21:53:19.951651Z","shell.execute_reply.started":"2024-06-20T21:53:19.945583Z","shell.execute_reply":"2024-06-20T21:53:19.950834Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Skipping saving dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    \n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        \n        if i % 5 == 0:\n            print(\"batch: \", i)\n        \n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n\n\ndef new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\n\ndef save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, mean_std, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs,\n        'mean_std': mean_std\n    }, os.path.join(model_filepath, model_name))\n    \n    print(\"Saved model\")\n\n\nmodel_rcnn = new_model()\nprint(\"ok match data_augmentation_type, new_model, save_checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:19.952867Z","iopub.execute_input":"2024-06-20T21:53:19.953133Z","iopub.status.idle":"2024-06-20T21:53:20.794552Z","shell.execute_reply.started":"2024-06-20T21:53:19.953109Z","shell.execute_reply":"2024-06-20T21:53:20.793619Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"ok match data_augmentation_type, new_model, save_checkpoint\n","output_type":"stream"}]},{"cell_type":"code","source":"# Normalmente andrebbero calcolati rispetto il dataset. Per ora facciamo delle prove,\n# e teniamo dati pre-calcolati (pur sapendo non siano accuratissimi)\n\nif calculate_mean_std:\n    logger.info(\"Calculated correct mean, std\")\n    image_mean_train, image_std_train = get_mean_std(train_loader)\n    image_mean_val, image_std_val = get_mean_std(test_loader)\n    image_mean_test, image_std_test = get_mean_std(val_loader)\nelse:\n    logger.info(\"Using approximated mean, std\")\n    match data_augmentation_type:\n        case 'nothing':\n            image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n            image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n            image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n            image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n            image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n            image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n\n        case 'geometric':\n            image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n            image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n            image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n            image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n            image_mean_test = torch.Tensor([0.2114, 0.2936, 0.3265])\n            image_std_test = torch.Tensor([0.0816, 0.0745, 0.0731])\n\n        case 'patch_gaussian':\n            image_mean_train = torch.Tensor([0.0941, 0.0936, 0.0942])\n            image_std_train = torch.Tensor([0.1021, 0.1032, 0.1025])\n\n            image_mean_val = torch.Tensor([0.0921, 0.0912, 0.0928])\n            image_std_val = torch.Tensor([0.1022, 0.1025, 0.1026])\n\n            image_mean_test = torch.Tensor([0.0979, 0.0949, 0.0937])\n            image_std_test = torch.Tensor([0.1061, 0.1042, 0.1011])\n\n        case 'fourier_random_noise':\n            image_mean_train = torch.Tensor([0., 0., 0.])\n            image_std_train = torch.Tensor([0., 0., 0.])\n\n            image_mean_val = torch.Tensor([0., 0., 0.])\n            image_std_val = torch.Tensor([0., 0., 0.])\n\n            image_mean_test = torch.Tensor([0., 0., 0.])\n            image_std_test = torch.Tensor([0., 0., 0.])\n\n        case 'fourier_basis_augmentation':\n            image_mean_train = torch.Tensor([0.0913, 0.0944, 0.0934])\n            image_std_train = torch.Tensor([0.0988, 0.1006, 0.1007])\n\n            image_mean_val = torch.Tensor([0.0949, 0.0938, 0.0963])\n            image_std_val = torch.Tensor([0.1025, 0.1029, 0.1040])\n\n            image_mean_test = torch.Tensor([0.0940, 0.0942, 0.0947])\n            image_std_test = torch.Tensor([0.1032, 0.1043, 0.1020])\n\n\nprint(f\"{image_mean_train = }, {image_std_train = }\")\nprint(f\"{image_mean_val = }, {image_std_val = }\")\nprint(f\"{image_mean_test = }, {image_std_test = }\")\nprint(f\"{data_augmentation_type = }\")\n\nlogger.info(f\"{image_mean_train = }, {image_std_train = }\")\nlogger.info(f\"{image_mean_val = }, {image_std_val = }\")\nlogger.info(f\"{image_mean_test = }, {image_std_test = }\")\nlogger.info(f\"{data_augmentation_type = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:20.795713Z","iopub.execute_input":"2024-06-20T21:53:20.795999Z","iopub.status.idle":"2024-06-20T21:53:20.820263Z","shell.execute_reply.started":"2024-06-20T21:53:20.795973Z","shell.execute_reply":"2024-06-20T21:53:20.819194Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"image_mean_train = tensor([0.0941, 0.0936, 0.0942]), image_std_train = tensor([0.1021, 0.1032, 0.1025])\nimage_mean_val = tensor([0.0921, 0.0912, 0.0928]), image_std_val = tensor([0.1022, 0.1025, 0.1026])\nimage_mean_test = tensor([0.0979, 0.0949, 0.0937]), image_std_test = tensor([0.1061, 0.1042, 0.1011])\ndata_augmentation_type = 'patch_gaussian'\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes\n\nif not test_only:\n    \n    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n\n    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n\n        model.transform.image_mean = image_mean_train\n        model.transform.image_std = image_std_train\n        model._skip_resize = True\n\n        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n\n            training_loss = 0.0\n            batch_cumsum = 0\n            model.train()\n            \n            for i, batch in enumerate(train_loader):\n                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n                if i % 50 == 0:\n                    print(f\"epoch {epoch} batch {i}\")\n                batch_cumsum += len(batch) # needed to compute the training loss later\n                optimizer.zero_grad()\n                \n                inputs = []\n                targets = []\n                \n                for el in batch:       # el = (image,dict) when transforms are active\n                    \n                    el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                    }\n                    \n                    if not el_dict[\"labels\"].numel():\n                        # filtering out empty images (model does not accept empty targets)\n                        continue\n                    else:\n                      #  print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                      #  print(f'el_dict has {el_dict[\"boxes\"] = }')\n                        \n                        image = el[0].to(device)\n                        el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n                        \n                        inputs.append(image)\n                        targets.append(el_dict)\n                        \n                        # Print images during training\n                        if print_images_during_training:\n                            num = len(el_dict[\"boxes\"])\n\n                            img = draw_bounding_boxes(\n                                (image*256).byte(),\n                                el_dict[\"boxes\"],\n                                width = 1,\n                                colors = 'yellow',\n                                # font='arial',\n                                font_size = 15\n                            )\n\n                            fig, ax = plt.subplots()\n                            fig.set_size_inches(16,9)\n                            fig.tight_layout(pad=5)\n                            ax.imshow(img.byte().permute(1, 2, 0))\n                            plt.show()\n                            plt.close()\n\n                        # print(f\"{el = }\")\n                        # Example el\n                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                        #          ...,\n                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                        #          ...,\n                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                        #          ...,\n                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n\n                if len(inputs) == 0:\n                    continue\n\n                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n                \"\"\" EXAMPLE :\n                    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n                     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n                     'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n\n                     How losses are computed:\n\n                     -loss_classifier-\n                     classification_loss = F.cross_entropy(class_logits, labels)\n\n                     -loss_box_reg-\n                     box_loss = F.smooth_l1_loss(\n                        box_regression[sampled_pos_inds_subset, labels_pos],\n                        regression_targets[sampled_pos_inds_subset],\n                        beta=1 / 9,\n                        reduction=\"sum\",\n                    )\n                    box_loss = box_loss / labels.numel()\n\n                    -loss_rpn_box_reg-\n                    box_loss = F.smooth_l1_loss(\n                    pred_bbox_deltas[sampled_pos_inds],\n                    regression_targets[sampled_pos_inds],\n                    beta=1 / 9,\n                    reduction=\"sum\",\n                    ) / (sampled_inds.numel())\n\n                    -loss_objectness-\n                    objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n                 \"\"\"\n\n                loss = sum(loss for loss in output.values())\n                loss.backward()\n                optimizer.step()\n                training_loss += loss.data.item() * len(batch)\n\n            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n            scheduler.step() # changes LR\n            training_loss /= batch_cumsum\n            training_losses.append(training_loss)\n            \n            # VALIDATION\n            model.transform.image_mean = image_mean_val\n            model.transform.image_std = image_std_val\n\n            model.train()\n            num_correct = 0\n            num_examples = 0\n            valid_loss = 0\n\n            with torch.no_grad():\n                for i,batch in enumerate(val_loader):\n                    if i % 20 == 0:\n                        print(\"(VAL) batch\", i)\n                    \n                    inputs = []\n                    targets = []\n\n                    for el in batch:       # el = (image,labels)\n                        \n                        el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                        }\n\n                        if not el_dict[\"labels\"].numel():\n                            # filtering out empty images (model does not accept empty targets)\n                            continue\n                        else:\n                        #    print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                        #    print(f'el_dict has {el_dict[\"boxes\"] = }')\n\n                            image = el[0].to(device)\n                            el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n\n                            inputs.append(image)\n                            targets.append(el_dict)\n            \n                        \n                        '''\n                        if el[1]['boxes'].size()[0] != 0:\n                            inputs.append(el[0].to(device))\n                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n                        '''\n                   \n                    if len(inputs) == 0:\n                        continue\n\n                    output = model(inputs, targets)\n\n                    loss = sum(loss for loss in output.values())\n                    valid_loss += loss.data.item() *len(batch)\n\n            valid_loss /= len(val_loader.dataset)\n            validation_losses.append(valid_loss)\n\n            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.10f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.10f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, mean_std=[image_mean_train, image_std_train,image_mean_val, image_std_val, image_mean_test, image_std_test])\n\nprint(f\"{data_augmentation_type = }\")","metadata":{"_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-06-20T21:53:20.821913Z","iopub.execute_input":"2024-06-20T21:53:20.822208Z","iopub.status.idle":"2024-06-20T21:53:20.869257Z","shell.execute_reply.started":"2024-06-20T21:53:20.822179Z","shell.execute_reply":"2024-06-20T21:53:20.868227Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"data_augmentation_type = 'patch_gaussian'\n","output_type":"stream"}]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n\n#     scheduler = torch.optim.lr_scheduler.StepLR(\n#         optimizer,\n#         gamma = 0.9,\n#         step_size = 5,\n#     )\n\n    scheduler = torch.optim.lr_scheduler.StepLR( # LR never changes\n        optimizer,\n        gamma = 0.9,\n        step_size = 15,\n    )\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n# plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:20.873077Z","iopub.execute_input":"2024-06-20T21:53:20.873694Z","iopub.status.idle":"2024-06-20T21:53:20.881366Z","shell.execute_reply.started":"2024-06-20T21:53:20.873659Z","shell.execute_reply":"2024-06-20T21:53:20.880402Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms.functional as F\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n\ndef test(model, test_loader, device=torch.device(\"cpu\")):\n    # Normally targets should be None\n    \n    model.transform.image_mean  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric =  MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0    \n    \n    for i, batch in enumerate(test_loader):\n        print(f\"Batch: {i}\")\n        logger.info(f\"TEST, Batch; {i}\")\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,dict)\n            if el[0].numel() and el[1]['labels'].numel(): # We're considering non-empty elements\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n\n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        for el in inputs:\n            el.cpu()\n        \n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(torch.device(\"cpu\"))\n            dic[\"labels\"] = dic[\"labels\"].to(torch.device(\"cpu\"))\n            dic[\"scores\"] = dic[\"scores\"].to(torch.device(\"cpu\"))\n            \n        for dic in targets:\n            dic[\"boxes\"] = dic[\"boxes\"].to(torch.device(\"cpu\"))\n            dic[\"labels\"] = dic[\"labels\"].to(torch.device(\"cpu\"))\n        \n        torch.cuda.empty_cache()\n        \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)\n    #log.info(f\"TEST, scored {mAP:.10f}\")\n    print(f\"TEST, scored {mAP:.10f}\")\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:20.882682Z","iopub.execute_input":"2024-06-20T21:53:20.882982Z","iopub.status.idle":"2024-06-20T21:53:22.354214Z","shell.execute_reply.started":"2024-06-20T21:53:20.882955Z","shell.execute_reply":"2024-06-20T21:53:22.353273Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# START MODEL TEST\n'''\nif model:\n    del model\n    torch.cuda.empty_cache()\n'''\n\n\nif do_model_test or test_only:\n    checkpoint = torch.load(os.path.join('/kaggle/input/model-batch-fourier', \"model.tar\")) # LUDO\n #   checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n  #  test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n    \n    model = new_model()\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:48.860463Z","iopub.execute_input":"2024-06-20T21:53:48.861362Z","iopub.status.idle":"2024-06-20T21:58:17.423302Z","shell.execute_reply.started":"2024-06-20T21:53:48.861326Z","shell.execute_reply":"2024-06-20T21:58:17.422301Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"model is now using cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Batch: 0\nBatch: 1\nBatch: 2\nBatch: 3\nBatch: 4\nBatch: 5\nBatch: 6\nBatch: 7\nBatch: 8\nBatch: 9\nBatch: 10\nBatch: 11\nBatch: 12\nBatch: 13\nBatch: 14\nBatch: 15\nBatch: 16\nBatch: 17\nBatch: 18\nBatch: 19\nBatch: 20\nBatch: 21\nBatch: 22\nBatch: 23\nBatch: 24\nBatch: 25\nBatch: 26\nBatch: 27\nBatch: 28\nBatch: 29\nBatch: 30\nBatch: 31\nBatch: 32\nBatch: 33\nBatch: 34\nBatch: 35\nBatch: 36\nBatch: 37\nBatch: 38\nBatch: 39\nBatch: 40\nBatch: 41\nBatch: 42\nBatch: 43\nBatch: 44\nBatch: 45\nBatch: 46\nBatch: 47\nBatch: 48\nBatch: 49\nBatch: 50\nBatch: 51\nBatch: 52\nBatch: 53\nBatch: 54\nBatch: 55\nBatch: 56\nBatch: 57\nBatch: 58\nBatch: 59\nBatch: 60\nBatch: 61\nBatch: 62\nBatch: 63\nBatch: 64\nBatch: 65\nBatch: 66\nBatch: 67\nBatch: 68\nBatch: 69\nBatch: 70\nBatch: 71\nBatch: 72\nBatch: 73\nBatch: 74\nBatch: 75\nBatch: 76\nBatch: 77\nBatch: 78\nBatch: 79\nBatch: 80\nBatch: 81\nBatch: 82\nBatch: 83\nBatch: 84\nBatch: 85\nBatch: 86\nBatch: 87\nBatch: 88\nBatch: 89\nBatch: 90\nBatch: 91\nBatch: 92\nBatch: 93\nBatch: 94\nBatch: 95\nBatch: 96\nBatch: 97\nBatch: 98\nBatch: 99\nBatch: 100\nBatch: 101\nBatch: 102\nBatch: 103\nBatch: 104\nBatch: 105\nBatch: 106\nBatch: 107\nBatch: 108\nBatch: 109\nBatch: 110\nBatch: 111\nBatch: 112\nBatch: 113\nBatch: 114\nBatch: 115\nBatch: 116\nBatch: 117\nBatch: 118\nBatch: 119\nBatch: 120\nBatch: 121\nBatch: 122\nBatch: 123\nBatch: 124\nBatch: 125\nBatch: 126\nBatch: 127\nBatch: 128\nBatch: 129\nBatch: 130\nBatch: 131\nBatch: 132\nBatch: 133\nBatch: 134\nBatch: 135\nBatch: 136\nBatch: 137\nBatch: 138\nBatch: 139\nBatch: 140\nBatch: 141\nBatch: 142\nBatch: 143\nBatch: 144\nBatch: 145\nBatch: 146\nBatch: 147\nBatch: 148\nBatch: 149\nBatch: 150\nBatch: 151\nBatch: 152\nBatch: 153\nBatch: 154\nBatch: 155\nBatch: 156\nBatch: 157\nBatch: 158\nBatch: 159\nBatch: 160\nBatch: 161\nBatch: 162\nBatch: 163\nBatch: 164\nBatch: 165\nBatch: 166\nBatch: 167\nBatch: 168\nBatch: 169\nBatch: 170\nBatch: 171\nBatch: 172\nBatch: 173\nBatch: 174\nBatch: 175\nBatch: 176\nBatch: 177\nBatch: 178\nBatch: 179\nBatch: 180\nBatch: 181\nBatch: 182\nBatch: 183\nBatch: 184\nBatch: 185\nBatch: 186\nBatch: 187\nBatch: 188\nBatch: 189\nBatch: 190\nBatch: 191\nBatch: 192\nBatch: 193\nBatch: 194\nBatch: 195\nBatch: 196\nBatch: 197\nBatch: 198\nBatch: 199\nBatch: 200\nBatch: 201\nBatch: 202\nBatch: 203\nBatch: 204\nBatch: 205\nBatch: 206\nBatch: 207\nBatch: 208\nBatch: 209\nBatch: 210\nBatch: 211\nBatch: 212\nBatch: 213\nBatch: 214\nBatch: 215\nBatch: 216\nBatch: 217\nBatch: 218\nBatch: 219\nBatch: 220\nBatch: 221\nBatch: 222\nBatch: 223\nBatch: 224\nBatch: 225\nBatch: 226\nBatch: 227\nBatch: 228\nBatch: 229\nBatch: 230\nBatch: 231\nBatch: 232\nBatch: 233\nBatch: 234\nBatch: 235\nBatch: 236\nBatch: 237\nBatch: 238\nBatch: 239\nBatch: 240\nBatch: 241\nBatch: 242\nBatch: 243\nBatch: 244\nBatch: 245\nBatch: 246\nBatch: 247\nBatch: 248\nBatch: 249\nBatch: 250\nBatch: 251\nBatch: 252\nBatch: 253\nBatch: 254\nBatch: 255\nBatch: 256\nBatch: 257\nBatch: 258\nBatch: 259\nBatch: 260\nBatch: 261\nBatch: 262\nBatch: 263\nBatch: 264\nBatch: 265\nBatch: 266\nBatch: 267\nBatch: 268\nBatch: 269\nBatch: 270\nBatch: 271\nBatch: 272\nBatch: 273\nBatch: 274\nBatch: 275\nBatch: 276\nBatch: 277\nBatch: 278\nBatch: 279\nBatch: 280\nBatch: 281\nBatch: 282\nBatch: 283\nBatch: 284\nBatch: 285\nBatch: 286\nBatch: 287\nBatch: 288\nBatch: 289\nBatch: 290\nBatch: 291\nBatch: 292\nBatch: 293\nBatch: 294\nBatch: 295\nBatch: 296\nBatch: 297\nBatch: 298\nBatch: 299\nBatch: 300\nBatch: 301\nBatch: 302\nBatch: 303\nBatch: 304\nBatch: 305\nBatch: 306\nBatch: 307\nBatch: 308\nBatch: 309\nBatch: 310\nBatch: 311\nBatch: 312\nBatch: 313\nBatch: 314\nBatch: 315\nBatch: 316\nBatch: 317\nBatch: 318\nBatch: 319\nBatch: 320\nBatch: 321\nBatch: 322\nBatch: 323\nBatch: 324\nBatch: 325\nBatch: 326\nBatch: 327\nBatch: 328\nBatch: 329\nBatch: 330\nBatch: 331\nBatch: 332\nBatch: 333\nBatch: 334\nBatch: 335\nBatch: 336\nBatch: 337\nBatch: 338\nBatch: 339\nBatch: 340\nBatch: 341\nBatch: 342\nBatch: 343\nBatch: 344\nBatch: 345\nBatch: 346\nBatch: 347\nBatch: 348\nBatch: 349\nBatch: 350\nBatch: 351\nBatch: 352\nBatch: 353\nBatch: 354\nBatch: 355\nBatch: 356\nBatch: 357\nBatch: 358\nBatch: 359\nBatch: 360\nBatch: 361\nBatch: 362\nBatch: 363\nBatch: 364\nBatch: 365\nBatch: 366\nBatch: 367\nBatch: 368\nBatch: 369\nBatch: 370\nBatch: 371\nBatch: 372\nBatch: 373\nBatch: 374\nBatch: 375\nBatch: 376\nBatch: 377\nBatch: 378\nBatch: 379\nBatch: 380\nBatch: 381\nBatch: 382\nBatch: 383\nBatch: 384\nBatch: 385\nBatch: 386\nBatch: 387\nBatch: 388\nBatch: 389\nBatch: 390\nBatch: 391\nBatch: 392\nBatch: 393\nBatch: 394\nBatch: 395\nBatch: 396\nBatch: 397\nBatch: 398\nBatch: 399\nBatch: 400\nBatch: 401\nTEST, scored 0.1435593218\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN AGAIN (Continue training)\n\nimport pickle\n\nif train_again:\n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n    print(\"Loadeders and model loaded succesfully\") \n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n    \n    criterion = nn.CrossEntropyLoss()\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch'] # Resume training from a specific epoch\n    \n    logger.info(f\"Continuing training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    logger.info(f\"Restarting from {epoch = }\")\n    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device, start_from_epoch=epoch)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:22.818949Z","iopub.status.idle":"2024-06-20T21:53:22.819283Z","shell.execute_reply.started":"2024-06-20T21:53:22.819124Z","shell.execute_reply":"2024-06-20T21:53:22.819137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ludo e' intelligente\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:22.820692Z","iopub.status.idle":"2024-06-20T21:53:22.821140Z","shell.execute_reply.started":"2024-06-20T21:53:22.820913Z","shell.execute_reply":"2024-06-20T21:53:22.820931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simo e' buono\nludo e' buonasimo e' attraenteludo e' intelligente\n\n\nenf e' belloenf e' intelligente\n\nsimo e' bello\n\n\nsimo e' intelligenteenf e' attraente\n\n\n\n\n\n\n\n\n\n\n\n\nenf e' buono\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-20T21:53:22.822501Z","iopub.status.idle":"2024-06-20T21:53:22.822931Z","shell.execute_reply.started":"2024-06-20T21:53:22.822709Z","shell.execute_reply":"2024-06-20T21:53:22.822727Z"},"trusted":true},"execution_count":null,"outputs":[]}]}