{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":8650287,"sourceType":"datasetVersion","datasetId":5181471}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n## MAIN CONFIGURATIONS\nmodel_id = '0'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 10  # Number of epochs the model will train for\nbatch_size = 32\ninit_lr = 1e-4 # Initial Learning Rate\ndata_augmentation_type = 'noaug'    # Which data augmentation tecnique are we using?\n                                    # 'none' :     only image resize\n                                    # 'noaug':     basic geometric transforms\n                                    # 'fourier':   fourier transforms\n\ntrain_percentage = 0.03 #  0.03 means 192_556 * 0.03 ~ 4.6k images in the training\nval_percentage = train_percentage/3\ntest_percentage = val_percentage\n            \n            \n## WHAT WILL THIS SESSION DO?\ntest_only = False # When True it doesn't train the model, but it just tests an existing one\ntrain_again = False # Trains the model again for num_epoch times\ndo_model_test = False # Tests the model after training\ncreate_log_file = True\nplot_data = True # Plot training data of '{model_filepath}/model.tar'\nprint_images_during_training = False\n\nmodel_filepath = f\"model_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"models\", model_filepath)\nprint(f\"{model_filepath = }\")\n\n# !tree # Prints folder structure\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:44:52.856634Z","iopub.execute_input":"2024-06-11T08:44:52.856982Z","iopub.status.idle":"2024-06-11T08:44:52.872836Z","shell.execute_reply.started":"2024-06-11T08:44:52.856953Z","shell.execute_reply":"2024-06-11T08:44:52.871862Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"model_filepath = 'models/model_noaug_id0'\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:44:52.874663Z","iopub.execute_input":"2024-06-11T08:44:52.875199Z","iopub.status.idle":"2024-06-11T08:44:58.035305Z","shell.execute_reply.started":"2024-06-11T08:44:52.875174Z","shell.execute_reply":"2024-06-11T08:44:58.034346Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install torchmetrics\n!pip install torchmetrics[detection]","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:44:58.036580Z","iopub.execute_input":"2024-06-11T08:44:58.036985Z","iopub.status.idle":"2024-06-11T08:45:23.663305Z","shell.execute_reply.started":"2024-06-11T08:44:58.036960Z","shell.execute_reply":"2024-06-11T08:45:23.662140Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: torchmetrics[detection] in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.11.2)\nRequirement already satisfied: torchvision>=0.8 in /opt/conda/lib/python3.10/site-packages (from torchmetrics[detection]) (0.16.2)\nCollecting pycocotools>2.0.0 (from torchmetrics[detection])\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics[detection]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics[detection]) (3.1.1)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>2.0.0->torchmetrics[detection]) (3.7.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics[detection]) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8->torchmetrics[detection]) (9.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.4.5)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics[detection]) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.8->torchmetrics[detection]) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics[detection]) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>2.0.0->torchmetrics[detection]) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"{device = }\")","metadata":{"_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","execution":{"iopub.status.busy":"2024-06-11T08:45:23.665825Z","iopub.execute_input":"2024-06-11T08:45:23.666156Z","iopub.status.idle":"2024-06-11T08:45:23.694027Z","shell.execute_reply.started":"2024-06-11T08:45:23.666127Z","shell.execute_reply":"2024-06-11T08:45:23.692937Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"device = device(type='cuda')\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nif create_log_file:\n    log_filepath = \"\"\n    logger = logging.getLogger('RootLogger')\n    log_filepath = os.path.join(model_filepath, f\"log\" + \".txt\")\n    print(f\"{log_filepath = }\")\n    \n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.INFO,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\n\nif log_filepath:\n    print(log_filepath)\nelse:\n    print(\"No logging\")","metadata":{"_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","execution":{"iopub.status.busy":"2024-06-11T08:45:23.695159Z","iopub.execute_input":"2024-06-11T08:45:23.695454Z","iopub.status.idle":"2024-06-11T08:45:23.707210Z","shell.execute_reply.started":"2024-06-11T08:45:23.695430Z","shell.execute_reply":"2024-06-11T08:45:23.706170Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"log_filepath = 'models/model_noaug_id0/log.txt'\nmodels/model_noaug_id0/log.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"# FOURIER DATA AUGMENTATION\n\nimport torch.fft as fft\nimport torchvision\nimport random\n\nfrom torch import sin, cos\n\nclass FourierRandomNoise(object):\n        \n    def __call__(self, *sample ):\n        image = sample[0]\n\n        # Fourier Transform\n        fourier = fft.rfftn(image)\n        magnitude, angle = self.__polar_form(fourier)\n\n        # Apply Noise in the Frequency Domain\n        noise = torch.rand(fourier.size())\n        noised_magnitude = torch.mul(magnitude,noise)\n\n        # Inverse Fourier Transform\n        fourier = self.__complex_form(noised_magnitude,angle)\n        modified_image = fft.irfftn(fourier).byte()\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return modified_image, label\n\n        return modified_image\n    \n    def __polar_form(self, complex_tensor):\n        return complex_tensor.abs(), complex_tensor.angle()\n\n    def __complex_form(self, magnitude, angle):\n        return torch.polar(magnitude,angle)\n    \n\nclass PatchGaussian(object):\n    \n    def __call__(self,*sample, patch_size=30, sigma_max=0.2):\n        '''\n        Applies a Gaussian Patch of size patch_size x patch_size to the image.\n        The noise of the patch can be modified by specifying its variance\n        '''\n        \n        image = sample[0]\n        size = image.size()\n        # Scale the image in range [0,1)\n        min_val = 0\n        max_val = 255\n        image = (image-min_val)/(max_val-min_val)\n\n        # Define Gaussian patch\n        patch = torch.empty(size).normal_(0,sigma_max)\n        # Sample Corner Indices\n        ci = random.sample([i for i in range(size[1]-patch_size)],1)[0]\n        cj = random.sample([i for i in range(size[2]-patch_size)],1)[0]\n        u, v = torch.meshgrid(torch.arange(size[1]), torch.arange(size[2]),indexing='ij')\n        u = torch.stack([u,u,u])\n        v = torch.stack([v,v,v])\n        mask = ((u<ci+patch_size)*(u>ci)*(v<cj+patch_size)*(v>cj)).int()\n        patch = mask*patch\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(image+patch,0,1), label\n        \n        return torch.clip(image+patch,0,1)\n\nclass FourierBasisAugmentation(object):\n    \n    def __call__(self,*sample, l=0.3):\n        '''\n        Adds a Fourier Basis Function to the image\n        '''\n        image = sample[0]\n        shape = image.size()\n        min_val = 0\n        max_val = 255\n        # Scale the image in range [0,1)\n        image = (image-min_val)/(max_val-min_val)\n\n        # Generate a frequency per channel, in the range [0, M], drawn uniformly,\n        # where M is the size of the image\n        f = (shape[1]-1)*torch.rand(3)\n        # Generate a omega per channel, in the range [0, pi], drawn uniformly,\n        w = (torch.pi-0)*torch.rand(3)\n\n        # Sample the decay parameter from a l-exponential distribution\n        sigma = torch.distributions.Exponential(1/l).sample((3,))\n\n        # Generate basis function\n        u, v = torch.meshgrid(torch.arange(shape[1]), torch.arange(shape[2]),indexing='ij')\n        basis_r = sigma[0]*sin(2*torch.pi*f[0]*(u*cos(w[0])+v*sin(w[0])-torch.pi/4))\n        basis_g = sigma[1]*sin(2*torch.pi*f[1]*(u*cos(w[1])+v*sin(w[1])-torch.pi/4))\n        basis_b = sigma[2]*sin(2*torch.pi*f[2]*(u*cos(w[2])+v*sin(w[2])-torch.pi/4))\n        noise = torch.stack([basis_r,basis_g,basis_b])\n\n        # Modify The Image\n        modified_image = image+noise\n        \n        if len(sample) >= 2:\n            label = sample[1]\n            return torch.clip(modified_image,0,1), label\n\n        return torch.clip(modified_image,0,1)\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:45:23.708587Z","iopub.execute_input":"2024-06-11T08:45:23.708828Z","iopub.status.idle":"2024-06-11T08:45:23.731990Z","shell.execute_reply.started":"2024-06-11T08:45:23.708807Z","shell.execute_reply":"2024-06-11T08:45:23.731036Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRANSFORMATIONS\n\nfrom torchvision.transforms import v2\n\nimg_dimensions = 224\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n        \n        img_train_transforms = v2.Compose([\n             v2.RandomRotation(50),\n             v2.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n             v2.RandomHorizontalFlip(p=0.5),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n    \n    case 'fourier':\n        \n        img_train_transforms = v2.Compose([\n             FourierRandomNoise(),\n             PatchGaussian(),\n             FourierBasisAugmentation(),\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n            ])\n\n        img_validation_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n\n        img_test_transforms = v2.Compose([\n            v2.Resize((img_dimensions, img_dimensions)),\n             v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]), # alternative to ToTensor\n             #v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n            ])\n    \n    case 'none':\n        img_train_transforms = v2.Compose([\n                v2.Resize((img_dimensions, img_dimensions)),\n                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n            ])\n        img_validation_transforms = v2.Compose([\n                v2.Resize((img_dimensions, img_dimensions)),\n                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n            ])\n        img_test_transforms = v2.Compose([\n                v2.Resize((img_dimensions, img_dimensions)),\n                v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n            ])\n\nprint('ok')","metadata":{"_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","id":"LIgECtVqMlCI","execution":{"iopub.status.busy":"2024-06-11T08:45:23.733531Z","iopub.execute_input":"2024-06-11T08:45:23.733797Z","iopub.status.idle":"2024-06-11T08:45:23.784207Z","shell.execute_reply.started":"2024-06-11T08:45:23.733774Z","shell.execute_reply":"2024-06-11T08:45:23.783338Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = sorted(file_list, key = lambda f: f.split('/')[-1])\n        self.targets = sorted(targets, key=lambda d: d['image_id'])\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        \n        try:\n            image = read_image(self.file_list[idx])    # numpy tensor\n        except RuntimeError as e:\n            Warning(f'Errore con {self.file_list[idx]}')\n            self.targets[idx]['labels'] = torch.tensor([])\n            return None, self.targets[idx]\n        \n        image = F.convert_image_dtype(image)\n        \n      #  print(self.file_list[idx])\n      #  print(self.targets[idx])\n        \n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n        \n        try:\n            label['boxes'] = torch.Tensor(label['boxes'])\n            label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        except IndexError as e:\n            Warning(f'Errore con {idx = }')\n            plt.imshow(image.permute(1, 2, 0))\n            plt.show()\n\n        if self.transform:\n            image, label = self.transform(image, label)\n\n        return image, label\n\nprint('ok')","metadata":{"_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","_kg_hide-input":true,"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","id":"V1Q6ogjksMqE","execution":{"iopub.status.busy":"2024-06-11T08:45:23.785275Z","iopub.execute_input":"2024-06-11T08:45:23.785573Z","iopub.status.idle":"2024-06-11T08:45:23.800283Z","shell.execute_reply.started":"2024-06-11T08:45:23.785550Z","shell.execute_reply":"2024-06-11T08:45:23.799489Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"if not test_only:\n\n    from sklearn.model_selection import train_test_split\n    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n    from torchvision import tv_tensors\n    from torch.utils.data import SubsetRandomSampler\n\n    # DATASET_DIR = os.path.join(\".\")\n    TRAIN_DIR = \"/kaggle/input/airbus-ship-detection/train_v2\"\n    TEST_DIR = \"/kaggle/input/airbus-ship-detection/test_v2\"\n    # print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\n    data_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\n#     train_list, test_list = train_test_split(train_list, test_size = float(1 - dataset_percentage)) # check first cell\n#     train_list, val_list = train_test_split(train_list, test_size = 0.2)\n#     test_list, _ = train_test_split(test_list, test_size = 0.99)\n#     test_list, _ = train_test_split(test_list, test_size = 0.5)\n    \n   # train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/targets-rcnn/rcnn_targets_modified.npy', allow_pickle='TRUE'))\n    ship_dataset = ShipsDataset(data_list, transforms = img_train_transforms, targets=np.load('/kaggle/input/targets-rcnn/rcnn_targets_modified.npy', allow_pickle='TRUE'))\n#     test_data = ShipsDataset(test_list, transforms = img_test_transforms, targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE'))\n#     val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('/kaggle/input/rcnn-dataset-py/rcnn_targets.npy', allow_pickle='TRUE') )\n\n    def custom_collate_fn(batch):\n        # Why custom_collate_fn? Previously, collate_fn when creating a loader was `lambda x: x`\n        # Pickle doesn't pickle function objects. It expects to find the function object by importing its module and looking up its name.\n        # Lambdas are anonymous functions (no name) so that doesn't work. The solution is to name the function at module level.\n\n        return batch\n\n    # Fix the generator for reproducibility, remove once we understand that it works\n   # generator = torch.Generator().manual_seed(42)\n    # Split the dataset into train and test\n #   train_dataset, test_dataset = torch.utils.data.random_split(ship_dataset, [0.7, 0.3], generator)\n#    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n #   test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n#     val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n#     test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True, collate_fn=custom_collate_fn)\n     \n    # Fix the generator for reproducibility, remove once we understand that it works\n    \n    generator = torch.Generator().manual_seed(42)\n    (\n        train_dataset,\n        val_dataset,\n        test_dataset,\n        unused_dataset\n    ) = torch.utils.data.random_split(ship_dataset, [train_percentage, val_percentage, test_percentage, 1 - train_percentage - val_percentage - test_percentage], generator)\n\n    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n    val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, collate_fn=custom_collate_fn)\n     \n    print(\"Whole dataset size:\",len(ship_dataset))\n    print(\"train loader size:\",len(train_loader),f\"({train_percentage}%)\")\n    print(\"validation loader size: \",len(val_loader),f\"({val_percentage}%)\")\n    print(\"test loader size:\",len(test_loader),f\"({test_percentage}%)\")\n    # print(f\"{len(train_loader)} batches for TRAINING, {len(train_idx)} images\")\n    # print(f\"{len(val_loader)} batches for VALIDATION, {len(val_idx)} images\")\n    # print(f\"{len(test_loader)} batches for TEST, {len(test_idx)} images\")\n\n    # logger.info(f\"{len(train_loader)} batches for TRAINING, {len(train_idx)} images\")\n    # logger.info(f\"{len(val_loader)} batches for VALIDATION, {len(val_idx)} images\")\n    # logger.info(f\"{len(test_loader)} batches for TEST, {len(test_idx)} images\")\n\n    # https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n    # La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n    # /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"\n\nprint('ok')","metadata":{"_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","id":"YW9039lzlK5S","execution":{"iopub.status.busy":"2024-06-11T08:45:23.801549Z","iopub.execute_input":"2024-06-11T08:45:23.801800Z","iopub.status.idle":"2024-06-11T08:45:35.125597Z","shell.execute_reply.started":"2024-06-11T08:45:23.801778Z","shell.execute_reply":"2024-06-11T08:45:35.124655Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Whole dataset size: 192556\ntrain loader size: 181 (0.03%)\nvalidation loader size:  61 (0.01%)\ntest loader size: 61 (0.01%)\nok\n","output_type":"stream"}]},{"cell_type":"code","source":" if not test_only:\n    # Save loaders\n    torch.save(train_loader, os.path.join(model_filepath, 'train_loader.pt'))\n    torch.save(val_loader, os.path.join(model_filepath, 'val_loader.pt'))\n    torch.save(test_loader, os.path.join(model_filepath, 'test_loader.pt'))\n\n    print('Dataset Loaders saved succesfully!')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:45:35.129627Z","iopub.execute_input":"2024-06-11T08:45:35.130073Z","iopub.status.idle":"2024-06-11T08:45:40.493340Z","shell.execute_reply.started":"2024-06-11T08:45:35.130039Z","shell.execute_reply":"2024-06-11T08:45:40.492328Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dataset Loaders saved succesfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef get_mean_std(loader):\n    # Compute the mean and standard deviation of all pixels in the dataset  \n    print(\"computing mean and std of this dataset split...\")\n    nimages = 0\n    mean = 0.\n    var = 0.\n    for i, batch in enumerate(loader):\n        inputs = []\n        for el in batch:      \n            inputs.append(el[0])\n        batch = torch.stack(inputs, dim=0)\n        # Rearrange batch to be the shape of [B, C, W * H]\n        batch = batch.view(batch.size(0), batch.size(1), -1)\n        # Update total number of images\n        nimages += batch.size(0)\n        # Compute mean and var\n        mean += batch.mean(2).sum(0) \n        var += batch.var(2).sum(0)\n        \n    mean /= nimages\n    var /= nimages\n    std = torch.sqrt(var)\n    print(\"Done\")\n    \n    return mean, std\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:45:40.494506Z","iopub.execute_input":"2024-06-11T08:45:40.494830Z","iopub.status.idle":"2024-06-11T08:45:40.518184Z","shell.execute_reply.started":"2024-06-11T08:45:40.494804Z","shell.execute_reply":"2024-06-11T08:45:40.517125Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0893, 0.0827, 0.0817]) original size\n# M: tensor([0.1927, 0.2736, 0.3115]) A: tensor([0.0879, 0.0811, 0.0800]) 224x224\n\n# image_mean_train, image_std_train = get_mean_std(train_loader)\n# image_mean_val, image_std_val = get_mean_std(val_loader)\n# image_mean_test, image_std_test = get_mean_std(test_loader)\n\nmatch data_augmentation_type:\n    \n    case 'noaug':\n\n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n    \n    case 'fourier':\n        \n        image_mean_train = torch.Tensor([0.0954, 0.0930, 0.0948])\n        image_std_train = torch.Tensor([0.1039, 0.1016, 0.1036])\n\n        image_mean_val = torch.Tensor([0.1925, 0.2728, 0.3091])\n        image_std_val = torch.Tensor([0.0881, 0.0804, 0.0795])\n\n        image_mean_test = torch.tensor([0.1985, 0.2808, 0.3167])\n        image_std_test = torch.tensor([0.0846, 0.0770, 0.0768])\n    \n    case 'none':\n\n       # image_mean_train, image_std_train = get_mean_std(train_loader)\n        \n        image_mean_train = torch.Tensor([0.1543, 0.2125, 0.2388])\n        image_std_train = torch.Tensor([0.1429, 0.1588, 0.1657])\n\n        image_mean_val = torch.Tensor([0.1541, 0.2128, 0.2395])\n        image_std_val = torch.Tensor([0.1415, 0.1594, 0.1676])\n\n        image_mean_test = torch.tensor([0.2114, 0.2936, 0.3265])\n        image_std_test = torch.tensor([0.0816, 0.0745, 0.0731])\n\nprint(f\"{image_mean_train = }, {image_std_train =}\")\nprint(f\"{image_mean_val = }, {image_std_val =}\")\nprint(f\"{image_mean_test = }, {image_std_test =}\")\nprint(f\"{data_augmentation_type}\")\n\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:45:40.519543Z","iopub.execute_input":"2024-06-11T08:45:40.519951Z","iopub.status.idle":"2024-06-11T08:45:40.598161Z","shell.execute_reply.started":"2024-06-11T08:45:40.519922Z","shell.execute_reply":"2024-06-11T08:45:40.597335Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"image_mean_train = tensor([0.1543, 0.2125, 0.2388]), image_std_train =tensor([0.1429, 0.1588, 0.1657])\nimage_mean_val = tensor([0.1541, 0.2128, 0.2395]), image_std_val =tensor([0.1415, 0.1594, 0.1676])\nimage_mean_test = tensor([0.2114, 0.2936, 0.3265]), image_std_test =tensor([0.0816, 0.0745, 0.0731])\nnoaug\nok\n","output_type":"stream"}]},{"cell_type":"code","source":"def new_model():\n    model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n\n    for module in model_rcnn.backbone.body.modules():\n        if isinstance(module, nn.Conv2d):\n            # Insert batch normalization after convolutional layers\n            module = nn.Sequential(\n                module,\n                nn.BatchNorm2d(module.out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n    for name, param in model_rcnn.named_parameters():\n          param.requires_grad = False\n\n    num_classes = 2 # background, ship\n    in_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\n    model_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model_rcnn\n\nmodel_rcnn = new_model()\n\nprint('ok new model')","metadata":{"_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","id":"5J9M_bnAxnDk","execution":{"iopub.status.busy":"2024-06-11T08:45:40.599387Z","iopub.execute_input":"2024-06-11T08:45:40.600559Z","iopub.status.idle":"2024-06-11T08:45:42.607145Z","shell.execute_reply.started":"2024-06-11T08:45:40.600528Z","shell.execute_reply":"2024-06-11T08:45:42.606208Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 163MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"ok new model\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs, model_name=\"model.tar\"):\n    \"\"\"\n        epoch: last trained epoch\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'training_losses': training_losses,\n        'validation_losses': validation_losses,\n        'lrs': lrs\n    }, os.path.join(model_filepath, model_name))\n    print(\"Saved model\")\n\nprint(\"ok\")","metadata":{"_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","id":"Du5q6_RRCmD4","execution":{"iopub.status.busy":"2024-06-11T08:45:42.608732Z","iopub.execute_input":"2024-06-11T08:45:42.609555Z","iopub.status.idle":"2024-06-11T08:45:42.616523Z","shell.execute_reply.started":"2024-06-11T08:45:42.609515Z","shell.execute_reply":"2024-06-11T08:45:42.615547Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# TRAIN\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import draw_bounding_boxes\n\nif not test_only:\n    \n    from torchvision.models.detection.transform import GeneralizedRCNNTransform\n\n    def train(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs=[], validation_losses=[], training_losses=[], epochs=1, device=torch.device(\"cpu\"), start_from_epoch=0):\n\n        model.transform.image_mean = image_mean_train\n        model.transform.image_std = image_std_train\n        model._skip_resize = True\n\n        for epoch in range(start_from_epoch, epochs): # if start_from_epoch=5, epoch will be [5, 6, 7, ..., epochs-1]\n\n            training_loss = 0.0\n            batch_cumsum = 0\n            model.train()\n            \n            for i, batch in enumerate(train_loader):\n                logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n                print(f\"epoch {epoch} batch {i}\")\n                batch_cumsum += len(batch) # needed to compute the training loss later\n                optimizer.zero_grad()\n                \n                inputs = []\n                targets = []\n                \n                for el in batch:       # el = (image,dict) when transforms are active\n                    \n                    el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                    }\n                    \n                    if not el_dict[\"labels\"].numel():\n                        # filtering out empty images (model does not accept empty targets)\n                        continue\n                    else:\n                      #  print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                      #  print(f'el_dict has {el_dict[\"boxes\"] = }')\n                        \n                        image = el[0].to(device)\n                        el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n                        \n                        inputs.append(image)\n                        targets.append(el_dict)\n                        \n                        # Print images during training\n                        if print_images_during_training:\n                            num = len(el_dict[\"boxes\"])\n\n                            img = draw_bounding_boxes(\n                                (image*256).byte(),\n                                el_dict[\"boxes\"]*224,\n                                width = 1,\n                                colors = 'yellow',\n                                # font='arial',\n                                font_size = 15\n                            )\n\n                            fig, ax = plt.subplots()\n                            fig.set_size_inches(16,9)\n                            fig.tight_layout(pad=5)\n                            ax.imshow(img.byte().permute(1, 2, 0))\n                            plt.show()\n                            plt.close()\n\n                        # print(f\"{el = }\")\n                        # Example el\n                        # el = (tensor([[[0.1006, 0.1249, 0.1552,  ..., 0.1552, 0.1395, 0.1321],\n                        #          [0.1224, 0.1331, 0.1243,  ..., 0.1218, 0.1260, 0.1410],\n                        #          [0.0948, 0.1149, 0.1300,  ..., 0.1381, 0.1356, 0.1356],\n                        #          ...,\n                        #          [0.1789, 0.1738, 0.1818,  ..., 0.1401, 0.1428, 0.1169],\n                        #          [0.1591, 0.1532, 0.1752,  ..., 0.1555, 0.1481, 0.1131],\n                        #          [0.1664, 0.1698, 0.1564,  ..., 0.1268, 0.1538, 0.1393]],\n\n                        #         [[0.2291, 0.2504, 0.2689,  ..., 0.2807, 0.2650, 0.2576],\n                        #          [0.2510, 0.2586, 0.2380,  ..., 0.2473, 0.2515, 0.2664],\n                        #          [0.2234, 0.2404, 0.2437,  ..., 0.2636, 0.2611, 0.2611],\n                        #          ...,\n                        #          [0.2966, 0.2914, 0.2995,  ..., 0.2460, 0.2486, 0.2228],\n                        #          [0.2768, 0.2709, 0.2928,  ..., 0.2613, 0.2540, 0.2190],\n                        #          [0.2840, 0.2874, 0.2741,  ..., 0.2327, 0.2596, 0.2452]],\n\n                        #         [[0.2880, 0.3092, 0.3317,  ..., 0.3396, 0.3238, 0.3164],\n                        #          [0.3098, 0.3174, 0.3007,  ..., 0.3062, 0.3103, 0.3253],\n                        #          [0.2822, 0.2993, 0.3064,  ..., 0.3224, 0.3199, 0.3199],\n                        #          ...,\n                        #          [0.3358, 0.3306, 0.3387,  ..., 0.2813, 0.2918, 0.2659],\n                        #          [0.3160, 0.3101, 0.3320,  ..., 0.2966, 0.2971, 0.2622],\n                        #          [0.3232, 0.3266, 0.3133,  ..., 0.2680, 0.3028, 0.2883]]]), {'boxes': tensor([[0.3932, 0.8464, 0.5208, 0.8776],\n                        #         [0.2331, 0.2643, 0.3268, 0.3060],\n                        #         [0.2435, 0.2995, 0.4062, 0.3724],\n                        #         [0.7188, 0.6198, 0.8281, 0.6784],\n                        #         [0.2279, 0.3229, 0.4154, 0.4128]]), 'labels': tensor([1, 1, 1, 1, 1])})\n\n                if len(inputs) == 0:\n                    continue\n\n                output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n                \"\"\" EXAMPLE :\n                    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n                     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n                     'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)}\n\n                     How losses are computed:\n\n                     -loss_classifier-\n                     classification_loss = F.cross_entropy(class_logits, labels)\n\n                     -loss_box_reg-\n                     box_loss = F.smooth_l1_loss(\n                        box_regression[sampled_pos_inds_subset, labels_pos],\n                        regression_targets[sampled_pos_inds_subset],\n                        beta=1 / 9,\n                        reduction=\"sum\",\n                    )\n                    box_loss = box_loss / labels.numel()\n\n                    -loss_rpn_box_reg-\n                    box_loss = F.smooth_l1_loss(\n                    pred_bbox_deltas[sampled_pos_inds],\n                    regression_targets[sampled_pos_inds],\n                    beta=1 / 9,\n                    reduction=\"sum\",\n                    ) / (sampled_inds.numel())\n\n                    -loss_objectness-\n                    objectness_loss = F.binary_cross_entropy_with_logits(objectness[sampled_inds], labels[sampled_inds])\n\n                 \"\"\"\n\n                loss = sum(loss for loss in output.values())\n                loss.backward()\n                optimizer.step()\n                training_loss += loss.data.item() * len(batch)\n\n            lrs.append(scheduler.get_last_lr()[0]) # append learning rate before changing\n            scheduler.step() # changes LR\n            training_loss /= batch_cumsum\n            training_losses.append(training_loss)\n            # save_checkpoint(epoch, model, optimizer, scheduler, training_loss, lrs)\n\n            # VALIDATION\n            model.transform.image_mean = image_mean_val\n            model.transform.image_std = image_std_val\n\n            model.train()\n            num_correct = 0\n            num_examples = 0\n            valid_loss = 0\n\n            with torch.no_grad():\n                for i,batch in enumerate(val_loader):\n                    print(\"batch\", i)\n                    inputs = []\n                    targets = []\n\n                    for el in batch:       # el = (image,labels)\n                        \n                        el_dict = {\n                        \"boxes\": el[1][\"boxes\"].to(device),\n                        \"labels\": el[1][\"labels\"].to(device)\n                        }\n\n                        if not el_dict[\"labels\"].numel():\n                            # filtering out empty images (model does not accept empty targets)\n                            continue\n                        else:\n                        #    print(f'el_dict has {el_dict[\"labels\"].numel()} bboxes')\n                        #    print(f'el_dict has {el_dict[\"boxes\"] = }')\n\n                            image = el[0].to(device)\n                            el_dict = {\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)}\n\n                            inputs.append(image)\n                            targets.append(el_dict)\n            \n                        \n                        '''\n                        if el[1]['boxes'].size()[0] != 0:\n                            inputs.append(el[0].to(device))\n                            targets.append({\"boxes\": el[1][\"boxes\"].to(device),\"labels\": el[1][\"labels\"].to(device)})\n                        '''\n                   \n                    if len(inputs) == 0:\n                        continue\n\n                    output = model(inputs, targets)\n\n                    loss = sum(loss for loss in output.values())\n                    valid_loss += loss.data.item() *len(batch)\n\n            valid_loss /= len(val_loader.dataset)\n            validation_losses.append(valid_loss)\n\n            print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, lr: {:.8f}'.format(epoch, training_loss,\n            valid_loss, lrs[-1]))\n\n            save_checkpoint(model, epoch, optimizer, scheduler, training_losses, validation_losses, lrs)\n        \n\n# from torchvision.utils import draw_bounding_boxes\n# score_threshold = .5\nprint('ok')","metadata":{"_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","id":"Mv8b06EulUK2","execution":{"iopub.status.busy":"2024-06-11T08:45:42.618108Z","iopub.execute_input":"2024-06-11T08:45:42.618697Z","iopub.status.idle":"2024-06-11T08:45:42.648272Z","shell.execute_reply.started":"2024-06-11T08:45:42.618665Z","shell.execute_reply":"2024-06-11T08:45:42.647372Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"#### START MODEL TRAINING\n\nif not test_only:\n    \n    model = new_model()\n    model.to(device)\n    torch.compile(model)\n    optimizer = optim.Adam(params = model.parameters(), lr = init_lr, weight_decay=0.01)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    logger.info(f\"Beginning training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"Beginning training, {num_epochs = }, {data_augmentation_type = }, {batch_size = }\")\n    print(f\"{device = }\")\n    \n    train(model, optimizer, scheduler, criterion, train_loader, val_loader, epochs=num_epochs, device=device)\n    \n    # plots\n#     fig, ax = plt.subplots()\n#     ax.plot(lrs)    \n#     ax.set(xlabel='epoch', ylabel='learning rate value')\n#     fig.savefig(os.path.join(model_filepath, \"lrs.png\"))\n#     print(f\"{lrs = }\")\n#     logger.info(f\"{lrs = }\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T08:45:42.649498Z","iopub.execute_input":"2024-06-11T08:45:42.649827Z","iopub.status.idle":"2024-06-11T09:30:18.039674Z","shell.execute_reply.started":"2024-06-11T08:45:42.649797Z","shell.execute_reply":"2024-06-11T09:30:18.037592Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Beginning training, num_epochs = 10, data_augmentation_type = 'noaug', batch_size = 32\ndevice = device(type='cuda')\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"epoch 0 batch 0\nepoch 0 batch 1\nepoch 0 batch 2\nepoch 0 batch 3\nepoch 0 batch 4\nepoch 0 batch 5\nepoch 0 batch 6\nepoch 0 batch 7\nepoch 0 batch 8\nepoch 0 batch 9\nepoch 0 batch 10\nepoch 0 batch 11\nepoch 0 batch 12\nepoch 0 batch 13\nepoch 0 batch 14\nepoch 0 batch 15\nepoch 0 batch 16\nepoch 0 batch 17\nepoch 0 batch 18\nepoch 0 batch 19\nepoch 0 batch 20\nepoch 0 batch 21\nepoch 0 batch 22\nepoch 0 batch 23\nepoch 0 batch 24\nepoch 0 batch 25\nepoch 0 batch 26\nepoch 0 batch 27\nepoch 0 batch 28\nepoch 0 batch 29\nepoch 0 batch 30\nepoch 0 batch 31\nepoch 0 batch 32\nepoch 0 batch 33\nepoch 0 batch 34\nepoch 0 batch 35\nepoch 0 batch 36\nepoch 0 batch 37\nepoch 0 batch 38\nepoch 0 batch 39\nepoch 0 batch 40\nepoch 0 batch 41\nepoch 0 batch 42\nepoch 0 batch 43\nepoch 0 batch 44\nepoch 0 batch 45\nepoch 0 batch 46\nepoch 0 batch 47\nepoch 0 batch 48\nepoch 0 batch 49\nepoch 0 batch 50\nepoch 0 batch 51\nepoch 0 batch 52\nepoch 0 batch 53\nepoch 0 batch 54\nepoch 0 batch 55\nepoch 0 batch 56\nepoch 0 batch 57\nepoch 0 batch 58\nepoch 0 batch 59\nepoch 0 batch 60\nepoch 0 batch 61\nepoch 0 batch 62\nepoch 0 batch 63\nepoch 0 batch 64\nepoch 0 batch 65\nepoch 0 batch 66\nepoch 0 batch 67\nepoch 0 batch 68\nepoch 0 batch 69\nepoch 0 batch 70\nepoch 0 batch 71\nepoch 0 batch 72\nepoch 0 batch 73\nepoch 0 batch 74\nepoch 0 batch 75\nepoch 0 batch 76\nepoch 0 batch 77\nepoch 0 batch 78\nepoch 0 batch 79\nepoch 0 batch 80\nepoch 0 batch 81\nepoch 0 batch 82\nepoch 0 batch 83\nepoch 0 batch 84\nepoch 0 batch 85\nepoch 0 batch 86\nepoch 0 batch 87\nepoch 0 batch 88\nepoch 0 batch 89\nepoch 0 batch 90\nepoch 0 batch 91\nepoch 0 batch 92\nepoch 0 batch 93\nepoch 0 batch 94\nepoch 0 batch 95\nepoch 0 batch 96\nepoch 0 batch 97\nepoch 0 batch 98\nepoch 0 batch 99\nepoch 0 batch 100\nepoch 0 batch 101\nepoch 0 batch 102\nepoch 0 batch 103\nepoch 0 batch 104\nepoch 0 batch 105\nepoch 0 batch 106\nepoch 0 batch 107\nepoch 0 batch 108\nepoch 0 batch 109\nepoch 0 batch 110\nepoch 0 batch 111\nepoch 0 batch 112\nepoch 0 batch 113\nepoch 0 batch 114\nepoch 0 batch 115\nepoch 0 batch 116\nepoch 0 batch 117\nepoch 0 batch 118\nepoch 0 batch 119\nepoch 0 batch 120\nepoch 0 batch 121\nepoch 0 batch 122\nepoch 0 batch 123\nepoch 0 batch 124\nepoch 0 batch 125\nepoch 0 batch 126\nepoch 0 batch 127\nepoch 0 batch 128\nepoch 0 batch 129\nepoch 0 batch 130\nepoch 0 batch 131\nepoch 0 batch 132\nepoch 0 batch 133\nepoch 0 batch 134\nepoch 0 batch 135\nepoch 0 batch 136\nepoch 0 batch 137\nepoch 0 batch 138\nepoch 0 batch 139\nepoch 0 batch 140\nepoch 0 batch 141\nepoch 0 batch 142\nepoch 0 batch 143\nepoch 0 batch 144\nepoch 0 batch 145\nepoch 0 batch 146\nepoch 0 batch 147\nepoch 0 batch 148\nepoch 0 batch 149\nepoch 0 batch 150\nepoch 0 batch 151\nepoch 0 batch 152\nepoch 0 batch 153\nepoch 0 batch 154\nepoch 0 batch 155\nepoch 0 batch 156\nepoch 0 batch 157\nepoch 0 batch 158\nepoch 0 batch 159\nepoch 0 batch 160\nepoch 0 batch 161\nepoch 0 batch 162\nepoch 0 batch 163\nepoch 0 batch 164\nepoch 0 batch 165\nepoch 0 batch 166\nepoch 0 batch 167\nepoch 0 batch 168\nepoch 0 batch 169\nepoch 0 batch 170\nepoch 0 batch 171\nepoch 0 batch 172\nepoch 0 batch 173\nepoch 0 batch 174\nepoch 0 batch 175\nepoch 0 batch 176\nepoch 0 batch 177\nepoch 0 batch 178\nepoch 0 batch 179\nepoch 0 batch 180\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nbatch 46\nbatch 47\nbatch 48\nbatch 49\nbatch 50\nbatch 51\nbatch 52\nbatch 53\nbatch 54\nbatch 55\nbatch 56\nbatch 57\nbatch 58\nbatch 59\nbatch 60\nEpoch: 0, Training Loss: 4.7482, Validation Loss: 4.6652, lr: 0.00010000\nSaved model\nepoch 1 batch 0\nepoch 1 batch 1\nepoch 1 batch 2\nepoch 1 batch 3\nepoch 1 batch 4\nepoch 1 batch 5\nepoch 1 batch 6\nepoch 1 batch 7\nepoch 1 batch 8\nepoch 1 batch 9\nepoch 1 batch 10\nepoch 1 batch 11\nepoch 1 batch 12\nepoch 1 batch 13\nepoch 1 batch 14\nepoch 1 batch 15\nepoch 1 batch 16\nepoch 1 batch 17\nepoch 1 batch 18\nepoch 1 batch 19\nepoch 1 batch 20\nepoch 1 batch 21\nepoch 1 batch 22\nepoch 1 batch 23\nepoch 1 batch 24\nepoch 1 batch 25\nepoch 1 batch 26\nepoch 1 batch 27\nepoch 1 batch 28\nepoch 1 batch 29\nepoch 1 batch 30\nepoch 1 batch 31\nepoch 1 batch 32\nepoch 1 batch 33\nepoch 3 batch 119\nepoch 3 batch 120\nepoch 3 batch 121\nepoch 3 batch 122\nepoch 3 batch 123\nepoch 3 batch 124\nepoch 3 batch 125\nepoch 3 batch 126\nepoch 3 batch 127\nepoch 3 batch 128\nepoch 3 batch 129\nepoch 3 batch 130\nepoch 3 batch 131\nepoch 3 batch 132\nepoch 3 batch 133\nepoch 3 batch 134\nepoch 3 batch 135\nepoch 3 batch 136\nepoch 3 batch 137\nepoch 3 batch 138\nepoch 3 batch 139\nepoch 3 batch 140\nepoch 3 batch 141\nepoch 3 batch 142\nepoch 3 batch 143\nepoch 3 batch 144\nepoch 3 batch 145\nepoch 3 batch 146\nepoch 3 batch 147\nepoch 3 batch 148\nepoch 3 batch 149\nepoch 3 batch 150\nepoch 3 batch 151\nepoch 3 batch 152\nepoch 3 batch 153\nepoch 3 batch 154\nepoch 3 batch 155\nepoch 3 batch 156\nepoch 3 batch 157\nepoch 3 batch 158\nepoch 3 batch 159\nepoch 3 batch 160\nepoch 3 batch 161\nepoch 3 batch 162\nepoch 3 batch 163\nepoch 3 batch 164\nepoch 3 batch 165\nepoch 3 batch 166\nepoch 3 batch 167\nepoch 3 batch 168\nepoch 3 batch 169\nepoch 3 batch 170\nepoch 3 batch 171\nepoch 3 batch 172\nepoch 3 batch 173\nepoch 3 batch 174\nepoch 3 batch 175\nepoch 3 batch 176\nepoch 3 batch 177\nepoch 3 batch 178\nepoch 3 batch 179\nepoch 3 batch 180\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nbatch 46\nbatch 47\nbatch 48\nbatch 49\nbatch 50\nbatch 51\nbatch 52\nbatch 53\nbatch 54\nbatch 55\nbatch 56\nbatch 57\nbatch 58\nbatch 59\nbatch 60\nEpoch: 3, Training Loss: 4.6624, Validation Loss: 4.6480, lr: 0.00010000\nSaved model\nepoch 4 batch 0\nepoch 4 batch 1\nepoch 4 batch 2\nepoch 4 batch 3\nepoch 4 batch 4\nepoch 4 batch 5\nepoch 4 batch 6\nepoch 4 batch 7\nepoch 4 batch 8\nepoch 4 batch 9\nepoch 4 batch 10\nepoch 4 batch 11\nepoch 4 batch 12\nepoch 4 batch 13\nepoch 4 batch 14\nepoch 4 batch 15\nepoch 4 batch 16\nepoch 4 batch 17\nepoch 4 batch 18\nepoch 4 batch 19\nepoch 4 batch 20\nepoch 4 batch 21\nepoch 4 batch 22\nepoch 4 batch 23\nepoch 4 batch 24\nepoch 4 batch 25\nepoch 4 batch 26\nepoch 4 batch 27\nepoch 4 batch 28\nepoch 4 batch 29\nepoch 4 batch 30\nepoch 4 batch 31\nepoch 4 batch 32\nepoch 4 batch 33\nepoch 4 batch 34\nepoch 4 batch 35\nepoch 4 batch 36\nepoch 4 batch 37\nepoch 4 batch 38\nepoch 4 batch 39\nepoch 4 batch 40\nepoch 4 batch 41\nepoch 4 batch 42\nepoch 4 batch 43\nepoch 4 batch 44\nepoch 4 batch 45\nepoch 4 batch 46\nepoch 4 batch 47\nepoch 4 batch 48\nepoch 4 batch 49\nepoch 4 batch 50\nepoch 4 batch 51\nepoch 4 batch 52\nepoch 4 batch 53\nepoch 4 batch 54\nepoch 4 batch 55\nepoch 4 batch 56\nepoch 4 batch 57\nepoch 4 batch 58\nepoch 4 batch 59\nepoch 4 batch 60\nepoch 4 batch 61\nepoch 4 batch 62\nepoch 4 batch 63\nepoch 4 batch 64\nepoch 4 batch 65\nepoch 4 batch 66\nepoch 4 batch 67\nepoch 4 batch 68\nepoch 4 batch 69\nepoch 4 batch 70\nepoch 4 batch 71\nepoch 4 batch 72\nepoch 4 batch 73\nepoch 4 batch 74\nepoch 4 batch 75\nepoch 4 batch 76\nepoch 4 batch 77\nepoch 4 batch 78\nepoch 4 batch 79\nepoch 4 batch 80\nepoch 4 batch 81\nepoch 4 batch 82\nepoch 4 batch 83\nepoch 4 batch 84\nepoch 4 batch 85\nepoch 4 batch 86\nepoch 4 batch 87\nepoch 4 batch 88\nepoch 4 batch 89\nepoch 4 batch 90\nepoch 4 batch 91\nepoch 4 batch 92\nepoch 4 batch 93\nepoch 4 batch 94\nepoch 4 batch 95\nepoch 4 batch 96\nepoch 4 batch 97\nepoch 4 batch 98\nepoch 4 batch 99\nepoch 4 batch 100\nepoch 4 batch 101\nepoch 4 batch 102\nepoch 4 batch 103\nepoch 4 batch 104\nepoch 4 batch 105\nepoch 4 batch 106\nepoch 4 batch 107\nepoch 4 batch 108\nepoch 4 batch 109\nepoch 4 batch 110\nepoch 4 batch 111\nepoch 4 batch 112\nepoch 4 batch 113\nepoch 4 batch 114\nepoch 4 batch 115\nepoch 4 batch 116\nepoch 4 batch 117\nepoch 4 batch 118\nepoch 4 batch 119\nepoch 4 batch 120\nepoch 4 batch 121\nepoch 4 batch 122\nepoch 4 batch 123\nepoch 4 batch 124\nepoch 4 batch 125\nepoch 4 batch 126\nepoch 4 batch 127\nepoch 4 batch 128\nepoch 4 batch 129\nepoch 4 batch 130\nepoch 4 batch 131\nepoch 4 batch 132\nepoch 4 batch 133\nepoch 4 batch 134\nepoch 4 batch 135\nepoch 4 batch 136\nepoch 4 batch 137\nepoch 4 batch 138\nepoch 4 batch 139\nepoch 4 batch 140\nepoch 4 batch 141\nepoch 4 batch 142\nepoch 4 batch 143\nepoch 4 batch 144\nepoch 4 batch 145\nepoch 4 batch 146\nepoch 4 batch 147\nepoch 4 batch 148\nepoch 4 batch 149\nepoch 4 batch 150\nepoch 4 batch 151\nepoch 4 batch 152\nepoch 4 batch 153\nepoch 4 batch 154\nepoch 4 batch 155\nepoch 4 batch 156\nepoch 4 batch 157\nepoch 4 batch 158\nepoch 4 batch 159\nepoch 4 batch 160\nepoch 4 batch 161\nepoch 4 batch 162\nepoch 4 batch 163\nepoch 4 batch 164\nepoch 4 batch 165\nepoch 4 batch 166\nepoch 4 batch 167\nepoch 4 batch 168\nepoch 4 batch 169\nepoch 4 batch 170\nepoch 4 batch 171\nepoch 4 batch 172\nepoch 4 batch 173\nepoch 4 batch 174\nepoch 4 batch 175\nepoch 4 batch 176\nepoch 4 batch 177\nepoch 4 batch 178\nepoch 4 batch 179\nepoch 4 batch 180\nbatch 0\nbatch 1\nbatch 2\nbatch 3\nbatch 4\nbatch 5\nbatch 6\nbatch 7\nbatch 8\nbatch 9\nbatch 10\nbatch 11\nbatch 12\nbatch 13\nbatch 14\nbatch 15\nbatch 16\nbatch 17\nbatch 18\nbatch 19\nbatch 20\nbatch 21\nbatch 22\nbatch 23\nbatch 24\nbatch 25\nbatch 26\nbatch 27\nbatch 28\nbatch 29\nbatch 30\nbatch 31\nbatch 32\nbatch 33\nbatch 34\nbatch 35\nbatch 36\nbatch 37\nbatch 38\nbatch 39\nbatch 40\nbatch 41\nbatch 42\nbatch 43\nbatch 44\nbatch 45\nbatch 46\nbatch 47\nbatch 48\nbatch 49\nbatch 50\nbatch 51\nbatch 52\nbatch 53\nbatch 54\nbatch 55\nbatch 56\nbatch 57\nbatch 58\nbatch 59\nbatch 60\nEpoch: 4, Training Loss: 4.6557, Validation Loss: 4.6382, lr: 0.00010000\nSaved model\nepoch 5 batch 0\nepoch 5 batch 1\n","output_type":"stream"},{"name":"stderr","text":"Process ForkProcess-3:\nProcess ForkProcess-2:\nProcess ForkProcess-1:\nProcess ForkProcess-4:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/opt/conda/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n    call_item = call_queue.get(block=True)\n  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n","output_type":"stream"},{"name":"stdout","text":"epoch 5 batch 2\n","output_type":"stream"},{"name":"stderr","text":"  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n    res = self._recv_bytes()\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning training, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_augmentation_type\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# plots\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     fig, ax = plt.subplots()\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     ax.plot(lrs)    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     print(f\"{lrs = }\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     logger.info(f\"{lrs = }\")\u001b[39;00m\n","Cell \u001b[0;32mIn[15], line 104\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, loss_fn, train_loader, val_loader, lrs, validation_losses, training_losses, epochs, device, start_from_epoch)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# NOTE: output is a dict with already computed losses within!\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" EXAMPLE :\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m     'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m \"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mvalues())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:370\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    366\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    372\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_proposals(proposals, objectness, images\u001b[38;5;241m.\u001b[39mimage_sizes, num_anchors_per_level)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    177\u001b[0m     rel_codes \u001b[38;5;241m=\u001b[39m rel_codes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    180\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    213\u001b[0m pred_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(dh) \u001b[38;5;241m*\u001b[39m heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m c_to_c_h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_ctr_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m pred_h\n\u001b[1;32m    217\u001b[0m c_to_c_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpred_ctr_x\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mpred_w\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m pred_w\n\u001b[1;32m    219\u001b[0m pred_boxes1 \u001b[38;5;241m=\u001b[39m pred_ctr_x \u001b[38;5;241m-\u001b[39m c_to_c_w\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Plot data\n\n# if plot_data:\n    \n#     import matplotlib.pyplot as plt\n    \n#     pass","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:30:18.041004Z","iopub.status.idle":"2024-06-11T09:30:18.041535Z","shell.execute_reply.started":"2024-06-11T09:30:18.041266Z","shell.execute_reply":"2024-06-11T09:30:18.041291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom torchmetrics.detection import MeanAveragePrecision\nimport torchvision.transforms.functional as F\n\ndef test(model, test_loader, device=torch.device(\"cpu\")): \n    \n    model.transform.image_mean  = image_mean_test\n    model.transform.image_std = image_std_test\n    model._skip_resize = True\n    \n    model.eval()\n    num_correct = 0\n    num_examples = 0\n    test_loss = 0\n    metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.75])\n    mAP = 0\n    \n    for i,batch in enumerate(test_loader):\n        print(\"batch\", i)\n        \n        inputs = []\n        targets = []\n        \n        for el in batch:       # el = (image,dict)\n            if el[0] != None and not el[1]['labels'].numel() :\n                inputs.append(el[0].to(device))\n                targets.append(el[1])\n                \n        if len(inputs) == 0:\n            continue\n        \n        output = model(inputs)\n        # print(type(model(torch.cuda.FloatTensor(inputs))))\n#         print(\"out :\\n\", output)\n#         print(\"target :\\n\",targets)\n        #     # Example output\n        #     {'boxes': tensor([[\n        #       0.3801,  0.3060,  3.5638,  3.0348],\n        #     [ 0.2037,  0.6570,  1.9538,  4.9389],\n        #     [ 0.4993,  0.7045,  5.1531,  5.5368],\n        #     [ 0.7172,  0.0860,  8.0819,  3.2724],\n        #     [ 0.3548,  1.4842,  3.9183,  9.8673],\n        #     [ 0.9226,  0.4096, 11.7943,  6.0310]], grad_fn=<StackBackward0>),\n        #     labels': tensor([1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9762, 0.9498, 0.9188, 0.8941, 0.3722, 0.2909],\n        #     grad_fn=<IndexBackward0>)},\n        \n        \"\"\"\n        scores come from RoIHeads class:\n        pred_scores = F.softmax(class_logits, -1)\n        after deleting empy boxes, low scored boxes and applying non-max suppression\n        \"\"\"\n        \n        for dic in output:\n            dic[\"boxes\"] = dic[\"boxes\"].to(device)\n            dic[\"labels\"] = dic[\"labels\"].to(device)\n            dic[\"scores\"] = dic[\"scores\"].to(device)\n            \n        res = metric(output,targets)\n        mAP += res['map_75']\n        #print(res)\n\n        \n    mAP /= len(test_loader)  \n    print( 'Mean Average Precision: {:.4f}'.format(mAP))\n\nprint(\"ok\")","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:30:18.043464Z","iopub.status.idle":"2024-06-11T09:30:18.043801Z","shell.execute_reply.started":"2024-06-11T09:30:18.043637Z","shell.execute_reply":"2024-06-11T09:30:18.043651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# START MODEL TEST\n\nif do_model_test or test_only:\n#     checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # LUDO\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device) # ENF\n    test_loader = torch.load(os.path.join(model_filepath, \"test_loader.pt\"), map_location=device)\n    \n    model = new_model()\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print(\"model is now using cuda\")\n\n    test(model.to(device), test_loader, device)\n\n# checkpoint = torch.load(os.path.join('/kaggle/input/model-trained', \"model.tar\")) # ludo\n# #checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\")) # enf\n# model.load_state_dict(checkpoint['model_state_dict'])\n# test(model.to(device), test_loader, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:30:18.045273Z","iopub.status.idle":"2024-06-11T09:30:18.045674Z","shell.execute_reply.started":"2024-06-11T09:30:18.045487Z","shell.execute_reply":"2024-06-11T09:30:18.045503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN AGAIN (Continue training)\n\nimport pickle\n\nif train_again:    \n    # Load loaders\n    train_loader = torch.load(os.path.join(model_filepath, 'train_loader.pt'), map_location=device)\n    val_loader = torch.load(os.path.join(model_filepath, 'val_loader.pt'), map_location=device)\n    test_loader = torch.load(os.path.join(model_filepath, 'test_loader.pt'), map_location=device)\n    print(\"Loadeders and model loaded succesfully\") \n    \n#     print(f\"{device = }\")\n    \n    model = new_model()\n    \n    # Load model from checkpoint\n    checkpoint = torch.load(os.path.join(model_filepath, \"model.tar\"), map_location=device)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    \n    optimizer = optim.Adam(params = model.parameters(), weight_decay=0.01, lr = 1e-2)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer,\n        gamma = 0.9,\n        step_size = 5,\n    )\n    \n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    training_losses = checkpoint['training_losses']\n    validation_losses = checkpoint['validation_losses']\n    lrs = checkpoint['lrs']\n    epoch = checkpoint['epoch']\n    # Resume training from a specific epoch\n    # optimizer = optim.Adam(params = model.parameters(), lr=0.01)\n    \n    # Il file salvato model.tar contiene optiimzer, scheduler, loss e tanto altro\n\n    logger.info(f\"Continuing training, {num_epochs = }, {device = }, {data_augmentation_type = }, {batch_size = }\")\n    train(model, optimizer, scheduler, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:30:18.047406Z","iopub.status.idle":"2024-06-11T09:30:18.047836Z","shell.execute_reply.started":"2024-06-11T09:30:18.047611Z","shell.execute_reply":"2024-06-11T09:30:18.047629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n\n\nsimo e' intelligente\n\nsimo e' intelligente, \n\nsimo e' buono, ludo e' bella,\nenf e' attraente, simo e' buonosimo e' intelligente, \nludo e' intelligente\n\nsimo e' intelligente\nludo e' buona\n\nenf e' attraente\n\nludo e' buona, \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{}}]}