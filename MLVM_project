{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# from google.colab import files\n\n# MAIN CONFIGURATIONS\ncreate_log_file = True\nsave_to_drive = False\nmodel_id = '0'  # We will train multiple models with the same settings. Keep it as a string!\nnum_epochs = 1\nbatch_size = 32\ndata_augmentation_type = 'noaug'  # Which data augmentation tecnique are we using?\n                                  # 'noaug':     no data augmentation\n\n# Save files to your google drive :)\n# This will enable the automatic saving of 1) the models 2) the logs\n\nif save_to_drive:\n  from google.colab import drive\n  drive.mount('/content/drive', force_remount=True)\n  os.makedirs('/content/drive/MyDrive/MLVM_project/', exist_ok=True)","metadata":{"_uuid":"f66e8f23-9459-4a78-b42d-592a8b72fd93","_cell_guid":"1eb3601c-59c5-4c49-8c24-91184a27a7e1","collapsed":false,"id":"NFc7Y_31k39Q","outputId":"13bf00d1-bde8-49aa-b24b-2e49f08ab1ab","execution":{"iopub.status.busy":"2024-05-11T19:32:17.800803Z","iopub.execute_input":"2024-05-11T19:32:17.801192Z","iopub.status.idle":"2024-05-11T19:32:17.807121Z","shell.execute_reply.started":"2024-05-11T19:32:17.801164Z","shell.execute_reply":"2024-05-11T19:32:17.806012Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.exists(\"kaggle.json\"): # non rompe i coglioni se già c'è\n  files.upload()    # upload your kaggle.json file with your login credentials (profile -> settings -> create new token)\n  !mkdir -p ~/.kaggle\n  !cp kaggle.json ~/.kaggle/\n  !chmod 600 ~/.kaggle/kaggle.json\n\n# UPLOAD 'rcnn_targets.npy' AS WELL! We're gonna use it later","metadata":{"_uuid":"ef6747a7-b0e5-45c6-9df8-03e8489afd75","_cell_guid":"42d6594c-0e59-45bd-93fa-b916e1be909d","collapsed":false,"id":"qEEdOAaiNhm3","execution":{"iopub.status.busy":"2024-05-11T19:32:24.611199Z","iopub.execute_input":"2024-05-11T19:32:24.611572Z","iopub.status.idle":"2024-05-11T19:32:24.641295Z","shell.execute_reply.started":"2024-05-11T19:32:24.611545Z","shell.execute_reply":"2024-05-11T19:32:24.640105Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions download -c airbus-ship-detection    # make sure you've accepted competition rules! (competition -> rules -> understand and accept)\n!unzip /content/airbus-ship-detection.zip","metadata":{"_uuid":"69d675b0-03e2-484f-92ee-9cd76bc29438","_cell_guid":"5712c7c4-362a-4c8c-b92e-511837b11ab8","collapsed":false,"id":"1MZ4x531k5pf","outputId":"7da22d51-7495-4ced-cddc-1f12413a5d34","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"b7ed3fcb-a4dc-4ac6-af5d-3a6ce6e8a1fb","_cell_guid":"26b9f378-6f8f-4afb-a4bb-d93ddcd10282","id":"aKTG9DX1XinP","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport glob\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torchvision.transforms.functional import rotate\nfrom torchvision.utils import draw_segmentation_masks\nfrom torchvision.ops import masks_to_boxes\nimport numpy as np\n\n!pip install torchsummary\nfrom torchsummary import summary","metadata":{"_uuid":"3a8323c1-3053-4c12-9ade-4fb84ebf9533","_cell_guid":"f18fd6df-1d38-488c-8322-fe053a7d6424","collapsed":false,"id":"kA71_KIbk72N","outputId":"4c825fdc-4725-4bc9-9f3f-c7f07e6431dd","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_filepath = f\"model_epochs{str(num_epochs)}_{data_augmentation_type}_id{model_id}\"\nmodel_filepath = os.path.join(\"drive\", \"MyDrive\", \"MLVM_project\", model_filepath)\nprint(model_filepath)\nos.makedirs(model_filepath, exist_ok=True)","metadata":{"_uuid":"948ae697-15e4-49eb-a3ee-7f95f4bdd20a","_cell_guid":"e919b6b7-2791-42ab-8f0d-7880affca507","collapsed":false,"id":"e311_SGllCN6","outputId":"450b5cd7-9f9a-4085-cb30-f1f22deb7a65","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\n\nimg_dimensions = 224\n\n\nif save_to_drive and create_log_file:\n    logger = logging.getLogger('RootLogger')\n    log_filepath = datetime.now().strftime(\"%m-%d_%H.%M.%S\")\n    log_filepath = os.path.join(model_filepath, f\"log_{log_filepath}\" + \".txt\")\n    print(log_filepath)\n    # with open(log_filepath, 'a') as f:\n    #   f.write(f'Initialized log at {datetime.now().strftime(\"%m-%d_%H:%M:%S\")}\\n')\n      # print('written successfully')\n\n    from pathlib import Path\n    print(Path.cwd())\n\n    logging.basicConfig(filename=log_filepath,\n                        filemode='a',\n                        format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)\nelse:\n    logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s',\n                        level=logging.DEBUG,\n                        datefmt='%m-%d %H:%M:%S',\n                        force=True)","metadata":{"_uuid":"e83dab63-8809-4b35-b264-4f1469682d4e","_cell_guid":"cefe5666-0a12-42de-90e4-9e6bfeeab010","collapsed":false,"id":"03slXsApk-6S","outputId":"30d3206d-bb54-495f-9e71-22ea77ee8cd0","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## TRANSFORMATIONS\n\n# # Normalize to the ImageNet mean and standard deviation\n# # Could calculate it for the cats/dogs data set, but the ImageNet\n# # values give acceptable results here.\n# img_train_transforms = transforms.Compose([\n#     transforms.RandomRotation(50),\n#     transforms.RandomAffine(degrees = 0, translate = (0.2, 0.2)),\n#     transforms.RandomHorizontalFlip(p=0.5),\n#     transforms.Resize((img_dimensions, img_dimensions)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n#     ])\n\n# img_validation_transforms = transforms.Compose([\n#     transforms.Resize((img_dimensions,img_dimensions)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225] )\n#     ])","metadata":{"_uuid":"86622d71-08ab-4f4a-8f6b-d8d20a66ee67","_cell_guid":"02a5ff4a-6f79-47fc-8863-6c66282ac3cf","collapsed":false,"id":"LIgECtVqMlCI","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Da cambiare\n\n# class dataset(torch.utils.data.Dataset):\n#     def __init__(self,file_list,transform = None):\n#         self.file_list=file_list\n#         self.transform=transform\n\n#     def __len__(self):\n#         self.filelength =len(self.file_list)\n#         return self.filelength\n\n#     def __getitem__(self,idx):\n#         img_path =self.file_list[idx]\n#         img = Image.open(img_path)\n#         img_transformed = self.transform(img)\n\n#         label = img_path.split('/')[-1].split('.')[0]\n#         if label == 'dog':\n#             label=1\n#         elif label == 'cat':\n#             label=0\n\n#         return img_transformed,label","metadata":{"_uuid":"f189b005-b775-4f98-8b03-dda76aa169dd","_cell_guid":"848c48eb-38a9-41ec-ba2c-e83ce580fed8","collapsed":false,"id":"XgblKB3hlBuX","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as F\n\ndef rl_decode(rl_str, height, length):\n  mask = np.zeros(shape=(1,height,length))\n  couples = rl_str.split()\n  for i in range(0, len(couples)-1, 2):\n    # print(i)\n    el = int(couples[i])\n    qty = int(couples[i+1])\n    r,c = np.unravel_index(el,(height,length))\n    for j in range(qty):\n      # mask[0, r, c+j] = 1\n      mask[0, c+j-1, r-1] = 1\n\n    # print(torch.Tensor(mask))\n  return torch.Tensor(mask).reshape((768, 768)).gt(0)\n\ndef show(imgs, rotation=None):\n\n    if rotation:\n          imgs = rotate(imgs, rotation)\n\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nclass ShipsDataset(torch.utils.data.Dataset):\n    def __init__(self, file_list, targets, transforms = None, target_transforms = None):\n        self.file_list = file_list\n        self.targets = targets\n        self.transform = transforms\n\n    def __len__(self):\n        self.filelength = len(self.file_list)\n        return self.filelength\n\n    def __getitem__(self, idx):\n        image = read_image(self.file_list[idx])    # numpy tensor\n\n        image = F.convert_image_dtype(image)\n        # Added this line to fix this problem (ENF) during training\n        # TypeError: Expected input images to be of floating type (in range [0, 1]), but found type torch.uint8 instead\n\n        # try:\n        label = self.targets[idx]       # dictionary {\"boxes\": , \"label\": }\n        label['boxes'] = torch.Tensor(label['boxes'])\n        label['labels'] = torch.Tensor(label['labels']).to(dtype=torch.int64).reshape((-1,))\n        # except IndexError as e:\n        #     Warning(f'Errore con {idx = }')\n        #     plt.imshow(image.permute(1, 2, 0))\n        #     plt.show()\n\n        if self.transform:\n            image = self.transform((label, image))\n\n            # prova ad indagare da qui\n            # image = self.transform(image)\n            # image = image.numpy()\n            # return image, label\n            # print(f\"{image = }\")\n            # print(f\"{label = }\")\n\n        return image, label","metadata":{"_uuid":"9d46e794-e37a-4cc3-afc4-4c31a63768a3","_cell_guid":"d7c3fba5-7966-4876-a902-31971e6e4121","collapsed":false,"id":"V1Q6ogjksMqE","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Mostra segmentation masks\n\n# # import random as rd # rd.sample(colors, k=1)\n# # colors = [\"yellow\", \"green\", \"blue\", \"red\"]\n\n# masks = [] # contains all the masks\n\n# targets = pd.read_csv(\"train_ship_segmentations_v2.csv\")\n# # image_id = \"000155de5.jpg\"\n# # image_id = \"000194a2d.jpg\"\n# # image_id = \"00021ddc3.jpg\"\n# image_id = \"0005d01c8.jpg\"\n\n# filtered_targets = targets[targets['ImageId'] == image_id] # Trova l'indice delle righe associate a image_id: ce n'è una per ogni classe\n\n# image = read_image(os.path.join(\"train_v2\", image_id))\n\n# for index in filtered_targets.index:\n#   label = targets.iloc[index][1]\n#   masks.append(rl_decode(label, image.size()[1], image.size()[2]))\n\n# mask_sum = torch.empty([768, 768])\n\n# for mask in masks:\n#   mask_sum += mask\n\n# mask_sum = mask_sum.gt(0.1) # shouldn't be there in the first place since all the values are boolean\n\n# show(draw_segmentation_masks(image, mask_sum, alpha=0.3, colors=\"yellow\"))\n\n# plt.show()","metadata":{"_uuid":"11446938-7061-43e5-b06f-43895b547869","_cell_guid":"0758e7af-69ca-4da4-9134-dcf474c7b40b","collapsed":false,"id":"tqXAKhOVB6Lm","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Bounding box\n\n# import cv2\n\n# boxes = []\n# for mask in masks:\n#   mask_np = (255*mask).byte().numpy() # to convert to integers\n#   mask_np = cv2.resize(mask_np, (768, 768))\n\n#   contours, hierarchy = cv2.findContours(mask_np,  cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n#   print(f\"{contours[0] = }\")\n#   rect = cv2.minAreaRect(contours[0])\n\n#   # yolo expects label like this\n#   # class x_center y_center width height\n\n#   # (center(x, y), (width, height), angle of rotation) = cv2.minAreaRect(points)\n\n#   print(f\"{rect = }\")\n\n#   # if rect[1][1] > rect[1][0]:\n#   #   angle = 90-rect[2]\n#   # else:\n#   #   angle = -rect[2]\n\n#   # box = cv2.boxPoints(rect)\n#   # box = np.intp(box)\n#   # boxes.append(box)\n\n# # Bounding boxes found for image_id = \"0005d01c8.jpg\"\n# # [[ 70.47716 712.8358 ]\n# #  [215.08986 597.5365 ]\n# #  [235.5294  623.1725 ]\n# #  [ 90.9167  738.4718 ]]\n# # [[ 70 712]\n# #  [215 597]\n# #  [235 623]\n# #  [ 90 738]]\n\n# # contours_image = cv2.imread(os.path.join(\"train_v2\", image_id))\n# # cv2.drawContours(contours_image, boxes, -1, (0,191,255), 2)\n# # plt.figure()\n# # plt.imshow(contours_image)","metadata":{"_uuid":"ec44ed7e-2f63-4e9f-9288-d8cf9047668c","_cell_guid":"33f92671-7170-4a7e-b09e-6dc7a929f757","collapsed":false,"id":"Z7cZp7FeSama","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision import tv_tensors\n\nimport os\nprint(os.getcwd())\n\n# DATASET_DIR = os.path.join(\".\")\nTRAIN_DIR = os.path.join(\"train_v2\")\nTEST_DIR = os.path.join(\"test_v2\")\n# print(DATASET_DIR, TRAIN_DIR, TEST_DIR)\n\ntrain_list = glob.glob(os.path.join(TRAIN_DIR,'*.jpg'))\ntrain_list, val_list = train_test_split(train_list, test_size = 0.2)\n\n# train_data = ShipsDataset(train_list, transforms = img_train_transforms, targets=np.load('rcnn_targets.npy', allow_pickle='TRUE'))\n# # test_data = ShipsDataset(train_list, transforms = img_train_transforms)\n# val_data = ShipsDataset(val_list, transforms = img_validation_transforms,targets=np.load('rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_data = ShipsDataset(train_list, transforms = None, targets=np.load('rcnn_targets.npy', allow_pickle='TRUE'))\n# test_data = ShipsDataset(train_list, transforms = img_train_transforms)\nval_data = ShipsDataset(val_list, transforms = None,targets=np.load('rcnn_targets.npy', allow_pickle='TRUE') )\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\nval_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size = batch_size, shuffle = True, collate_fn=lambda x: x)\n\nprint(len(train_data),len(train_loader))\nprint(len(val_data), len(val_loader))\n\nmodel_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n# https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n# La documentazione non è chiara sulla posizione dei punti per le ground-truth!\n# /Users/ludovicamazzucco/Library/Python/3.9/lib/python/site-packages/torchvision/models/detection/generalized_rcnn.py\"","metadata":{"_uuid":"e881bc76-8dce-441c-8229-3a46d9083e40","_cell_guid":"734a631c-9fb5-451d-844c-a7ab77f6b9ff","collapsed":false,"id":"YW9039lzlK5S","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test enf\n\n# print(type(train_data))\n\n# print(train_data.__getitem__(4))\n\n# print(f\"{image = }\")\n# print(f\"{label = }\")","metadata":{"_uuid":"2ebc937a-f57f-4cd7-aa58-3f53d80bce5a","_cell_guid":"9f7a5061-62a4-490e-a93c-196c21e9b16a","collapsed":false,"id":"0E9_zq-Og5t0","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## STEP 1. freeze backbone layers, add final layers and train the network\n\nfor name, param in model_rcnn.named_parameters():\n      param.requires_grad = False\n\nnum_classes = 2 # background, ship\n\nin_features = model_rcnn.roi_heads.box_predictor.cls_score.in_features\nmodel_rcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"_uuid":"d03e9038-bb70-4a25-844c-caf367db09b3","_cell_guid":"205f69cf-8dd0-443e-a21a-345bb8c0a3ac","collapsed":false,"id":"5J9M_bnAxnDk","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# How to save in google drive something else\n# if save_to_drive:\n#   with open('/content/drive/MyDrive/MLVM_project/file.txt', 'w') as f:\n#     f.write('content')\n\ndef save_checkpoint(epoch, model, optimizer, train_loss, val_loss, model_name=\"model.tar\"):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss\n    }, os.path.join(model_filepath, model_name))","metadata":{"_uuid":"6eba70ff-96e9-4522-b787-d1946d2b9017","_cell_guid":"528b2af3-593a-4e0a-ae5b-ef64c8d760e4","collapsed":false,"id":"Du5q6_RRCmD4","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN\n\ndef train(model, optimizer, loss_fn, train_loader, val_loader, epochs=1, device=\"cpu\"):\n    for epoch in range(epochs):\n        training_loss = 0.0\n        valid_loss = 0.0\n        model.train()\n\n        for i, batch in enumerate(train_loader):\n            logger.info(f\"E: {str(epoch)} B: {str(i)}\")\n            print(\"batch\", i) # this should be unneccessary\n            optimizer.zero_grad()\n            # inputs, targets = batch\n            \"\"\" inputs = [img for i,el in enumerate(batch)]\n            targets = [lab for img,lab in batch] \"\"\"\n\n            # filtering out empty images (model does not accept empty targets)\n            inputs = []\n            targets = []\n            for el in batch:       # el = (image,labels)\n                if el[1]['boxes'].size()[0] != 0:\n                    inputs.append(el[0])\n                    targets.append(el[1])\n\n           # inputs = inputs.to(device)\n           # targets = targets.to(device)\n            output = model(inputs,targets)  # NOTE: output is a dict with already computed losses within!\n\n            \"\"\" EXAMPLE :\n            {'loss_classifier': tensor(1.0206, grad_fn=<NllLossBackward0>),\n             'loss_box_reg': tensor(0.0071, grad_fn=<DivBackward0>),\n             'loss_objectness': tensor(1.8541), 'loss_rpn_box_reg': tensor(1.8591)} \"\"\"\n\n            loss = sum(loss for loss in output.values())\n            #train_loss_list.append(loss.detach().cpu().numpy())\n            loss.backward()\n            optimizer.step()\n            training_loss += loss.data.item() * len(inputs)\n        training_loss /= len(train_loader.dataset)\n\n        model.eval()\n        num_correct = 0\n        num_examples = 0\n        for batch in val_loader:\n            inputs = [img for img,lab in batch]\n            targets = [lab for img,lab in batch]\n           # inputs = inputs.to(device)\n            output = model(inputs)\n           # targets = targets.to(device)\n            loss = sum(loss for loss in output.values())\n            valid_loss += loss.data.item() * len(inputs)\n\n            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets).view(-1)\n            num_correct += torch.sum(correct).item()\n            num_examples += correct.shape[0]\n        valid_loss /= len(val_loader.dataset)\n\n        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.4f}'.format(epoch, training_loss,\n        valid_loss, num_correct / num_examples))\n\n        logger.info('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, accuracy = {:.4f}'.format(epoch, training_loss,\n        valid_loss, num_correct / num_examples))\n\n        # salva quando finisce l'epoch corrente\n        if save_to_drive:\n          save_checkpoint(epoch, model, optimizer, training_loss, valid_loss)","metadata":{"_uuid":"a9a507f8-c784-4847-a373-79f1a84ba9aa","_cell_guid":"ee5ce9ae-8a53-4e0e-9db9-99e5a583fd43","collapsed":false,"id":"Mv8b06EulUK2","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","metadata":{"_uuid":"320e60fe-fabe-487b-91d3-a2b3d5965cba","_cell_guid":"3d46d08a-3a51-4452-b79e-882309e42a16","collapsed":false,"id":"LloRuEg8lWyf","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model_rcnn.to(device)\ntorch.compile(model)\noptimizer = optim.Adam(params = model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()","metadata":{"_uuid":"5a384475-40a4-4c13-9c8a-8efeb96ed8f2","_cell_guid":"2f312861-b958-45c9-92e5-ed33d593194f","collapsed":false,"id":"2LUibV2Elccf","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(model, optimizer, torch.nn.CrossEntropyLoss(), train_loader, val_loader, epochs=num_epochs , device=device) # Change epochs later","metadata":{"_uuid":"a79ed4b4-e470-4dfc-8f51-ef43b63f3ae5","_cell_guid":"620eeb9f-ac5f-4f99-b241-c62df61659a8","collapsed":false,"id":"COB3KM9Fx4i7","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_state_dict')","metadata":{"_uuid":"123fa931-9b11-4218-9ce3-d890d311ca3a","_cell_guid":"284a80bc-f408-43c9-bbcb-f22c00d3fff5","collapsed":false,"id":"G3kByaG-hTrc","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correct = 0\n# total = 0\n# with torch.no_grad():\n#     for data in val_loader:\n#         images, labels = data[0].to(device), data[1].to(device)\n#         predictions = torch.argmax(model(images),dim=1)\n\n#         total += labels.size(0)\n#         correct += (predictions == labels).sum().item()\n\n# print('accuracy = {:f}'.format(correct / total))\n# print('correct: {:d}  total: {:d}'.format(correct, total))","metadata":{"_uuid":"ce6b85c5-91c9-488b-9ae0-74bd3514487f","_cell_guid":"93e16d46-9dc5-41c7-8837-52d4920e6149","collapsed":false,"id":"MLPxPQrile1o","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}